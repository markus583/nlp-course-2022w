{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Please fill out the information of your group!\n",
    "\n",
    "| <p style=\"text-align: center;\">First Name</p>  | <p style=\"text-align: center;\">Family Name</p> | Matr.-No. |\n",
    "| ---------------------------------------------- | ---------------------------------------------- | -------- |\n",
    "| <p style=\"text-align: left\">Markus</p>| <p style=\"text-align: left\">Frohmann</p> | *EDIT!* |\n",
    "| <p style=\"text-align: left\">Tobias</p>| <p style=\"text-align: left\">Morocutti</p> | k12008172 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2 style=\"text-align: center\">344.105/6/7 UE: Natural Language Processing (WS2022/23)</h2>\n",
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 1</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Document Classification with Standard Machine Learning Methods</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"background-color:rgb(224, 243, 255)\">\n",
    "<b>Terms of Use</b><br>\n",
    "This  material is prepared for educational purposes at the Johannes Kepler University (JKU) Linz, and is exclusively provided to the registered students of the mentioned course at JKU. It is strictly forbidden to distribute the current file, the contents of the assignment, and its solution. The use or reproduction of this manuscript is only allowed for educational purposes in non-profit organizations, while in this case, the explicit prior acceptance of the author(s) is required.\n",
    "\n",
    "**Author:** Navid Rekab-saz<br>\n",
    "**Email:** navid.rekabsaz@jku.at<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<h2>Table of contents</h2>\n",
    "<ol>\n",
    "    <a href=\"#section-general-guidelines\"><li style=\"font-size:large;font-weight:bold\">General Guidelines</li></a>\n",
    "    <a href=\"#section-preprocessing\"><li style=\"font-size:large;font-weight:bold\">Task A: Pre-processing & Feature Extraction (15 points)</li></a>\n",
    "    <a href=\"#section-training\"><li style=\"font-size:large;font-weight:bold\">Task B: Training and Results Analysis (15 points)</li></a>\n",
    "    <a href=\"#section-optional\"><li style=\"font-size:large;font-weight:bold\">Task C: Linear Model Interpretability (2 extra point)</li></a>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"section-general-guidelines\"></a><h2 style=\"color:rgb(0,120,170)\">General Guidelines</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"background-color:rgb(224, 243, 255)\">\n",
    "\n",
    "### Assignment objective\n",
    "\n",
    "The aim of this assignment is to implement a document (sentence) classification model using (standard) machine learning methods. The assignment in total has **30 points**; it also offers **2 extra points** which can cover any missing point.\n",
    "\n",
    "This Notebook encompasses all aspects of the assignment, namely the descriptions of tasks as well as your solutions and reports. Feel free to add any required cell for solutions. The cells can contain code, reports, charts, tables, or any other material, required for the assignment. Feel free to provide the solutions in an interactive and visual way! \n",
    "\n",
    "Please discuss any unclear point in the assignment in the provided forum in MOODLE. It is also encouraged to provide answers to your peer's questions. However when submitting a post, keep in mind to avoid providing solutions. Please let the tutor(s) know shall you find any error or unclarity in the assignment.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"background-color:rgb(224, 243, 255)\">\n",
    "\n",
    "### Libraries & Dataset\n",
    "\n",
    "The assignment should be implemented with recent versions of `Python` (>3.7). Any standard Python library can be used, so far that the library is free and can be simply installed using `pip` or `conda`. Examples of potentially useful libraries are `scikit-learn`, `numpy`, `scipy`, `gensim`, `nltk`, `spaCy`, and `AllenNLP`. Use the latest stable version of each library.\n",
    "\n",
    "To conduct the experiments, two datasets are provided. The datasets are taken from the data of `thedeep` project, produced by the DEEP (https://www.thedeep.io) platform. The DEEP is an open-source platform, which aims to facilitate processing of textual data for international humanitarian response organizations. The platform enables the classification of text excerpts, extracted from news and reports into a set of domain specific classes. The provided dataset has 12 classes (labels) like agriculture, health, and protection. \n",
    "\n",
    "The difference between the datasets is in their sizes. We refer to these as `medium` and `small`, containing an overall number of 38,000 and 12,000 annotated text excerpts, respectively. Select one of the datasets, and use it for all of the tasks. `medium` provides more data and therefore reflects a more realistic scenario. `small` is however provided for the sake of convenience, particularly if running the experiments on your available hardware takes too long. Using `medium` is generally recommended, but from the point of view of assignment grading, there is no difference between the datasets.\n",
    "\n",
    "Download the dataset from [this link](https://drive.jku.at/filr/public-link/file-download/0cce88f083887a040183c5bebe366b04/43388/4737518022038762454/nlp2022_23_data.zip).\n",
    "\n",
    "Whether `medium` or `small`, you will find the following files in the provided zip file:\n",
    "- `thedeep.$name$.train.txt`: Train set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.validation.txt`: Validation set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.test.txt`: Test set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.$name$.label.txt`: Captions of the labels.\n",
    "- `README.txt`: Terms of use of the dataset.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"background-color:rgb(224, 243, 255)\">\n",
    "\n",
    "### Submission\n",
    "\n",
    "Each group should submit the following two files:\n",
    "\n",
    "- One Jupyter Notebook file (`.ipynb`), containing all the code, results, visualizations, etc. **In the submitted Notebook, all the results and visualizations should already be present, and can be observed simply by loading the Notebook in a browser.** The Notebook must be self-contained, meaning that (if necessary) one can run all the cells from top to bottom without any error. Do not forget to put in your names and student numbers in the first cell of the Notebook. \n",
    "- The HTML file (`.html`) achieved from exporting the Jupyter Notebook to HTML (Download As HTML).\n",
    "\n",
    "You do not need to include the data files in the submission.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"section-preprocessing\"></a><h2 style=\"color:rgb(0,120,170)\">Task A: Pre-processing & Feature Extraction (15 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"background-color:rgb(224, 243, 255)\">\n",
    "    \n",
    "**Preprocessing (4 points).** Load the train, validation, and test sets. Study the text and according to your judgements, apply at least <ins>two text cleaning/preprocessing methods</ins>. Punctuations marks, numbers, dates, case-sensitivity are some examples of the elements which can be potentially considered for cleaning/preprocessing. Tokenize the result text with a tokenizer of your choice. Report your approaches to text cleaning and tokenization and the reasons of your choices. Provide some examples, showing the effects of the applied approaches on the text.\n",
    "\n",
    "**Creating dictionary (4 points).** Create a dictionary of vocabularies following the guidelines discussed in the lecture. Next, reduce the size of dictionary using a method of your choice, for instance by considering a cut-off threshold on the tokens with low frequencies. When removing tokens from the dictionary, consider a strategy for handling Out-Of-Vocabulary (OOV) tokens, namely the ones in the train/validation/test datasets that that are not anymore in the dictionary. Some possible strategies could be to remove OOVs completely from the texts, or to replace them with a special token like <OOV\\>. Explain your approaches and report the statistics of the dictionary before and after the reduction.\n",
    "\n",
    "**Creating sentence vectors (4 points).** Use the dictionary to prepare <ins>two variations of document representation vectors</ins>, separately for train, validation, and test sets. Both variations follow a Bag-of-Words approach with a different token weighting method. One applied weighting must be `tf-idf` and the other one can be any other methods discussed in the lecture such as `tc`, `tf`, `BM25`. These term weighting methods should be implemented; using a library to readily calculate the term weightings is not allowed. Report the applied approaches. Calculate and report the sparsity rate of the vectors of train, validation, and test sets, namely what percentages of the vectors in each set are filled with zeros.\n",
    "\n",
    "**Dimensionality reduction (3 points).** Reduce vectors' dimensions to $k$ by applying Latent Semantic Analysis (LSA) to the vectors of both variations. $k$ is a hyper-parameter and can be $10<k<1000$. Keep in mind the training and inference phases of LSA, when applied to the train, validation, and test sets. \n",
    "\n",
    "At the end of Task A, you should have the <ins>four feature vectors variations</ins> shown below, each consisting of the sets of train, validation, and test:\n",
    "- **`Token Weighting I - High Dimensional`**\n",
    "- **`Token Weighting I - Low Dimensional`**\n",
    "- **`Token Weighting II - High Dimensional`**\n",
    "- **`Token Weighting II - Low Dimensional`**\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the train, validation, and test sets\n",
    "BASE_DIR = 'data/nlp2022_23_data/'\n",
    "TRAIN_FILE = BASE_DIR + 'thedeep.subset' + '.train.txt'\n",
    "VAL_FILE = BASE_DIR + 'thedeep.subset' + '.validation.txt' \n",
    "TEST_FILE = BASE_DIR + 'thedeep.subset' + '.test.txt'\n",
    "LABEL_FILE = BASE_DIR + 'thedeep.' + 'labels.txt'\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILE, sep=',', header=None, names=['sentence_id', 'text', 'label'])\n",
    "val_df = pd.read_csv(VAL_FILE, sep=',', header=None, names=['sentence_id', 'text', 'label'])\n",
    "test_df = pd.read_csv(TEST_FILE, sep=',', header=None, names=['sentence_id', 'text', 'label'])\n",
    "label_df = pd.read_csv(LABEL_FILE, sep=',', header=None, names=['label', 'caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5446</td>\n",
       "      <td>In addition to the immediate life-saving inter...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8812</td>\n",
       "      <td>There are approximately 2.6 million people cla...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16709</td>\n",
       "      <td>While aid imports have held up recently, comme...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3526</td>\n",
       "      <td>Heavy rainfalls as well as onrush of water fro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4928</td>\n",
       "      <td>Based on field reports 9 , the main production...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12105</th>\n",
       "      <td>12744</td>\n",
       "      <td>The total gap in the number of people who requ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12106</th>\n",
       "      <td>9655</td>\n",
       "      <td>A food crisis is looming in the country with t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12107</th>\n",
       "      <td>6963</td>\n",
       "      <td>? Acute watery diarrhoea (AWD) continues to be...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12108</th>\n",
       "      <td>923</td>\n",
       "      <td>As South India grapples with drought and water...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12109</th>\n",
       "      <td>15880</td>\n",
       "      <td>Mirroring trends in South Africa, the main sou...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12110 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_id                                               text  label\n",
       "0             5446  In addition to the immediate life-saving inter...      9\n",
       "1             8812  There are approximately 2.6 million people cla...      3\n",
       "2            16709  While aid imports have held up recently, comme...      5\n",
       "3             3526  Heavy rainfalls as well as onrush of water fro...      0\n",
       "4             4928  Based on field reports 9 , the main production...      3\n",
       "...            ...                                                ...    ...\n",
       "12105        12744  The total gap in the number of people who requ...      8\n",
       "12106         9655  A food crisis is looming in the country with t...      0\n",
       "12107         6963  ? Acute watery diarrhoea (AWD) continues to be...      4\n",
       "12108          923  As South India grapples with drought and water...     11\n",
       "12109        15880  Mirroring trends in South Africa, the main sou...      3\n",
       "\n",
       "[12110 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set length:  12110\n",
      "Validation set length:  2596\n",
      "Test set length:  2595\n"
     ]
    }
   ],
   "source": [
    "# print length of each set\n",
    "print('Train set length: ', len(train_df))\n",
    "print('Validation set length: ', len(val_df))\n",
    "print('Test set length: ', len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of text in train set:  450.86\n",
      "Average length of text in validation set:  446.5\n",
      "Average length of text in test set:  440.51\n"
     ]
    }
   ],
   "source": [
    "# print average length of text in each set\n",
    "print('Average length of text in train set: ', np.round(np.mean(train_df['text'].str.len()), 2))\n",
    "print('Average length of text in validation set: ', np.round(np.mean(val_df['text'].str.len()), 2))\n",
    "print('Average length of text in test set: ', np.round(np.mean(test_df['text'].str.len()), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Agriculture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Cross</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Livelihood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Logistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>NFI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Protection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Shelter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>WASH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label      caption\n",
       "0       0  Agriculture\n",
       "1       1        Cross\n",
       "2       2    Education\n",
       "3       3         Food\n",
       "4       4       Health\n",
       "5       5   Livelihood\n",
       "6       6     Logistic\n",
       "7       7          NFI\n",
       "8       8    Nutrition\n",
       "9       9   Protection\n",
       "10     10      Shelter\n",
       "11     11         WASH"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each label corresponds to a given caption, i.e., a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def map_label_to_caption(label):\n",
    "    \"\"\"\n",
    "    Map label id to caption using label_df\n",
    "    :param label: label id, int\n",
    "    :return: label caption, str\n",
    "    \"\"\"\n",
    "    return label_df[label_df['label'] == label]['caption'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.assign(caption=train_df['label'].apply(map_label_to_caption))\n",
    "val_df = val_df.assign(caption=val_df['label'].apply(map_label_to_caption))\n",
    "test_df = test_df.assign(caption=test_df['label'].apply(map_label_to_caption))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove punctuation marks, replace dates & numbers, apply case-sensitivity\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing punctuation marks, replacing dates & numbers, applying case-sensitivity\n",
    "    :param text: text to clean, str\n",
    "    :return: cleaned text, str\n",
    "    \"\"\"\n",
    "    # remove punctuation marks\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # replace dates\n",
    "    text = re.sub(r'\\d{1,2}/\\d{1,2}/\\d{2,4}', '< date >', text)\n",
    "    # replace numbers\n",
    "    text = re.sub(r'\\d+', '< num >', text)\n",
    "    # apply case-sensitivity\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n",
    "val_df['cleaned_text'] = val_df['text'].apply(clean_text)\n",
    "test_df['cleaned_text'] = test_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>caption</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5446</td>\n",
       "      <td>In addition to the immediate life-saving inter...</td>\n",
       "      <td>9</td>\n",
       "      <td>Protection</td>\n",
       "      <td>in addition to the immediate lifesaving interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8812</td>\n",
       "      <td>There are approximately 2.6 million people cla...</td>\n",
       "      <td>3</td>\n",
       "      <td>Food</td>\n",
       "      <td>there are approximately &lt; num &gt; million people...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16709</td>\n",
       "      <td>While aid imports have held up recently, comme...</td>\n",
       "      <td>5</td>\n",
       "      <td>Livelihood</td>\n",
       "      <td>while aid imports have held up recently commer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3526</td>\n",
       "      <td>Heavy rainfalls as well as onrush of water fro...</td>\n",
       "      <td>0</td>\n",
       "      <td>Agriculture</td>\n",
       "      <td>heavy rainfalls as well as onrush of water fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4928</td>\n",
       "      <td>Based on field reports 9 , the main production...</td>\n",
       "      <td>3</td>\n",
       "      <td>Food</td>\n",
       "      <td>based on field reports &lt; num &gt;  the main produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12105</th>\n",
       "      <td>12744</td>\n",
       "      <td>The total gap in the number of people who requ...</td>\n",
       "      <td>8</td>\n",
       "      <td>Nutrition</td>\n",
       "      <td>the total gap in the number of people who requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12106</th>\n",
       "      <td>9655</td>\n",
       "      <td>A food crisis is looming in the country with t...</td>\n",
       "      <td>0</td>\n",
       "      <td>Agriculture</td>\n",
       "      <td>a food crisis is looming in the country with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12107</th>\n",
       "      <td>6963</td>\n",
       "      <td>? Acute watery diarrhoea (AWD) continues to be...</td>\n",
       "      <td>4</td>\n",
       "      <td>Health</td>\n",
       "      <td>acute watery diarrhoea awd continues to be re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12108</th>\n",
       "      <td>923</td>\n",
       "      <td>As South India grapples with drought and water...</td>\n",
       "      <td>11</td>\n",
       "      <td>WASH</td>\n",
       "      <td>as south india grapples with drought and water...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12109</th>\n",
       "      <td>15880</td>\n",
       "      <td>Mirroring trends in South Africa, the main sou...</td>\n",
       "      <td>3</td>\n",
       "      <td>Food</td>\n",
       "      <td>mirroring trends in south africa the main sour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12110 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_id                                               text  label  \\\n",
       "0             5446  In addition to the immediate life-saving inter...      9   \n",
       "1             8812  There are approximately 2.6 million people cla...      3   \n",
       "2            16709  While aid imports have held up recently, comme...      5   \n",
       "3             3526  Heavy rainfalls as well as onrush of water fro...      0   \n",
       "4             4928  Based on field reports 9 , the main production...      3   \n",
       "...            ...                                                ...    ...   \n",
       "12105        12744  The total gap in the number of people who requ...      8   \n",
       "12106         9655  A food crisis is looming in the country with t...      0   \n",
       "12107         6963  ? Acute watery diarrhoea (AWD) continues to be...      4   \n",
       "12108          923  As South India grapples with drought and water...     11   \n",
       "12109        15880  Mirroring trends in South Africa, the main sou...      3   \n",
       "\n",
       "           caption                                       cleaned_text  \n",
       "0       Protection  in addition to the immediate lifesaving interv...  \n",
       "1             Food  there are approximately < num > million people...  \n",
       "2       Livelihood  while aid imports have held up recently commer...  \n",
       "3      Agriculture  heavy rainfalls as well as onrush of water fro...  \n",
       "4             Food  based on field reports < num >  the main produ...  \n",
       "...            ...                                                ...  \n",
       "12105    Nutrition  the total gap in the number of people who requ...  \n",
       "12106  Agriculture  a food crisis is looming in the country with t...  \n",
       "12107       Health   acute watery diarrhoea awd continues to be re...  \n",
       "12108         WASH  as south india grapples with drought and water...  \n",
       "12109         Food  mirroring trends in south africa the main sour...  \n",
       "\n",
       "[12110 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# normalize text\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text by tokenizing and lemmatizing\n",
    "    :param text: text to normalize, str\n",
    "    :return: normalized text, str\n",
    "    \"\"\"\n",
    "    # tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # lemmatize text\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tobias/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tobias/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/tobias/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df['normalized_text'] = train_df['cleaned_text'].apply(normalize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use BPE tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "files =[TRAIN_FILE]\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train_df['tokenized_text'] = train_df['cleaned_text'].apply(tokenizer.encode).apply(lambda x: x.tokens)\n",
    "# train_df['tokenized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use NLTK tokenizer (TreebankWordTokenizer)\n",
    "from nltk.tokenize import word_tokenize\n",
    "train_df['tokenized_text'] = train_df['cleaned_text'].apply(word_tokenize)\n",
    "val_df['tokenized_text'] = val_df['cleaned_text'].apply(word_tokenize)\n",
    "test_df['tokenized_text'] = test_df['cleaned_text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tobias/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "train_df['tokenized_text'] = train_df['tokenized_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "val_df['tokenized_text'] = val_df['tokenized_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "test_df['tokenized_text'] = test_df['tokenized_text'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# compare text before preprocessing, after preprocessing, and after tokenization\n",
    "def compare_text(text, cleaned_text, tokenized_text):\n",
    "    \"\"\"\n",
    "    Compare text before preprocessing, after preprocessing, and after tokenization\n",
    "    :param text: text before preprocessing, str\n",
    "    :param cleaned_text: text after preprocessing, str\n",
    "    :param tokenized_text: text after tokenization, list\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    print('Text before preprocessing: \\n', text)\n",
    "    print('Text after preprocessing: \\n', cleaned_text)\n",
    "    print('Text after tokenization: \\n', tokenized_text, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before preprocessing: \n",
      " While aid imports have held up recently, commercial food and fuel imports remain well short of pre-blockade averages. I am particularly concerned about the recent decline of commercial food imports through the Red Sea ports. Pressure on the currency and a liquidity crisis in the Yemeni banking system make imports less viable for traders. Confidence among commercial shipping companies has eroded due to delays, including as a result of inspections undertaken by the Saudiled Coalition after these vessels have been cleared by UNVIM.\n",
      "Text after preprocessing: \n",
      " while aid imports have held up recently commercial food and fuel imports remain well short of preblockade averages i am particularly concerned about the recent decline of commercial food imports through the red sea ports pressure on the currency and a liquidity crisis in the yemeni banking system make imports less viable for traders confidence among commercial shipping companies has eroded due to delays including as a result of inspections undertaken by the saudiled coalition after these vessels have been cleared by unvim\n",
      "Text after tokenization: \n",
      " ['aid', 'imports', 'held', 'recently', 'commercial', 'food', 'fuel', 'imports', 'remain', 'well', 'short', 'preblockade', 'averages', 'particularly', 'concerned', 'recent', 'decline', 'commercial', 'food', 'imports', 'red', 'sea', 'ports', 'pressure', 'currency', 'liquidity', 'crisis', 'yemeni', 'banking', 'system', 'make', 'imports', 'less', 'viable', 'traders', 'confidence', 'among', 'commercial', 'shipping', 'companies', 'eroded', 'due', 'delays', 'including', 'result', 'inspections', 'undertaken', 'saudiled', 'coalition', 'vessels', 'cleared', 'unvim']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "\n",
    "compare_text(train_df['text'][idx], train_df['cleaned_text'][idx], train_df['tokenized_text'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before preprocessing: \n",
      "  According to media reports, in 2017 and as of 9 October 2017, 100 possible cholera cases have been reported in Saptari region in Nepal. \n",
      "Text after preprocessing: \n",
      "  according to media reports in < num > and as of < num > october < num > < num > possible cholera cases have been reported in saptari region in nepal \n",
      "Text after tokenization: \n",
      " ['according', 'media', 'reports', '<', 'num', '>', '<', 'num', '>', 'october', '<', 'num', '>', '<', 'num', '>', 'possible', 'cholera', 'cases', 'reported', 'saptari', 'region', 'nepal']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 42\n",
    "\n",
    "compare_text(train_df['text'][idx], train_df['cleaned_text'][idx], train_df['tokenized_text'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text before preprocessing: \n",
      " With Sao Tome heavily dependent on food imports, food availability is unpredictable: there is no deep sea port; in bad weather, landing is difficult on the country’s one short airstrip. No cereals are cultivated on the island. The country is prone to natural disasters such as floods and landslides, which negatively affect crops and road access as well as destroy houses and household assets.\n",
      "Text after preprocessing: \n",
      " with sao tome heavily dependent on food imports food availability is unpredictable there is no deep sea port in bad weather landing is difficult on the countrys one short airstrip no cereals are cultivated on the island the country is prone to natural disasters such as floods and landslides which negatively affect crops and road access as well as destroy houses and household assets\n",
      "Text after tokenization: \n",
      " ['sao', 'tome', 'heavily', 'dependent', 'food', 'imports', 'food', 'availability', 'unpredictable', 'deep', 'sea', 'port', 'bad', 'weather', 'landing', 'difficult', 'countrys', 'one', 'short', 'airstrip', 'cereals', 'cultivated', 'island', 'country', 'prone', 'natural', 'disasters', 'floods', 'landslides', 'negatively', 'affect', 'crops', 'road', 'access', 'well', 'destroy', 'houses', 'household', 'assets']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 100\n",
    "\n",
    "compare_text(train_df['text'][idx], train_df['cleaned_text'][idx], train_df['tokenized_text'][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create dictionary using train set\n",
    "def create_dictionary(tokenized_text):\n",
    "    \"\"\"\n",
    "    Create dictionary using train set\n",
    "    :param tokenized_text: tokenized text, list\n",
    "    :return: dictionary, dict\n",
    "    \"\"\"\n",
    "    dictionary = {}\n",
    "    for tokens in tokenized_text:\n",
    "        for token in tokens:\n",
    "            if token not in dictionary:\n",
    "                dictionary[token] = 1\n",
    "            else:\n",
    "                dictionary[token] += 1\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = create_dictionary(train_df['tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dictionary:  30396\n"
     ]
    }
   ],
   "source": [
    "# print length of dictionary\n",
    "print('Length of dictionary: ', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in dictionary:  [('<', 41249), ('num', 41249), ('>', 41249), ('cases', 4758), ('food', 4138), ('people', 3799), ('reported', 3676), ('children', 2894), ('areas', 2551), ('water', 2519)]\n"
     ]
    }
   ],
   "source": [
    "# print most common words in dictionary\n",
    "print('Most common words in dictionary: ', sorted(dictionary.items(), key=lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# keep only top-N most frequent words\n",
    "# removing any word with a lower frequency than a threshold\n",
    "# OOV (out-of-vocabulary) words will be replaced with the <oov> token\n",
    "def clean_dictionary(dictionary, N: int, threshold: int):\n",
    "    \"\"\"\n",
    "    Clean dictionary by keeping only top-N most frequent words and removing any word with a lower frequency than a threshold\n",
    "    :param dictionary: dictionary, dict\n",
    "    :param N: top-N most frequent words, int\n",
    "    :param threshold: threshold of word frequency, int\n",
    "    :return: cleaned dictionary, dict\n",
    "    \"\"\"\n",
    "    # keep only top-N most frequent words\n",
    "    dictionary = dict(sorted(dictionary.items(), key=lambda x: x[1], reverse=True)[:N])\n",
    "\n",
    "    # replace any word with a lower frequency than a threshold\n",
    "    for key, value in dictionary.copy().items():\n",
    "        if value < threshold:\n",
    "            dictionary['<oov>'] = dictionary.get('<oov>', 0) + value\n",
    "            dictionary.pop(key)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = clean_dictionary(dictionary, N=100_000, threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dictionary:  8195\n"
     ]
    }
   ],
   "source": [
    "# print length of dictionary\n",
    "print('Length of dictionary: ', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<', 41249),\n",
       " ('num', 41249),\n",
       " ('>', 41249),\n",
       " ('<oov>', 33980),\n",
       " ('cases', 4758),\n",
       " ('food', 4138),\n",
       " ('people', 3799),\n",
       " ('reported', 3676),\n",
       " ('children', 2894),\n",
       " ('areas', 2551),\n",
       " ('water', 2519),\n",
       " ('health', 2476),\n",
       " ('said', 1919),\n",
       " ('access', 1829),\n",
       " ('including', 1793),\n",
       " ('per', 1703),\n",
       " ('also', 1690),\n",
       " ('percent', 1657),\n",
       " ('due', 1639),\n",
       " ('affected', 1638),\n",
       " ('since', 1600),\n",
       " ('deaths', 1469),\n",
       " ('number', 1406),\n",
       " ('households', 1390),\n",
       " ('total', 1354),\n",
       " ('humanitarian', 1322),\n",
       " ('two', 1320),\n",
       " ('security', 1309),\n",
       " ('million', 1298),\n",
       " ('week', 1298),\n",
       " ('one', 1285),\n",
       " ('prices', 1268),\n",
       " ('new', 1246),\n",
       " ('cent', 1234),\n",
       " ('refugees', 1230),\n",
       " ('state', 1221),\n",
       " ('country', 1169),\n",
       " ('government', 1165),\n",
       " ('year', 1154),\n",
       " ('displaced', 1148),\n",
       " ('assistance', 1136),\n",
       " ('according', 1119),\n",
       " ('cholera', 1116),\n",
       " ('need', 1115),\n",
       " ('phase', 1114),\n",
       " ('suspected', 1080),\n",
       " ('south', 1079),\n",
       " ('high', 1070),\n",
       " ('acute', 1063),\n",
       " ('ipc', 1060),\n",
       " ('may', 1038),\n",
       " ('women', 1038),\n",
       " ('many', 1028),\n",
       " ('region', 1027),\n",
       " ('season', 1018),\n",
       " ('last', 1002),\n",
       " ('outbreak', 1002),\n",
       " ('services', 976),\n",
       " ('area', 973),\n",
       " ('three', 971),\n",
       " ('population', 970),\n",
       " ('situation', 953),\n",
       " ('emergency', 944),\n",
       " ('malnutrition', 925),\n",
       " ('families', 904),\n",
       " ('idps', 904),\n",
       " ('years', 903),\n",
       " ('january', 902),\n",
       " ('local', 887),\n",
       " ('average', 886),\n",
       " ('increased', 883),\n",
       " ('poor', 875),\n",
       " ('confirmed', 871),\n",
       " ('insecurity', 870),\n",
       " ('increase', 851),\n",
       " ('case', 837),\n",
       " ('march', 830),\n",
       " ('lack', 828),\n",
       " ('crisis', 816),\n",
       " ('across', 812),\n",
       " ('expected', 805),\n",
       " ('risk', 804),\n",
       " ('needs', 797),\n",
       " ('facilities', 782),\n",
       " ('among', 778),\n",
       " ('city', 776),\n",
       " ('schools', 772),\n",
       " ('least', 770),\n",
       " ('camps', 769),\n",
       " ('support', 756),\n",
       " ('conflict', 747),\n",
       " ('school', 739),\n",
       " ('estimated', 732),\n",
       " ('however', 729),\n",
       " ('conditions', 725),\n",
       " ('production', 723),\n",
       " ('levels', 720),\n",
       " ('compared', 717),\n",
       " ('medical', 715),\n",
       " ('months', 706),\n",
       " ('february', 706),\n",
       " ('communities', 705),\n",
       " ('five', 701),\n",
       " ('april', 698),\n",
       " ('national', 696),\n",
       " ('remain', 677),\n",
       " ('violence', 676),\n",
       " ('continue', 670),\n",
       " ('education', 665),\n",
       " ('likely', 661),\n",
       " ('authorities', 657),\n",
       " ('camp', 654),\n",
       " ('response', 650),\n",
       " ('period', 647),\n",
       " ('rate', 643),\n",
       " ('well', 635),\n",
       " ('limited', 635),\n",
       " ('regions', 628),\n",
       " ('still', 628),\n",
       " ('human', 627),\n",
       " ('armed', 623),\n",
       " ('june', 615),\n",
       " ('rights', 614),\n",
       " ('protection', 613),\n",
       " ('care', 610),\n",
       " ('october', 609),\n",
       " ('states', 609),\n",
       " ('central', 608),\n",
       " ('severe', 603),\n",
       " ('december', 598),\n",
       " ('killed', 592),\n",
       " ('september', 580),\n",
       " ('refugee', 571),\n",
       " ('report', 569),\n",
       " ('rains', 567),\n",
       " ('sites', 564),\n",
       " ('forces', 562),\n",
       " ('maize', 561),\n",
       " ('community', 557),\n",
       " ('groups', 548),\n",
       " ('four', 547),\n",
       " ('main', 542),\n",
       " ('districts', 541),\n",
       " ('northern', 539),\n",
       " ('livestock', 536),\n",
       " ('east', 534),\n",
       " ('district', 534),\n",
       " ('ongoing', 533),\n",
       " ('province', 530),\n",
       " ('living', 528),\n",
       " ('west', 521),\n",
       " ('hospital', 519),\n",
       " ('recorded', 518),\n",
       " ('sudan', 516),\n",
       " ('first', 516),\n",
       " ('north', 515),\n",
       " ('following', 511),\n",
       " ('civilians', 509),\n",
       " ('august', 506),\n",
       " ('around', 502),\n",
       " ('currently', 499),\n",
       " ('eastern', 499),\n",
       " ('month', 497),\n",
       " ('reports', 496),\n",
       " ('parts', 495),\n",
       " ('nutrition', 493),\n",
       " ('international', 493),\n",
       " ('markets', 488),\n",
       " ('rainfall', 486),\n",
       " ('particularly', 484),\n",
       " ('recent', 483),\n",
       " ('time', 483),\n",
       " ('aid', 482),\n",
       " ('remains', 482),\n",
       " ('displacement', 477),\n",
       " ('result', 463),\n",
       " ('al', 460),\n",
       " ('disease', 459),\n",
       " ('early', 459),\n",
       " ('drought', 457),\n",
       " ('several', 455),\n",
       " ('partners', 455),\n",
       " ('july', 455),\n",
       " ('southern', 455),\n",
       " ('patients', 453),\n",
       " ('shelter', 453),\n",
       " ('weeks', 452),\n",
       " ('vulnerable', 452),\n",
       " ('november', 451),\n",
       " ('town', 451),\n",
       " ('market', 450),\n",
       " ('activities', 448),\n",
       " ('treatment', 445),\n",
       " ('basic', 443),\n",
       " ('continued', 443),\n",
       " ('residents', 440),\n",
       " ('assessment', 437),\n",
       " ('idp', 436),\n",
       " ('low', 436),\n",
       " ('could', 436),\n",
       " ('houses', 435),\n",
       " ('un', 432),\n",
       " ('girls', 430),\n",
       " ('border', 428),\n",
       " ('supplies', 427),\n",
       " ('military', 427),\n",
       " ('israeli', 425),\n",
       " ('level', 423),\n",
       " ('majority', 423),\n",
       " ('ministry', 419),\n",
       " ('already', 414),\n",
       " ('attacks', 413),\n",
       " ('death', 413),\n",
       " ('day', 411),\n",
       " ('el', 409),\n",
       " ('days', 408),\n",
       " ('continues', 407),\n",
       " ('harvest', 407),\n",
       " ('six', 406),\n",
       " ('forced', 402),\n",
       " ('homes', 402),\n",
       " ('measles', 402),\n",
       " ('damaged', 399),\n",
       " ('gaza', 397),\n",
       " ('higher', 397),\n",
       " ('host', 395),\n",
       " ('price', 393),\n",
       " ('reportedly', 390),\n",
       " ('past', 390),\n",
       " ('public', 389),\n",
       " ('palestinian', 388),\n",
       " ('available', 385),\n",
       " ('even', 383),\n",
       " ('land', 383),\n",
       " ('syrian', 382),\n",
       " ('supply', 382),\n",
       " ('shelters', 380),\n",
       " ('agricultural', 373),\n",
       " ('within', 371),\n",
       " ('villages', 370),\n",
       " ('county', 370),\n",
       " ('fever', 367),\n",
       " ('destroyed', 366),\n",
       " ('end', 366),\n",
       " ('previous', 366),\n",
       " ('told', 365),\n",
       " ('increasing', 365),\n",
       " ('current', 360),\n",
       " ('without', 359),\n",
       " ('group', 359),\n",
       " ('household', 358),\n",
       " ('crops', 357),\n",
       " ('child', 356),\n",
       " ('nearly', 355),\n",
       " ('age', 355),\n",
       " ('face', 355),\n",
       " ('died', 354),\n",
       " ('counties', 352),\n",
       " ('injured', 352),\n",
       " ('sources', 350),\n",
       " ('especially', 349),\n",
       " ('would', 349),\n",
       " ('major', 347),\n",
       " ('return', 345),\n",
       " ('police', 344),\n",
       " ('availability', 343),\n",
       " ('reporting', 343),\n",
       " ('severely', 342),\n",
       " ('safe', 338),\n",
       " ('sanitation', 337),\n",
       " ('significant', 336),\n",
       " ('approximately', 335),\n",
       " ('cfr', 334),\n",
       " ('wash', 334),\n",
       " ('beginning', 334),\n",
       " ('critical', 332),\n",
       " ('western', 332),\n",
       " ('received', 331),\n",
       " ('heavy', 329),\n",
       " ('rates', 329),\n",
       " ('addition', 328),\n",
       " ('half', 328),\n",
       " ('additional', 328),\n",
       " ('caused', 327),\n",
       " ('less', 323),\n",
       " ('provide', 323),\n",
       " ('members', 322),\n",
       " ('despite', 322),\n",
       " ('river', 322),\n",
       " ('diseases', 321),\n",
       " ('crop', 319),\n",
       " ('rural', 319),\n",
       " ('unhcr', 318),\n",
       " ('concern', 316),\n",
       " ('syria', 314),\n",
       " ('malaria', 312),\n",
       " ('highest', 312),\n",
       " ('provinces', 312),\n",
       " ('near', 309),\n",
       " ('impact', 306),\n",
       " ('lower', 305),\n",
       " ('control', 304),\n",
       " ('work', 303),\n",
       " ('others', 303),\n",
       " ('use', 303),\n",
       " ('found', 303),\n",
       " ('men', 303),\n",
       " ('primary', 301),\n",
       " ('large', 301),\n",
       " ('identified', 299),\n",
       " ('fuel', 298),\n",
       " ('capacity', 298),\n",
       " ('far', 296),\n",
       " ('overall', 293),\n",
       " ('locations', 291),\n",
       " ('almost', 291),\n",
       " ('management', 290),\n",
       " ('populations', 290),\n",
       " ('provided', 289),\n",
       " ('outcomes', 289),\n",
       " ('settlements', 288),\n",
       " ('persons', 287),\n",
       " ('system', 286),\n",
       " ('damage', 286),\n",
       " ('although', 286),\n",
       " ('dengue', 285),\n",
       " ('world', 284),\n",
       " ('centre', 284),\n",
       " ('family', 284),\n",
       " ('income', 284),\n",
       " ('figure', 283),\n",
       " ('insecure', 283),\n",
       " ('needed', 282),\n",
       " ('capital', 282),\n",
       " ('part', 282),\n",
       " ('attack', 282),\n",
       " ('facing', 282),\n",
       " ('unicef', 280),\n",
       " ('rice', 280),\n",
       " ('centres', 280),\n",
       " ('migrants', 278),\n",
       " ('workers', 277),\n",
       " ('source', 276),\n",
       " ('lean', 276),\n",
       " ('information', 275),\n",
       " ('fatality', 275),\n",
       " ('livelihood', 272),\n",
       " ('thousands', 272),\n",
       " ('fighting', 271),\n",
       " ('laboratory', 271),\n",
       " ('left', 269),\n",
       " ('resources', 269),\n",
       " ('reduced', 269),\n",
       " ('consumption', 268),\n",
       " ('official', 266),\n",
       " ('somali', 263),\n",
       " ('seven', 263),\n",
       " ('rohingya', 263),\n",
       " ('normal', 263),\n",
       " ('used', 262),\n",
       " ('darfur', 262),\n",
       " ('drinking', 261),\n",
       " ('positive', 260),\n",
       " ('improved', 259),\n",
       " ('livelihoods', 259),\n",
       " ('spread', 258),\n",
       " ('movement', 258),\n",
       " ('nigeria', 257),\n",
       " ('floods', 257),\n",
       " ('registered', 256),\n",
       " ('conducted', 256),\n",
       " ('sam', 256),\n",
       " ('remained', 256),\n",
       " ('staple', 256),\n",
       " ('power', 255),\n",
       " ('farmers', 253),\n",
       " ('late', 253),\n",
       " ('along', 252),\n",
       " ('borno', 251),\n",
       " ('village', 251),\n",
       " ('sexual', 251),\n",
       " ('th', 248),\n",
       " ('sector', 247),\n",
       " ('incidents', 247),\n",
       " ('civilian', 246),\n",
       " ('operations', 242),\n",
       " ('data', 242),\n",
       " ('countries', 241),\n",
       " ('hygiene', 241),\n",
       " ('governorates', 240),\n",
       " ('significantly', 240),\n",
       " ('second', 240),\n",
       " ('casualties', 239),\n",
       " ('somalia', 239),\n",
       " ('respectively', 237),\n",
       " ('working', 236),\n",
       " ('place', 235),\n",
       " ('yemen', 235),\n",
       " ('staff', 235),\n",
       " ('led', 234),\n",
       " ('hospitals', 234),\n",
       " ('nile', 234),\n",
       " ('another', 233),\n",
       " ('live', 231),\n",
       " ('wfp', 231),\n",
       " ('serious', 231),\n",
       " ('bank', 231),\n",
       " ('organization', 229),\n",
       " ('harvests', 229),\n",
       " ('army', 229),\n",
       " ('mainly', 229),\n",
       " ('started', 228),\n",
       " ('concerns', 228),\n",
       " ('says', 227),\n",
       " ('gam', 226),\n",
       " ('opportunities', 226),\n",
       " ('start', 226),\n",
       " ('pastoral', 225),\n",
       " ('located', 225),\n",
       " ('flooding', 225),\n",
       " ('relief', 225),\n",
       " ('cereal', 225),\n",
       " ('closed', 224),\n",
       " ('outbreaks', 223),\n",
       " ('urgent', 223),\n",
       " ('include', 222),\n",
       " ('infrastructure', 222),\n",
       " ('officials', 221),\n",
       " ('stressed', 221),\n",
       " ('often', 220),\n",
       " ('based', 219),\n",
       " ('rainy', 219),\n",
       " ('service', 219),\n",
       " ('dry', 219),\n",
       " ('us', 219),\n",
       " ('governorate', 218),\n",
       " ('reached', 218),\n",
       " ('open', 218),\n",
       " ('distribution', 218),\n",
       " ('individuals', 217),\n",
       " ('red', 216),\n",
       " ('zone', 216),\n",
       " ('targeted', 216),\n",
       " ('close', 216),\n",
       " ('able', 215),\n",
       " ('affecting', 215),\n",
       " ('agency', 215),\n",
       " ('resulted', 215),\n",
       " ('media', 215),\n",
       " ('numbers', 215),\n",
       " ('home', 215),\n",
       " ('given', 214),\n",
       " ('teachers', 214),\n",
       " ('arrivals', 214),\n",
       " ('made', 213),\n",
       " ('hit', 213),\n",
       " ('site', 212),\n",
       " ('civil', 209),\n",
       " ('long', 209),\n",
       " ('outside', 209),\n",
       " ('fled', 208),\n",
       " ('stocks', 208),\n",
       " ('sudanese', 208),\n",
       " ('improve', 208),\n",
       " ('kenya', 208),\n",
       " ('key', 207),\n",
       " ('eight', 207),\n",
       " ('efforts', 206),\n",
       " ('decline', 204),\n",
       " ('related', 204),\n",
       " ('reach', 204),\n",
       " ('prevalence', 203),\n",
       " ('ethiopia', 202),\n",
       " ('general', 202),\n",
       " ('items', 201),\n",
       " ('united', 200),\n",
       " ('risks', 200),\n",
       " ('meet', 199),\n",
       " ('zones', 199),\n",
       " ('famine', 198),\n",
       " ('actors', 198),\n",
       " ('require', 198),\n",
       " ('republic', 198),\n",
       " ('malnourished', 198),\n",
       " ('monday', 197),\n",
       " ('yet', 197),\n",
       " ('awd', 196),\n",
       " ('challenges', 195),\n",
       " ('students', 195),\n",
       " ('samples', 195),\n",
       " ('department', 195),\n",
       " ('shortage', 193),\n",
       " ('global', 193),\n",
       " ('aged', 193),\n",
       " ('trend', 192),\n",
       " ('war', 191),\n",
       " ('using', 191),\n",
       " ('urban', 191),\n",
       " ('decrease', 191),\n",
       " ('lives', 190),\n",
       " ('detention', 189),\n",
       " ('cluster', 189),\n",
       " ('seasonal', 189),\n",
       " ('greater', 188),\n",
       " ('today', 188),\n",
       " ('results', 187),\n",
       " ('back', 187),\n",
       " ('violations', 186),\n",
       " ('agriculture', 186),\n",
       " ('arrived', 185),\n",
       " ('adequate', 185),\n",
       " ('economic', 185),\n",
       " ('radio', 185),\n",
       " ('held', 184),\n",
       " ('wheat', 184),\n",
       " ('electricity', 184),\n",
       " ('healthcare', 184),\n",
       " ('recently', 183),\n",
       " ('plan', 183),\n",
       " ('shortages', 183),\n",
       " ('resulting', 183),\n",
       " ('lost', 183),\n",
       " ('mostly', 182),\n",
       " ('ghouta', 182),\n",
       " ('clashes', 182),\n",
       " ('road', 182),\n",
       " ('regional', 181),\n",
       " ('countrys', 181),\n",
       " ('latrines', 181),\n",
       " ('social', 181),\n",
       " ('followed', 181),\n",
       " ('development', 180),\n",
       " ('transmission', 180),\n",
       " ('cumulative', 180),\n",
       " ('suffering', 179),\n",
       " ('like', 178),\n",
       " ('campaign', 178),\n",
       " ('much', 178),\n",
       " ('throughout', 178),\n",
       " ('nations', 177),\n",
       " ('belowaverage', 177),\n",
       " ('rise', 177),\n",
       " ('restrictions', 176),\n",
       " ('negative', 176),\n",
       " ('hundreds', 175),\n",
       " ('returnees', 175),\n",
       " ('essential', 174),\n",
       " ('nine', 174),\n",
       " ('fire', 173),\n",
       " ('decreased', 173),\n",
       " ('funding', 173),\n",
       " ('law', 173),\n",
       " ('ensure', 172),\n",
       " ('taken', 172),\n",
       " ('required', 172),\n",
       " ('fall', 171),\n",
       " ('safety', 171),\n",
       " ('myanmar', 171),\n",
       " ('minister', 171),\n",
       " ('pregnant', 171),\n",
       " ('labor', 171),\n",
       " ('internally', 170),\n",
       " ('analysis', 170),\n",
       " ('remaining', 170),\n",
       " ('sorghum', 170),\n",
       " ('observed', 170),\n",
       " ('latest', 170),\n",
       " ('order', 169),\n",
       " ('roads', 169),\n",
       " ('diarrhoea', 168),\n",
       " ('earlier', 168),\n",
       " ('centers', 167),\n",
       " ('victims', 167),\n",
       " ('say', 166),\n",
       " ('wednesday', 166),\n",
       " ('measures', 166),\n",
       " ('occurred', 166),\n",
       " ('assessed', 166),\n",
       " ('northeast', 165),\n",
       " ('leading', 165),\n",
       " ('unable', 165),\n",
       " ('returned', 165),\n",
       " ('daily', 164),\n",
       " ('injuries', 164),\n",
       " ('young', 163),\n",
       " ('demand', 163),\n",
       " ('leaving', 162),\n",
       " ('experiencing', 162),\n",
       " ('next', 162),\n",
       " ('rain', 162),\n",
       " ('presence', 162),\n",
       " ('quality', 161),\n",
       " ('hours', 161),\n",
       " ('third', 161),\n",
       " ('interventions', 160),\n",
       " ('coping', 160),\n",
       " ('slightly', 159),\n",
       " ('increases', 159),\n",
       " ('take', 159),\n",
       " ('organizations', 159),\n",
       " ('boys', 159),\n",
       " ('monitoring', 159),\n",
       " ('detained', 159),\n",
       " ('vaccination', 158),\n",
       " ('agencies', 158),\n",
       " ('help', 158),\n",
       " ('rapid', 158),\n",
       " ('away', 157),\n",
       " ('oromia', 157),\n",
       " ('trade', 157),\n",
       " ('tuesday', 156),\n",
       " ('either', 156),\n",
       " ('seen', 156),\n",
       " ('imports', 155),\n",
       " ('survey', 155),\n",
       " ('journalists', 155),\n",
       " ('soldiers', 155),\n",
       " ('providing', 155),\n",
       " ('cross', 154),\n",
       " ('go', 154),\n",
       " ('cash', 154),\n",
       " ('provision', 153),\n",
       " ('probable', 153),\n",
       " ('flood', 152),\n",
       " ('possible', 152),\n",
       " ('difficult', 152),\n",
       " ('cost', 151),\n",
       " ('niger', 151),\n",
       " ('temporary', 151),\n",
       " ('night', 150),\n",
       " ('important', 150),\n",
       " ('political', 149),\n",
       " ('africa', 148),\n",
       " ('epidemic', 147),\n",
       " ('active', 147),\n",
       " ('lgas', 147),\n",
       " ('indicated', 146),\n",
       " ('office', 146),\n",
       " ('former', 146),\n",
       " ('date', 146),\n",
       " ('libya', 146),\n",
       " ('surveillance', 145),\n",
       " ('kordofan', 145),\n",
       " ('diphtheria', 145),\n",
       " ('different', 144),\n",
       " ('experienced', 144),\n",
       " ('mosul', 144),\n",
       " ('pasture', 144),\n",
       " ('sudans', 144),\n",
       " ('equatoria', 144),\n",
       " ('leave', 143),\n",
       " ('see', 143),\n",
       " ('rakhine', 143),\n",
       " ('receive', 143),\n",
       " ('associated', 142),\n",
       " ('indicate', 142),\n",
       " ('small', 142),\n",
       " ('better', 142),\n",
       " ('classified', 141),\n",
       " ('arrested', 141),\n",
       " ('coming', 141),\n",
       " ('medicines', 141),\n",
       " ('process', 141),\n",
       " ('buildings', 140),\n",
       " ('inside', 140),\n",
       " ('sea', 139),\n",
       " ('thursday', 139),\n",
       " ('widespread', 139),\n",
       " ('operation', 139),\n",
       " ('gaps', 139),\n",
       " ('learning', 139),\n",
       " ('newly', 139),\n",
       " ('times', 139),\n",
       " ('largest', 139),\n",
       " ('friday', 139),\n",
       " ('settlement', 138),\n",
       " ('life', 138),\n",
       " ('facility', 138),\n",
       " ('disaster', 138),\n",
       " ('carried', 137),\n",
       " ('commodities', 137),\n",
       " ('congo', 137),\n",
       " ('uganda', 137),\n",
       " ('quarter', 137),\n",
       " ('make', 136),\n",
       " ('opposition', 136),\n",
       " ('watch', 136),\n",
       " ('every', 136),\n",
       " ('began', 136),\n",
       " ('sunday', 136),\n",
       " ('impacted', 136),\n",
       " ('stable', 136),\n",
       " ('immediate', 135),\n",
       " ('though', 135),\n",
       " ('become', 135),\n",
       " ('prevent', 135),\n",
       " ('minimal', 135),\n",
       " ('good', 135),\n",
       " ('inadequate', 134),\n",
       " ('watery', 134),\n",
       " ('msf', 134),\n",
       " ('cattle', 134),\n",
       " ('head', 134),\n",
       " ('influx', 134),\n",
       " ('tested', 134),\n",
       " ('programme', 134),\n",
       " ('move', 133),\n",
       " ('body', 133),\n",
       " ('added', 133),\n",
       " ('building', 133),\n",
       " ('structures', 133),\n",
       " ('absence', 132),\n",
       " ('center', 132),\n",
       " ('issues', 132),\n",
       " ('released', 132),\n",
       " ('systems', 131),\n",
       " ('chronic', 131),\n",
       " ('juba', 131),\n",
       " ('towns', 131),\n",
       " ('declined', 131),\n",
       " ('causing', 130),\n",
       " ('ground', 130),\n",
       " ('net', 130),\n",
       " ('bangladesh', 130),\n",
       " ('oil', 130),\n",
       " ('put', 129),\n",
       " ('old', 129),\n",
       " ('coordination', 128),\n",
       " ('jerusalem', 128),\n",
       " ('fear', 128),\n",
       " ('private', 128),\n",
       " ('female', 128),\n",
       " ('team', 127),\n",
       " ('coverage', 127),\n",
       " ('attacked', 127),\n",
       " ('white', 127),\n",
       " ('extremely', 127),\n",
       " ('issued', 126),\n",
       " ('president', 126),\n",
       " ('teams', 126),\n",
       " ('projected', 126),\n",
       " ('virus', 126),\n",
       " ('wau', 126),\n",
       " ('force', 125),\n",
       " ('e', 125),\n",
       " ('car', 125),\n",
       " ('various', 125),\n",
       " ('lassa', 125),\n",
       " ('get', 125),\n",
       " ('action', 124),\n",
       " ('line', 124),\n",
       " ('neighbouring', 124),\n",
       " ('status', 124),\n",
       " ('israel', 124),\n",
       " ('meningitis', 124),\n",
       " ('foods', 124),\n",
       " ('asylum', 123),\n",
       " ('killing', 123),\n",
       " ('field', 122),\n",
       " ('means', 122),\n",
       " ('existing', 122),\n",
       " ('scale', 122),\n",
       " ('present', 122),\n",
       " ('onset', 122),\n",
       " ('physical', 121),\n",
       " ('director', 121),\n",
       " ('proportion', 121),\n",
       " ('terms', 121),\n",
       " ('hunger', 121),\n",
       " ('democratic', 121),\n",
       " ('council', 121),\n",
       " ('labour', 121),\n",
       " ('gap', 121),\n",
       " ('extreme', 121),\n",
       " ('declared', 120),\n",
       " ('infection', 120),\n",
       " ('airstrikes', 120),\n",
       " ('committee', 120),\n",
       " ('explosive', 120),\n",
       " ('dead', 119),\n",
       " ('similar', 119),\n",
       " ('secondary', 119),\n",
       " ('estimates', 119),\n",
       " ('domestic', 119),\n",
       " ('points', 119),\n",
       " ('worst', 118),\n",
       " ('took', 118),\n",
       " ('construction', 118),\n",
       " ('potential', 118),\n",
       " ('moderate', 118),\n",
       " ('weather', 118),\n",
       " ('gas', 118),\n",
       " ('receiving', 118),\n",
       " ('priority', 117),\n",
       " ('delivery', 117),\n",
       " ('doctors', 117),\n",
       " ('known', 117),\n",
       " ('grain', 117),\n",
       " ('enough', 117),\n",
       " ('point', 117),\n",
       " ('milk', 117),\n",
       " ('southeastern', 117),\n",
       " ('particular', 116),\n",
       " ('returning', 116),\n",
       " ('materials', 116),\n",
       " ('challenge', 116),\n",
       " ('ago', 116),\n",
       " ('ending', 116),\n",
       " ('functioning', 116),\n",
       " ('iom', 116),\n",
       " ('aleppo', 115),\n",
       " ('cause', 115),\n",
       " ('mortality', 115),\n",
       " ('travel', 115),\n",
       " ('statement', 115),\n",
       " ('burundi', 115),\n",
       " ('loss', 115),\n",
       " ('generally', 115),\n",
       " ('accessing', 115),\n",
       " ('turkey', 115),\n",
       " ('mission', 115),\n",
       " ('figures', 115),\n",
       " ('plague', 115),\n",
       " ('threshold', 114),\n",
       " ('african', 114),\n",
       " ('lead', 114),\n",
       " ('infected', 114),\n",
       " ('factors', 114),\n",
       " ('announced', 114),\n",
       " ('respondents', 114),\n",
       " ('pressure', 113),\n",
       " ('property', 113),\n",
       " ('km', 113),\n",
       " ('table', 113),\n",
       " ('migration', 113),\n",
       " ('supported', 113),\n",
       " ('traders', 112),\n",
       " ('flour', 112),\n",
       " ('person', 112),\n",
       " ('poverty', 112),\n",
       " ('showed', 112),\n",
       " ('sufficient', 112),\n",
       " ('financial', 112),\n",
       " ('deterioration', 111),\n",
       " ('insufficient', 111),\n",
       " ('damascus', 111),\n",
       " ('search', 111),\n",
       " ('little', 111),\n",
       " ('sustained', 111),\n",
       " ('largely', 111),\n",
       " ('common', 111),\n",
       " ('reduce', 111),\n",
       " ('considered', 110),\n",
       " ('partially', 110),\n",
       " ('air', 110),\n",
       " ('must', 110),\n",
       " ('experience', 110),\n",
       " ('makeshift', 110),\n",
       " ('money', 110),\n",
       " ('yobe', 110),\n",
       " ('authority', 110),\n",
       " ('abuse', 110),\n",
       " ('places', 109),\n",
       " ('included', 109),\n",
       " ('costs', 109),\n",
       " ('reduction', 109),\n",
       " ('torture', 109),\n",
       " ('deteriorate', 109),\n",
       " ('strip', 109),\n",
       " ('relatively', 109),\n",
       " ('show', 109),\n",
       " ('towards', 109),\n",
       " ('alone', 109),\n",
       " ('citizens', 109),\n",
       " ('address', 109),\n",
       " ('leaders', 109),\n",
       " ('tonnes', 109),\n",
       " ('legal', 108),\n",
       " ('equipment', 108),\n",
       " ('shows', 108),\n",
       " ('meanwhile', 108),\n",
       " ('purchasing', 108),\n",
       " ('governments', 107),\n",
       " ('documented', 107),\n",
       " ('news', 107),\n",
       " ('freedom', 107),\n",
       " ('c', 107),\n",
       " ('rising', 106),\n",
       " ('longer', 106),\n",
       " ('improvement', 106),\n",
       " ('incidence', 106),\n",
       " ('evacuation', 106),\n",
       " ('feeding', 106),\n",
       " ('admitted', 106),\n",
       " ('saturday', 106),\n",
       " ('making', 106),\n",
       " ('forecast', 106),\n",
       " ('set', 106),\n",
       " ('problem', 106),\n",
       " ('jonglei', 105),\n",
       " ('chad', 105),\n",
       " ('upper', 105),\n",
       " ('growing', 104),\n",
       " ('fews', 104),\n",
       " ('disrupted', 104),\n",
       " ('treated', 104),\n",
       " ('wounded', 104),\n",
       " ('male', 104),\n",
       " ('threats', 103),\n",
       " ('consecutive', 103),\n",
       " ('alert', 103),\n",
       " ('palestinians', 103),\n",
       " ('noted', 103),\n",
       " ('court', 103),\n",
       " ('issue', 103),\n",
       " ('planting', 103),\n",
       " ('came', 103),\n",
       " ('way', 103),\n",
       " ('surveyed', 103),\n",
       " ('targeting', 103),\n",
       " ('bahr', 103),\n",
       " ('minimum', 102),\n",
       " ('psychosocial', 102),\n",
       " ('persist', 102),\n",
       " ('cut', 102),\n",
       " ('afghanistan', 102),\n",
       " ('fields', 102),\n",
       " ('island', 102),\n",
       " ('islamic', 102),\n",
       " ('multiple', 102),\n",
       " ('mental', 102),\n",
       " ('reasons', 102),\n",
       " ('fiveyear', 102),\n",
       " ('coast', 102),\n",
       " ('strategies', 102),\n",
       " ('armyworm', 102),\n",
       " ('unity', 102),\n",
       " ('turkana', 101),\n",
       " ('commissioner', 101),\n",
       " ('earthquake', 101),\n",
       " ('housing', 101),\n",
       " ('territory', 101),\n",
       " ('suffer', 101),\n",
       " ('destruction', 101),\n",
       " ('short', 100),\n",
       " ('threat', 100),\n",
       " ('constraints', 100),\n",
       " ('nearby', 100),\n",
       " ('established', 100),\n",
       " ('practices', 100),\n",
       " ('therefore', 100),\n",
       " ('de', 100),\n",
       " ('strong', 100),\n",
       " ('clean', 100),\n",
       " ('strike', 100),\n",
       " ('cooking', 100),\n",
       " ('thus', 99),\n",
       " ('imported', 99),\n",
       " ('occupied', 99),\n",
       " ('ten', 99),\n",
       " ('direct', 99),\n",
       " ('adamawa', 99),\n",
       " ('recruitment', 99),\n",
       " ('boko', 98),\n",
       " ('rape', 98),\n",
       " ('crossing', 98),\n",
       " ('urgently', 98),\n",
       " ('yesterday', 98),\n",
       " ('condition', 98),\n",
       " ('commercial', 97),\n",
       " ('flee', 97),\n",
       " ('missing', 97),\n",
       " ('affect', 97),\n",
       " ('trends', 97),\n",
       " ('monthly', 97),\n",
       " ('includes', 96),\n",
       " ('lebanon', 96),\n",
       " ('hepatitis', 96),\n",
       " ('cover', 96),\n",
       " ('weekly', 96),\n",
       " ('assessments', 95),\n",
       " ('regular', 95),\n",
       " ('network', 95),\n",
       " ('brought', 95),\n",
       " ('opened', 95),\n",
       " ('personnel', 95),\n",
       " ('aboveaverage', 95),\n",
       " ('come', 95),\n",
       " ('called', 95),\n",
       " ('localized', 95),\n",
       " ('peak', 95),\n",
       " ('northwest', 95),\n",
       " ('transport', 95),\n",
       " ('mass', 95),\n",
       " ('losses', 94),\n",
       " ('kits', 94),\n",
       " ('drc', 94),\n",
       " ('locality', 94),\n",
       " ('infections', 94),\n",
       " ('fleeing', 94),\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dictionary.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in dictionary:  [('<', 41249), ('num', 41249), ('>', 41249), ('<oov>', 33980), ('cases', 4758), ('food', 4138), ('people', 3799), ('reported', 3676), ('children', 2894), ('areas', 2551)]\n",
      "Least common words in dictionary:  [('banking', 5), ('averaged', 5), ('yearthe', 5), ('droughtinduced', 5), ('secondlargest', 5), ('jilaal', 5), ('oppression', 5), ('teenage', 5), ('slum', 5), ('usaidfunded', 5)]\n"
     ]
    }
   ],
   "source": [
    "# print most common words in dictionary\n",
    "print('Most common words in dictionary: ', sorted(dictionary.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "print('Least common words in dictionary: ', sorted(dictionary.items(), key=lambda x: x[1], reverse=False)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Creating Sentence Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define term frequency\n",
    "def term_frequency(word, tokenized_text):\n",
    "    \"\"\"\n",
    "    Calculate term frequency\n",
    "    :param word: word, str\n",
    "    :param tokenized_text: tokenized text, list\n",
    "    :return: term frequency, float\n",
    "    \"\"\"\n",
    "    return np.log(tokenized_text.count(word) + 1)\n",
    "\n",
    "# define inverse document frequency\n",
    "def inverse_document_frequency(word, tokenized_text_list):\n",
    "    \"\"\"\n",
    "    Calculate inverse document frequency\n",
    "    :param word: word, str\n",
    "    :param tokenized_text_list: list of tokenized texts, list\n",
    "    :return: inverse document frequency, float\n",
    "    \"\"\"\n",
    "    num_texts_containing_word = sum(word in tokenized_text for tokenized_text in tokenized_text_list)\n",
    "    return np.log(len(tokenized_text_list) / (1 + num_texts_containing_word))\n",
    "\n",
    "# define tf-idf\n",
    "def tf_idf(word, tokenized_text, tokenized_text_list):\n",
    "    \"\"\"\n",
    "    Calculate tf-idf\n",
    "    :param word: word, str\n",
    "    :param tokenized_text: tokenized text, list\n",
    "    :param tokenized_text_list: list of tokenized texts, list\n",
    "    :return: tf-idf, float\n",
    "    \"\"\"\n",
    "    return term_frequency(word, tokenized_text) * inverse_document_frequency(word, tokenized_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF of word \"num\" in text 1:  0.23354538677276349\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print('TF-IDF of word \"num\" in text 1: ', tf_idf('num', train_df['tokenized_text'][0], train_df['tokenized_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# create tf-idf matrix\n",
    "def create_tf_idf_matrix(tokenized_text_list, dictionary, idf):\n",
    "    \"\"\"\n",
    "    Create tf-idf matrix\n",
    "    :param tokenized_text_list: list of tokenized texts, list\n",
    "    :param dictionary: dictionary, dict\n",
    "    :param idf: idf for all words in dictionary, dict\n",
    "    :return: tf-idf matrix, np.array\n",
    "    \"\"\"\n",
    "    tf_idf_matrix = np.zeros((len(tokenized_text_list), len(dictionary)))\n",
    "    for i, tokenized_text in enumerate(tqdm(tokenized_text_list, total=len(tokenized_text_list))):\n",
    "        for j, word in enumerate(dictionary):\n",
    "            tf_idf_matrix[i, j] = term_frequency(word, tokenized_text) * idf[word]\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54fcc4b9c4b47e4a6bed2ec1953a568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define idf for all words in dictionary using vectorization, from scratch\n",
    "def idf_all_words(dictionary, tokenized_text_list):\n",
    "    \"\"\"\n",
    "    Calculate idf for all words in dictionary using vectorization, from scratch\n",
    "    :param dictionary: dictionary, dict\n",
    "    :param tokenized_text_list: list of tokenized texts, list\n",
    "    :return: idf for all words in dictionary, dict\n",
    "    \"\"\"\n",
    "    return {word: inverse_document_frequency(word, tokenized_text_list) for word in tqdm(dictionary, total=len(dictionary))}\n",
    "\n",
    "idf = idf_all_words(dictionary, train_df['tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b416d50de9bc42dea2f16f3db8deefdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf_idf_matrix_train = create_tf_idf_matrix(train_df['tokenized_text'], dictionary, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928f459b993e4b11a85f63fe2849aaa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2595 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95eed59ed984613bd471ae0529456dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2596 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf_idf_matrix_test = create_tf_idf_matrix(test_df['tokenized_text'], dictionary, idf)\n",
    "tf_idf_matrix_val = create_tf_idf_matrix(val_df['tokenized_text'], dictionary, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(tf_idf_matrix_train, open('tf_idf_matrix_train.pkl', 'wb'))\n",
    "pkl.dump(tf_idf_matrix_test, open('tf_idf_matrix_test.pkl', 'wb'))\n",
    "pkl.dump(tf_idf_matrix_val, open('tf_idf_matrix_val.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23354539, 0.23354539, 0.23354539, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.70063616, 0.70063616, 0.70063616, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.46709077, 0.46709077, 0.46709077, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tf-idf matrix:  (12110, 8195)\n",
      "Average tf-idf value of word \"num\" in text 1:  0.23354538677276349\n"
     ]
    }
   ],
   "source": [
    "# print shape of tf-idf matrix\n",
    "selected_word = list(dictionary.keys())[1]\n",
    "print('Shape of tf-idf matrix: ', tf_idf_matrix_train.shape)\n",
    "print(f'Average tf-idf value of word \"{selected_word}\" in text 1: ', tf_idf_matrix_train[0, list(dictionary.keys()).index(selected_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dictionary.keys()).index(list(dictionary.keys())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define BM25\n",
    "avg_len_text = np.mean([len(tokenized_text) for tokenized_text in train_df['tokenized_text']])\n",
    "\n",
    "def bm25(word, tokenized_text, tokenized_text_list, idf=idf, k=1.4, b=0.9):\n",
    "    \"\"\"\n",
    "    Calculate BM25\n",
    "    :param word: word, str\n",
    "    :param tokenized_text: tokenized text, list\n",
    "    :param tokenized_text_list: list of tokenized texts, list\n",
    "    :param k: k, float\n",
    "    :param b: b, float\n",
    "    :return: BM25, float\n",
    "    \"\"\"\n",
    "    return idf[word] * (term_frequency(word, tokenized_text) * (k + 1)) / (term_frequency(word, tokenized_text) + k * (1 - b + b * len(tokenized_text) / avg_len_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 of word \"num\" in text 1:  0.44945051332030167\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "print('BM25 of word \"num\" in text 1: ', bm25('num', train_df['tokenized_text'][0], train_df['tokenized_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<': 0.3369347713195608,\n",
       " 'num': 0.3369347713195608,\n",
       " '>': 0.3369347713195608,\n",
       " 'cases': 1.9106992430122622,\n",
       " 'food': 1.824664905670459,\n",
       " 'people': 1.5396746248843896,\n",
       " 'reported': 1.7587832009864202,\n",
       " 'children': 1.9106992430122622,\n",
       " 'areas': 1.917418193261007,\n",
       " 'water': 2.230898358034633,\n",
       " 'health': 2.1094496603732606,\n",
       " 'said': 2.158988913753382,\n",
       " 'access': 2.194667980339382,\n",
       " 'including': 2.0972708900869828,\n",
       " 'per': 2.5938518928472116,\n",
       " 'also': 2.101314022279339,\n",
       " 'percent': 2.688830635870068,\n",
       " 'due': 2.1726729587538363,\n",
       " 'affected': 2.2653036279568903,\n",
       " 'since': 2.1618542452266687,\n",
       " 'deaths': 2.5687551037609375,\n",
       " 'number': 2.3691625755191317,\n",
       " 'households': 2.6851920630261596,\n",
       " 'total': 2.4673896266185795,\n",
       " 'humanitarian': 2.52862300233462,\n",
       " 'two': 2.389671542240758,\n",
       " 'security': 2.500049629890564,\n",
       " 'million': 2.6460179145628824,\n",
       " 'week': 2.789745801714046,\n",
       " 'one': 2.4189240850781957,\n",
       " 'prices': 2.9325365197513653,\n",
       " 'new': 2.5327723858814313,\n",
       " 'cent': 2.97691781264175,\n",
       " 'refugees': 2.7593000351798818,\n",
       " 'state': 2.6128150935549677,\n",
       " 'country': 2.5183242501340453,\n",
       " 'government': 2.5327723858814313,\n",
       " 'year': 2.569833270981283,\n",
       " 'displaced': 2.6460179145628824,\n",
       " 'assistance': 2.630997412638158,\n",
       " 'according': 2.4673896266185795,\n",
       " 'cholera': 2.9202097072707067,\n",
       " 'need': 2.5927475305041585,\n",
       " 'phase': 3.1752501672596725,\n",
       " 'suspected': 2.8665455655334795,\n",
       " 'south': 2.7221876507027547,\n",
       " 'high': 2.623001938861961,\n",
       " 'acute': 2.819761697654312,\n",
       " 'ipc': 3.2342703456587967,\n",
       " 'may': 2.690046441490958,\n",
       " 'women': 2.8521360943133276,\n",
       " 'many': 2.6161991915392084,\n",
       " 'region': 2.756695867041494,\n",
       " 'season': 2.7951366503489226,\n",
       " 'last': 2.6241402429120213,\n",
       " 'outbreak': 2.814236821722342,\n",
       " 'services': 2.8032778079326226,\n",
       " 'area': 2.728488868779484,\n",
       " 'three': 2.6755534341883913,\n",
       " 'population': 2.7109445591285746,\n",
       " 'situation': 2.7272254447327122,\n",
       " 'emergency': 2.761911002720602,\n",
       " 'malnutrition': 3.0864288350248033,\n",
       " 'families': 2.9624364654470394,\n",
       " 'idps': 3.073850052817943,\n",
       " 'years': 2.773745460367605,\n",
       " 'january': 2.8309038742075536,\n",
       " 'local': 2.769785059151508,\n",
       " 'average': 3.001529391238317,\n",
       " 'increased': 2.8183776143883734,\n",
       " 'poor': 2.9403186601934204,\n",
       " 'confirmed': 3.0404843589741426,\n",
       " 'insecurity': 2.943448553202348,\n",
       " 'increase': 2.833708925135162,\n",
       " 'case': 3.151811594287655,\n",
       " 'march': 2.946588273207016,\n",
       " 'lack': 2.8782245303976257,\n",
       " 'crisis': 2.900497166006749,\n",
       " 'across': 2.81011310453848,\n",
       " 'expected': 3.0491574402275714,\n",
       " 'risk': 2.917151600911886,\n",
       " 'needs': 2.9202097072707067,\n",
       " 'facilities': 3.077427874165827,\n",
       " 'among': 2.8989967906315144,\n",
       " 'city': 3.0474167957497875,\n",
       " 'schools': 3.305962274114913,\n",
       " 'least': 2.8796940383769853,\n",
       " 'camps': 3.117652675476336,\n",
       " 'support': 2.9624364654470394,\n",
       " 'conflict': 3.0048571813309914,\n",
       " 'school': 3.3241445931981035,\n",
       " 'estimated': 2.9340881104427843,\n",
       " 'however': 2.8593148760403335,\n",
       " 'conditions': 2.9850545540348117,\n",
       " 'production': 3.1891807407956194,\n",
       " 'levels': 3.0596654178259866,\n",
       " 'compared': 3.009869723154536,\n",
       " 'medical': 3.219701929830506,\n",
       " 'months': 2.9592466700789393,\n",
       " 'february': 3.121390997586943,\n",
       " 'communities': 3.136485623809428,\n",
       " 'five': 2.9720673585080006,\n",
       " 'april': 3.0828187228007033,\n",
       " 'national': 3.0065252384316885,\n",
       " 'remain': 2.9998696398199525,\n",
       " 'violence': 3.136485623809428,\n",
       " 'continue': 2.998212638612323,\n",
       " 'education': 3.3663054040223814,\n",
       " 'likely': 3.1972290739784475,\n",
       " 'authorities': 3.0267620167190414,\n",
       " 'camp': 3.4003719585859877,\n",
       " 'response': 3.157619935883402,\n",
       " 'period': 3.0900520273942234,\n",
       " 'rate': 3.2554575788782407,\n",
       " 'well': 3.013225431001508,\n",
       " 'limited': 3.1028375896911955,\n",
       " 'regions': 3.2342703456587967,\n",
       " 'still': 3.0404843589741426,\n",
       " 'human': 3.2448078509615823,\n",
       " 'armed': 3.2448078509615823,\n",
       " 'june': 3.2053427087526174,\n",
       " 'rights': 3.299228241933569,\n",
       " 'protection': 3.266221945465399,\n",
       " 'care': 3.4279772246778766,\n",
       " 'october': 3.230086239136223,\n",
       " 'states': 3.4668926409275502,\n",
       " 'central': 3.1307984046888384,\n",
       " 'severe': 3.1139282763853533,\n",
       " 'december': 3.230086239136223,\n",
       " 'killed': 3.24692874253072,\n",
       " 'september': 3.2511840681008586,\n",
       " 'refugee': 3.340329917619121,\n",
       " 'report': 3.2342703456587967,\n",
       " 'rains': 3.3639159166250003,\n",
       " 'sites': 3.7046933500417336,\n",
       " 'forces': 3.2903194970444596,\n",
       " 'maize': 3.6428850626698575,\n",
       " 'community': 3.2792940270327517,\n",
       " 'groups': 3.340329917619121,\n",
       " 'four': 3.217637945609655,\n",
       " 'main': 3.2384720325124965,\n",
       " 'districts': 3.3711015762858745,\n",
       " 'northern': 3.2405795148520613,\n",
       " 'livestock': 3.593644346566694,\n",
       " 'east': 3.3711015762858745,\n",
       " 'district': 3.3929736511045427,\n",
       " 'ongoing': 3.1952109098222103,\n",
       " 'province': 3.388071680504336,\n",
       " 'living': 3.2903194970444596,\n",
       " 'west': 3.3687006147483363,\n",
       " 'hospital': 3.6210433207548087,\n",
       " 'recorded': 3.3663054040223814,\n",
       " 'sudan': 3.4828929822739916,\n",
       " 'first': 3.2705603470639972,\n",
       " 'north': 3.3807634871976115,\n",
       " 'following': 3.228000732645201,\n",
       " 'civilians': 3.4668926409275502,\n",
       " 'august': 3.3759208627218236,\n",
       " 'around': 3.274917652432953,\n",
       " 'currently': 3.257601202421492,\n",
       " 'eastern': 3.3831936220509036,\n",
       " 'month': 3.340329917619121,\n",
       " 'reports': 3.2925392537827727,\n",
       " 'parts': 3.3241445931981035,\n",
       " 'nutrition': 3.5611451791737396,\n",
       " 'international': 3.3450028233185134,\n",
       " 'markets': 3.6210433207548087,\n",
       " 'rainfall': 3.5787409410641193,\n",
       " 'particularly': 3.2814894175961875,\n",
       " 'recent': 3.2554575788782407,\n",
       " 'time': 3.31047695446944,\n",
       " 'aid': 3.515682805096982,\n",
       " 'remains': 3.3037125543808976,\n",
       " 'displacement': 3.395433676945405,\n",
       " 'result': 3.3150121096348313,\n",
       " 'al': 3.8923984999191608,\n",
       " 'disease': 3.4485435022593536,\n",
       " 'early': 3.3929736511045427,\n",
       " 'drought': 3.453751847366492,\n",
       " 'several': 3.3759208627218236,\n",
       " 'partners': 3.4254359272492043,\n",
       " 'july': 3.4153348312627,\n",
       " 'southern': 3.4128254196572745,\n",
       " 'patients': 3.694676571798262,\n",
       " 'shelter': 3.538155660949041,\n",
       " 'weeks': 3.4305249967566755,\n",
       " 'vulnerable': 3.3929736511045427,\n",
       " 'november': 3.5495843567726637,\n",
       " 'town': 3.5495843567726637,\n",
       " 'market': 3.639735453766961,\n",
       " 'activities': 3.485584773939703,\n",
       " 'treatment': 3.4964249884925676,\n",
       " 'basic': 3.422901071646016,\n",
       " 'continued': 3.378339243586105,\n",
       " 'residents': 3.538155660949041,\n",
       " 'assessment': 3.504632968910397,\n",
       " 'idp': 3.714811480207318,\n",
       " 'low': 3.4668926409275502,\n",
       " 'could': 3.435640097423446,\n",
       " 'houses': 3.5906458435704374,\n",
       " 'un': 3.5611451791737396,\n",
       " 'girls': 3.714811480207318,\n",
       " 'border': 3.5846756765839336,\n",
       " 'supplies': 3.510142624721367,\n",
       " 'military': 3.593644346566694,\n",
       " 'israeli': 3.8923984999191608,\n",
       " 'level': 3.518464448058859,\n",
       " 'majority': 3.4279772246778766,\n",
       " 'ministry': 3.4991535031457714,\n",
       " 'already': 3.4407814969238646,\n",
       " 'attacks': 3.5906458435704374,\n",
       " 'death': 3.5268561056951073,\n",
       " 'day': 3.5324899234133635,\n",
       " 'el': 3.8449587748476004,\n",
       " 'days': 3.5268561056951073,\n",
       " 'continues': 3.453751847366492,\n",
       " 'harvest': 3.6523938506388847,\n",
       " 'six': 3.5073840022822873,\n",
       " 'forced': 3.5324899234133635,\n",
       " 'homes': 3.575786729166688,\n",
       " 'measles': 3.8964553006147753,\n",
       " 'damaged': 3.6057290857817663,\n",
       " 'gaza': 3.916989903056483,\n",
       " 'higher': 3.581703906194776,\n",
       " 'host': 3.655583646006985,\n",
       " 'price': 3.8220570105609157,\n",
       " 'reportedly': 3.6210433207548087,\n",
       " 'past': 3.5129088782142572,\n",
       " 'public': 3.5611451791737396,\n",
       " 'palestinian': 3.8923984999191608,\n",
       " 'available': 3.6118266656498847,\n",
       " 'even': 3.504632968910397,\n",
       " 'land': 3.714811480207318,\n",
       " 'syrian': 3.8220570105609157,\n",
       " 'supply': 3.6241345133244813,\n",
       " 'shelters': 3.8803259186848913,\n",
       " 'agricultural': 3.698004361890937,\n",
       " 'within': 3.5966518676306496,\n",
       " 'villages': 3.7598797656090244,\n",
       " 'county': 3.8566093920675755,\n",
       " 'fever': 3.9128491103904515,\n",
       " 'destroyed': 3.630345713417122,\n",
       " 'end': 3.5353187796138412,\n",
       " 'previous': 3.608773228162994,\n",
       " 'told': 3.5846756765839336,\n",
       " 'increasing': 3.5728412189369307,\n",
       " 'current': 3.575786729166688,\n",
       " 'without': 3.5669760994845325,\n",
       " 'group': 3.674939088959941,\n",
       " 'household': 3.678201734594757,\n",
       " 'crops': 3.7705750547257724,\n",
       " 'child': 3.7886587301590677,\n",
       " 'nearly': 3.6118266656498847,\n",
       " 'age': 3.7457950257272854,\n",
       " 'face': 3.6492141977215047,\n",
       " 'died': 3.7182070692084563,\n",
       " 'counties': 3.900528626002411,\n",
       " 'injured': 3.8070754569452987,\n",
       " 'sources': 3.725033034278856,\n",
       " 'especially': 3.593644346566694,\n",
       " 'would': 3.725033034278856,\n",
       " 'major': 3.665214539067946,\n",
       " 'return': 3.7777693303597997,\n",
       " 'police': 3.8843339400824304,\n",
       " 'availability': 3.752812598385932,\n",
       " 'reporting': 3.7046933500417336,\n",
       " 'severely': 3.698004361890937,\n",
       " 'safe': 3.735360148434706,\n",
       " 'sanitation': 3.7598797656090244,\n",
       " 'significant': 3.6428850626698575,\n",
       " 'approximately': 3.7182070692084563,\n",
       " 'cfr': 3.9946150650870194,\n",
       " 'wash': 3.981251837274852,\n",
       " 'beginning': 3.6428850626698575,\n",
       " 'critical': 3.752812598385932,\n",
       " 'western': 3.735360148434706,\n",
       " 'received': 3.714811480207318,\n",
       " 'heavy': 3.7284635693756454,\n",
       " 'rates': 3.8220570105609157,\n",
       " 'addition': 3.646044622960226,\n",
       " 'half': 3.6716870535735637,\n",
       " 'additional': 3.678201734594757,\n",
       " 'caused': 3.665214539067946,\n",
       " 'less': 3.7563399389039,\n",
       " 'provide': 3.688054031037769,\n",
       " 'members': 3.7813859708299877,\n",
       " 'despite': 3.6587836487376557,\n",
       " 'river': 3.84110520553161,\n",
       " 'diseases': 3.8145381781468886,\n",
       " 'crop': 3.900528626002411,\n",
       " 'rural': 3.833442332786041,\n",
       " 'unhcr': 3.981251837274852,\n",
       " 'concern': 3.731905913566618,\n",
       " 'syria': 3.8763338974153543,\n",
       " 'malaria': 4.176040162833937,\n",
       " 'highest': 3.7457950257272854,\n",
       " 'provinces': 3.8964553006147753,\n",
       " 'near': 3.7598797656090244,\n",
       " 'impact': 3.7705750547257724,\n",
       " 'lower': 3.872357749035715,\n",
       " 'control': 3.803364877548763,\n",
       " 'work': 3.8763338974153543,\n",
       " 'others': 3.735360148434706,\n",
       " 'use': 3.7886587301590677,\n",
       " 'found': 3.8763338974153543,\n",
       " 'men': 3.942201322402979,\n",
       " 'primary': 3.8843339400824304,\n",
       " 'large': 3.7669972333778885,\n",
       " 'identified': 3.8372664292244445,\n",
       " 'fuel': 4.149513408500508,\n",
       " 'capacity': 3.8449587748476004,\n",
       " 'far': 3.7563399389039,\n",
       " 'overall': 3.7886587301590677,\n",
       " 'locations': 4.098481928488062,\n",
       " 'almost': 3.7669972333778885,\n",
       " 'management': 3.916989903056483,\n",
       " 'populations': 3.929516162875663,\n",
       " 'provided': 3.8964553006147753,\n",
       " 'outcomes': 4.242731537332609,\n",
       " 'settlements': 4.021889483006678,\n",
       " 'persons': 3.916989903056483,\n",
       " 'system': 3.929516162875663,\n",
       " 'damage': 3.916989903056483,\n",
       " 'although': 3.7959847702511404,\n",
       " 'dengue': 4.425053094126564,\n",
       " 'world': 3.7813859708299877,\n",
       " 'centre': 4.06424875684582,\n",
       " 'family': 3.8923984999191608,\n",
       " 'income': 4.008159290194777,\n",
       " 'figure': 3.9087253932065895,\n",
       " 'insecure': 4.008159290194777,\n",
       " 'needed': 3.9128491103904515,\n",
       " 'capital': 3.8070754569452987,\n",
       " 'part': 3.818290527765439,\n",
       " 'attack': 4.088580857505351,\n",
       " 'facing': 3.8964553006147753,\n",
       " 'unicef': 3.9464657211894365,\n",
       " 'rice': 4.170678219692552,\n",
       " 'centres': 4.035810821525287,\n",
       " 'migrants': 4.277822857143879,\n",
       " 'workers': 3.9507483829814376,\n",
       " 'source': 4.012715106730637,\n",
       " 'lean': 3.9128491103904515,\n",
       " 'information': 3.9211479132051466,\n",
       " 'fatality': 4.069068043281769,\n",
       " 'livelihood': 3.9946150650870194,\n",
       " 'thousands': 3.8644525695286016,\n",
       " 'fighting': 3.9464657211894365,\n",
       " 'laboratory': 4.0739106677575565,\n",
       " 'left': 3.84110520553161,\n",
       " 'resources': 3.900528626002411,\n",
       " 'reduced': 3.916989903056483,\n",
       " 'consumption': 4.1339286774838095,\n",
       " 'official': 3.916989903056483,\n",
       " 'somali': 4.128787277983391,\n",
       " 'seven': 3.900528626002411,\n",
       " 'rohingya': 4.248495242049359,\n",
       " 'normal': 4.098481928488062,\n",
       " 'used': 3.9593691260253445,\n",
       " 'darfur': 4.197780149470343,\n",
       " 'drinking': 4.054679305829669,\n",
       " 'positive': 4.170678219692552,\n",
       " 'improved': 4.021889483006678,\n",
       " 'livelihoods': 3.972441207592697,\n",
       " 'spread': 3.9768368190657353,\n",
       " 'movement': 3.9464657211894365,\n",
       " 'nigeria': 4.128787277983391,\n",
       " 'floods': 4.059452584582327,\n",
       " 'registered': 3.942201322402979,\n",
       " 'conducted': 3.942201322402979,\n",
       " 'sam': 4.265988399496877,\n",
       " 'remained': 4.008159290194777,\n",
       " 'staple': 4.113519805852603,\n",
       " 'power': 4.026508428862973,\n",
       " 'farmers': 4.154762764386652,\n",
       " 'late': 3.981251837274852,\n",
       " 'along': 3.916989903056483,\n",
       " 'borno': 4.231302841508986,\n",
       " 'village': 4.088580857505351,\n",
       " 'sexual': 4.186851078938152,\n",
       " 'th': 4.254292359733685,\n",
       " 'sector': 4.083666842702922,\n",
       " 'incidents': 4.220003286255053,\n",
       " 'civilian': 4.220003286255053,\n",
       " 'operations': 4.040494670837713,\n",
       " 'data': 4.049928703071071,\n",
       " 'countries': 4.021889483006678,\n",
       " 'hygiene': 4.1339286774838095,\n",
       " 'governorates': 4.248495242049359,\n",
       " 'significantly': 3.9637075276239426,\n",
       " 'second': 4.026508428862973,\n",
       " 'casualties': 4.391151542450882,\n",
       " 'somalia': 4.1653448737171885,\n",
       " 'respectively': 4.049928703071071,\n",
       " 'working': 4.031148808419475,\n",
       " 'place': 4.012715106730637,\n",
       " 'yemen': 4.231302841508986,\n",
       " 'staff': 4.083666842702922,\n",
       " 'led': 4.012715106730637,\n",
       " 'hospitals': 4.1653448737171885,\n",
       " 'nile': 4.289799048190595,\n",
       " 'another': 3.990140784692098,\n",
       " 'live': 4.054679305829669,\n",
       " 'wfp': 4.208829985656927,\n",
       " 'serious': 4.040494670837713,\n",
       " 'bank': 4.214401030706383,\n",
       " 'organization': 4.0739106677575565,\n",
       " 'harvests': 4.181431011468813,\n",
       " 'army': 4.1339286774838095,\n",
       " 'mainly': 4.008159290194777,\n",
       " 'started': 4.035810821525287,\n",
       " 'concerns': 4.06424875684582,\n",
       " 'says': 4.123672177316621,\n",
       " 'gam': 4.581505270942101,\n",
       " 'opportunities': 4.123672177316621,\n",
       " 'start': 4.040494670837713,\n",
       " 'pastoral': 4.3713489151547025,\n",
       " 'located': 4.054679305829669,\n",
       " 'flooding': 4.139096647642252,\n",
       " 'relief': 4.144291464519356,\n",
       " 'cereal': 4.364834234133509,\n",
       " 'closed': 4.144291464519356,\n",
       " 'outbreaks': 4.1653448737171885,\n",
       " 'urgent': 4.026508428862973,\n",
       " 'include': 4.059452584582327,\n",
       " 'infrastructure': 4.1034694699991014,\n",
       " 'officials': 4.123672177316621,\n",
       " 'stressed': 4.186851078938152,\n",
       " 'often': 4.160039821487495,\n",
       " 'based': 4.059452584582327,\n",
       " 'rainy': 4.170678219692552,\n",
       " 'service': 4.208829985656927,\n",
       " 'dry': 4.1653448737171885,\n",
       " 'us': 4.283793024130383,\n",
       " 'governorate': 4.225637103973309,\n",
       " 'reached': 4.049928703071071,\n",
       " 'open': 4.181431011468813,\n",
       " 'distribution': 4.220003286255053,\n",
       " 'individuals': 4.208829985656927,\n",
       " 'red': 4.225637103973309,\n",
       " 'zone': 4.314190501314754,\n",
       " 'targeted': 4.113519805852603,\n",
       " 'close': 4.0739106677575565,\n",
       " 'able': 4.069068043281769,\n",
       " 'affecting': 4.078776857408729,\n",
       " 'agency': 4.1339286774838095,\n",
       " 'resulted': 4.113519805852603,\n",
       " 'media': 4.231302841508986,\n",
       " 'numbers': 4.093519139145934,\n",
       " 'home': 4.139096647642252,\n",
       " 'given': 4.1034694699991014,\n",
       " 'teachers': 4.489131950811086,\n",
       " 'arrivals': 4.225637103973309,\n",
       " 'made': 4.069068043281769,\n",
       " 'hit': 4.154762764386652,\n",
       " 'site': 4.34554103119883,\n",
       " 'civil': 4.170678219692552,\n",
       " 'long': 4.160039821487495,\n",
       " 'outside': 4.1339286774838095,\n",
       " 'fled': 4.176040162833937,\n",
       " 'stocks': 4.231302841508986,\n",
       " 'sudanese': 4.289799048190595,\n",
       " 'improve': 4.220003286255053,\n",
       " 'kenya': 4.34554103119883,\n",
       " 'key': 4.160039821487495,\n",
       " 'eight': 4.139096647642252,\n",
       " 'efforts': 4.113519805852603,\n",
       " 'decline': 4.170678219692552,\n",
       " 'related': 4.154762764386652,\n",
       " 'reach': 4.197780149470343,\n",
       " 'prevalence': 4.397840530601679,\n",
       " 'ethiopia': 4.308036635740375,\n",
       " 'general': 4.197780149470343,\n",
       " 'items': 4.254292359733685,\n",
       " 'united': 4.1653448737171885,\n",
       " 'risks': 4.231302841508986,\n",
       " 'meet': 4.208829985656927,\n",
       " 'zones': 4.34554103119883,\n",
       " 'famine': 4.489131950811086,\n",
       " 'actors': 4.271888121624064,\n",
       " 'require': 4.192300683705717,\n",
       " 'republic': 4.231302841508986,\n",
       " 'malnourished': 4.453026946168969,\n",
       " 'monday': 4.1653448737171885,\n",
       " 'yet': 4.1339286774838095,\n",
       " 'awd': 4.605996290950396,\n",
       " 'challenges': 4.208829985656927,\n",
       " 'students': 4.557599750088547,\n",
       " 'samples': 4.496512058108708,\n",
       " 'department': 4.326613021313311,\n",
       " 'shortage': 4.220003286255053,\n",
       " 'global': 4.208829985656927,\n",
       " 'aged': 4.332882634326906,\n",
       " 'trend': 4.214401030706383,\n",
       " 'war': 4.277822857143879,\n",
       " 'using': 4.237000862623624,\n",
       " 'urban': 4.308036635740375,\n",
       " 'decrease': 4.30192040872294,\n",
       " 'lives': 4.181431011468813,\n",
       " 'detention': 4.397840530601679,\n",
       " 'cluster': 4.3713489151547025,\n",
       " 'seasonal': 4.34554103119883,\n",
       " 'greater': 4.518984913960767,\n",
       " 'today': 4.237000862623624,\n",
       " 'results': 4.277822857143879,\n",
       " 'back': 4.260123280044478,\n",
       " 'violations': 4.460144413937834,\n",
       " 'agriculture': 4.314190501314754,\n",
       " 'arrived': 4.265988399496877,\n",
       " 'adequate': 4.283793024130383,\n",
       " 'economic': 4.271888121624064,\n",
       " 'radio': 4.277822857143879,\n",
       " 'held': 4.339191803520171,\n",
       " 'wheat': 4.605996290950396,\n",
       " 'electricity': 4.418180214838801,\n",
       " 'healthcare': 4.391151542450882,\n",
       " 'recently': 4.197780149470343,\n",
       " 'plan': 4.283793024130383,\n",
       " 'shortages': 4.2958413626465575,\n",
       " 'resulting': 4.237000862623624,\n",
       " 'lost': 4.314190501314754,\n",
       " 'mostly': 4.225637103973309,\n",
       " 'ghouta': 4.581505270942101,\n",
       " 'clashes': 4.377906315700861,\n",
       " 'road': 4.431973536971137,\n",
       " 'regional': 4.339191803520171,\n",
       " 'countrys': 4.242731537332609,\n",
       " 'latrines': 4.6396129017493815,\n",
       " 'social': 4.308036635740375,\n",
       " 'followed': 4.30192040872294,\n",
       " 'development': 4.277822857143879,\n",
       " 'transmission': 4.418180214838801,\n",
       " 'cumulative': 4.397840530601679,\n",
       " 'suffering': 4.277822857143879,\n",
       " 'like': 4.260123280044478,\n",
       " 'campaign': 4.358361719627891,\n",
       " 'much': 4.283793024130383,\n",
       " 'throughout': 4.283793024130383,\n",
       " 'nations': 4.248495242049359,\n",
       " 'belowaverage': 4.364834234133509,\n",
       " 'rise': 4.271888121624064,\n",
       " 'restrictions': 4.3713489151547025,\n",
       " 'negative': 4.3845069997322135,\n",
       " 'hundreds': 4.2958413626465575,\n",
       " 'returnees': 4.511437708325384,\n",
       " 'essential': 4.326613021313311,\n",
       " 'nine': 4.2958413626465575,\n",
       " 'fire': 4.541974432185466,\n",
       " 'decreased': 4.358361719627891,\n",
       " 'funding': 4.364834234133509,\n",
       " 'law': 4.503947036596227,\n",
       " 'ensure': 4.314190501314754,\n",
       " 'taken': 4.314190501314754,\n",
       " 'required': 4.289799048190595,\n",
       " 'fall': 4.3845069997322135,\n",
       " 'safety': 4.34554103119883,\n",
       " 'myanmar': 4.589602481174721,\n",
       " 'minister': 4.404574562783023,\n",
       " 'pregnant': 4.332882634326906,\n",
       " 'labor': 4.605996290950396,\n",
       " 'internally': 4.320382471562675,\n",
       " 'analysis': 4.3845069997322135,\n",
       " 'remaining': 4.2958413626465575,\n",
       " 'sorghum': 4.6396129017493815,\n",
       " 'observed': 4.3713489151547025,\n",
       " 'latest': 4.277822857143879,\n",
       " 'order': 4.314190501314754,\n",
       " 'roads': 4.411354249768402,\n",
       " 'diarrhoea': 4.489131950811086,\n",
       " 'earlier': 4.34554103119883,\n",
       " 'centers': 4.541974432185466,\n",
       " 'victims': 4.445959778945877,\n",
       " 'say': 4.326613021313311,\n",
       " 'wednesday': 4.3713489151547025,\n",
       " 'measures': 4.377906315700861,\n",
       " 'occurred': 4.320382471562675,\n",
       " 'assessed': 4.573473099244837,\n",
       " 'northeast': 4.474533151389934,\n",
       " 'leading': 4.320382471562675,\n",
       " 'unable': 4.332882634326906,\n",
       " 'returned': 4.438942206287231,\n",
       " 'daily': 4.358361719627891,\n",
       " 'injuries': 4.597765791813881,\n",
       " 'young': 4.404574562783023,\n",
       " 'demand': 4.460144413937834,\n",
       " 'leaving': 4.351930829297601,\n",
       " 'experiencing': 4.418180214838801,\n",
       " 'next': 4.332882634326906,\n",
       " 'rain': 4.496512058108708,\n",
       " 'presence': 4.411354249768402,\n",
       " 'quality': 4.481805910719013,\n",
       " 'hours': 4.404574562783023,\n",
       " 'third': 4.364834234133509,\n",
       " 'interventions': 4.445959778945877,\n",
       " 'coping': 4.526589513345987,\n",
       " 'slightly': 4.445959778945877,\n",
       " 'increases': 4.467312903416446,\n",
       " 'take': 4.404574562783023,\n",
       " 'organizations': 4.425053094126564,\n",
       " 'boys': 4.589602481174721,\n",
       " 'monitoring': 4.411354249768402,\n",
       " 'detained': 4.526589513345987,\n",
       " 'vaccination': 4.511437708325384,\n",
       " 'agencies': 4.425053094126564,\n",
       " 'help': 4.397840530601679,\n",
       " 'rapid': 4.397840530601679,\n",
       " 'away': 4.431973536971137,\n",
       " 'oromia': 4.656854708183888,\n",
       " 'trade': 4.534252386091556,\n",
       " 'tuesday': 4.391151542450882,\n",
       " 'either': 4.391151542450882,\n",
       " 'seen': 4.397840530601679,\n",
       " 'imports': 4.738347742435071,\n",
       " 'survey': 4.534252386091556,\n",
       " 'journalists': 4.837438645079302,\n",
       " 'soldiers': 4.622663343435608,\n",
       " 'providing': 4.460144413937834,\n",
       " 'cross': 4.526589513345987,\n",
       " 'go': 4.431973536971137,\n",
       " 'cash': 4.597765791813881,\n",
       " 'provision': 4.481805910719013,\n",
       " 'probable': 4.816819357876566,\n",
       " 'flood': 4.692256635234804,\n",
       " 'possible': 4.404574562783023,\n",
       " 'difficult': 4.391151542450882,\n",
       " 'cost': 4.557599750088547,\n",
       " 'niger': 4.674399017834797,\n",
       " 'temporary': 4.445959778945877,\n",
       " 'night': 4.589602481174721,\n",
       " 'important': 4.56550492959566,\n",
       " 'political': 4.496512058108708,\n",
       " 'africa': 4.541974432185466,\n",
       " 'epidemic': 4.518984913960767,\n",
       " 'active': 4.489131950811086,\n",
       " 'lgas': 5.032338984080116,\n",
       " 'indicated': 4.573473099244837,\n",
       " 'office': 4.534252386091556,\n",
       " 'former': 4.816819357876566,\n",
       " 'date': 4.474533151389934,\n",
       " 'libya': 4.719655609422918,\n",
       " 'surveillance': 4.622663343435608,\n",
       " 'kordofan': 4.776814023262867,\n",
       " 'diphtheria': 5.0579814146934545,\n",
       " 'different': 4.489131950811086,\n",
       " 'experienced': 4.503947036596227,\n",
       " 'mosul': 4.827075858043755,\n",
       " 'pasture': 4.622663343435608,\n",
       " 'sudans': 4.481805910719013,\n",
       " 'equatoria': 4.837438645079302,\n",
       " 'leave': 4.589602481174721,\n",
       " 'see': 4.573473099244837,\n",
       " 'rakhine': 4.858492054277134,\n",
       " 'receive': 4.474533151389934,\n",
       " 'associated': 4.489131950811086,\n",
       " 'indicate': 4.518984913960767,\n",
       " 'small': 4.489131950811086,\n",
       " 'better': 4.481805910719013,\n",
       " 'classified': 4.837438645079302,\n",
       " 'arrested': 4.656854708183888,\n",
       " 'coming': 4.474533151389934,\n",
       " 'medicines': 4.728958002085232,\n",
       " 'process': 4.614295093765092,\n",
       " 'buildings': 4.6396129017493815,\n",
       " 'inside': 4.557599750088547,\n",
       " 'sea': 4.622663343435608,\n",
       " 'thursday': 4.503947036596227,\n",
       " 'widespread': 4.481805910719013,\n",
       " 'operation': 4.614295093765092,\n",
       " 'gaps': 4.573473099244837,\n",
       " 'learning': 4.827075858043755,\n",
       " 'newly': 4.511437708325384,\n",
       " 'times': 4.56550492959566,\n",
       " 'largest': 4.496512058108708,\n",
       " 'friday': 4.503947036596227,\n",
       " 'settlement': 4.747826486389615,\n",
       " 'life': 4.518984913960767,\n",
       " 'facility': 4.913150466814998,\n",
       " 'disaster': 4.622663343435608,\n",
       " 'carried': 4.526589513345987,\n",
       " 'commodities': 4.738347742435071,\n",
       " 'congo': 4.6396129017493815,\n",
       " 'uganda': 4.665588388152642,\n",
       " 'quarter': 4.816819357876566,\n",
       " 'make': 4.511437708325384,\n",
       " 'opposition': 4.622663343435608,\n",
       " 'watch': 4.656854708183888,\n",
       " 'every': 4.549756572627521,\n",
       " 'began': 4.518984913960767,\n",
       " 'sunday': 4.541974432185466,\n",
       " 'impacted': 4.683287965252044,\n",
       " 'stable': 4.589602481174721,\n",
       " 'immediate': 4.541974432185466,\n",
       " 'though': 4.511437708325384,\n",
       " 'become': 4.518984913960767,\n",
       " 'prevent': 4.526589513345987,\n",
       " 'minimal': 4.557599750088547,\n",
       " 'good': 4.56550492959566,\n",
       " 'inadequate': 4.589602481174721,\n",
       " 'watery': 4.6396129017493815,\n",
       " 'msf': 4.995067589282884,\n",
       " 'cattle': 4.913150466814998,\n",
       " 'head': 4.589602481174721,\n",
       " 'influx': 4.56550492959566,\n",
       " 'tested': 4.674399017834797,\n",
       " 'programme': 4.683287965252044,\n",
       " 'move': 4.589602481174721,\n",
       " 'body': 4.710438954317994,\n",
       " 'added': 4.557599750088547,\n",
       " 'building': 4.728958002085232,\n",
       " 'structures': 4.879998259498097,\n",
       " 'absence': 4.597765791813881,\n",
       " 'center': 4.757395937405765,\n",
       " 'issues': 4.614295093765092,\n",
       " 'released': 4.56550492959566,\n",
       " 'systems': 4.648196645440773,\n",
       " 'chronic': 4.597765791813881,\n",
       " 'juba': 4.827075858043755,\n",
       " 'towns': 4.614295093765092,\n",
       " 'declined': 4.622663343435608,\n",
       " 'causing': 4.597765791813881,\n",
       " 'ground': 4.56550492959566,\n",
       " 'net': 4.757395937405765,\n",
       " 'bangladesh': 4.719655609422918,\n",
       " 'oil': 4.913150466814998,\n",
       " 'put': 4.581505270942101,\n",
       " 'old': 4.6396129017493815,\n",
       " 'coordination': 4.622663343435608,\n",
       " 'jerusalem': 4.913150466814998,\n",
       " 'fear': 4.622663343435608,\n",
       " 'private': 4.683287965252044,\n",
       " 'female': 4.837438645079302,\n",
       " 'team': 4.767057848317502,\n",
       " 'coverage': 4.701306470754722,\n",
       " 'attacked': 4.710438954317994,\n",
       " 'white': 4.827075858043755,\n",
       " 'extremely': 4.581505270942101,\n",
       " 'issued': 4.710438954317994,\n",
       " 'president': 4.6396129017493815,\n",
       " 'teams': 4.747826486389615,\n",
       " 'projected': 4.665588388152642,\n",
       " 'virus': 4.879998259498097,\n",
       " 'wau': 5.139106959505822,\n",
       " 'force': 4.674399017834797,\n",
       " 'e': 4.816819357876566,\n",
       " 'car': 4.869187343393882,\n",
       " 'various': 4.581505270942101,\n",
       " 'lassa': 5.182279131371032,\n",
       " 'get': 4.656854708183888,\n",
       " 'action': 4.747826486389615,\n",
       " 'line': 4.665588388152642,\n",
       " 'neighbouring': 4.622663343435608,\n",
       " 'status': 4.710438954317994,\n",
       " 'israel': 4.858492054277134,\n",
       " 'meningitis': 5.071053496260807,\n",
       " 'foods': 4.901977166216873,\n",
       " 'asylum': 4.827075858043755,\n",
       " 'killing': 4.665588388152642,\n",
       " 'field': 4.806666986412548,\n",
       " 'means': 4.622663343435608,\n",
       " 'existing': 4.710438954317994,\n",
       " 'scale': 4.692256635234804,\n",
       " 'present': 4.622663343435608,\n",
       " 'onset': 4.692256635234804,\n",
       " 'physical': 4.648196645440773,\n",
       " 'director': 4.656854708183888,\n",
       " 'proportion': 4.683287965252044,\n",
       " 'terms': 4.701306470754722,\n",
       " 'hunger': 4.827075858043755,\n",
       " 'democratic': 4.648196645440773,\n",
       " 'council': 4.738347742435071,\n",
       " 'labour': 4.816819357876566,\n",
       " 'gap': 4.901977166216873,\n",
       " 'extreme': 4.665588388152642,\n",
       " 'declared': 4.683287965252044,\n",
       " 'infection': 4.738347742435071,\n",
       " 'airstrikes': 4.901977166216873,\n",
       " 'committee': 4.728958002085232,\n",
       " 'explosive': 4.924450022068932,\n",
       " 'dead': 4.701306470754722,\n",
       " 'similar': 4.648196645440773,\n",
       " 'secondary': 4.719655609422918,\n",
       " 'estimates': 4.648196645440773,\n",
       " 'domestic': 4.701306470754722,\n",
       " 'points': 4.719655609422918,\n",
       " 'worst': 4.6396129017493815,\n",
       " 'took': 4.674399017834797,\n",
       " 'construction': 4.776814023262867,\n",
       " 'potential': 4.665588388152642,\n",
       " 'moderate': 4.656854708183888,\n",
       " 'weather': 4.701306470754722,\n",
       " 'gas': 4.924450022068932,\n",
       " 'receiving': 4.665588388152642,\n",
       " 'priority': 4.786666319705878,\n",
       " 'delivery': 4.728958002085232,\n",
       " 'doctors': 4.837438645079302,\n",
       " 'known': 4.692256635234804,\n",
       " 'grain': 5.071053496260807,\n",
       " 'enough': 4.701306470754722,\n",
       " 'point': 4.728958002085232,\n",
       " 'milk': 4.959135580056821,\n",
       " 'southeastern': 4.719655609422918,\n",
       " 'particular': 4.701306470754722,\n",
       " 'returning': 4.747826486389615,\n",
       " 'materials': 4.796616650559047,\n",
       " 'challenge': 4.719655609422918,\n",
       " 'ago': 4.674399017834797,\n",
       " 'ending': 4.869187343393882,\n",
       " 'functioning': 4.796616650559047,\n",
       " 'iom': 5.0073376818746995,\n",
       " 'aleppo': 4.837438645079302,\n",
       " 'cause': 4.683287965252044,\n",
       " 'mortality': 4.796616650559047,\n",
       " 'travel': 4.757395937405765,\n",
       " 'statement': 4.776814023262867,\n",
       " 'burundi': 4.890927330030288,\n",
       " 'loss': 4.757395937405765,\n",
       " 'generally': 4.728958002085232,\n",
       " 'accessing': 4.767057848317502,\n",
       " 'turkey': 4.98294622875054,\n",
       " 'mission': 4.747826486389615,\n",
       " 'figures': 4.767057848317502,\n",
       " 'plague': 5.764200676820752,\n",
       " 'threshold': 4.827075858043755,\n",
       " 'african': 4.738347742435071,\n",
       " 'lead': 4.710438954317994,\n",
       " 'infected': 4.786666319705878,\n",
       " 'factors': 4.701306470754722,\n",
       " 'announced': 4.710438954317994,\n",
       " 'respondents': 5.274652451502046,\n",
       " 'pressure': 4.728958002085232,\n",
       " 'property': 4.738347742435071,\n",
       " 'km': 4.890927330030288,\n",
       " 'table': 5.182279131371032,\n",
       " 'migration': 4.786666319705878,\n",
       " 'supported': 4.728958002085232,\n",
       " 'traders': 5.032338984080116,\n",
       " 'flour': 4.98294622875054,\n",
       " 'person': 4.806666986412548,\n",
       " 'poverty': 4.879998259498097,\n",
       " 'showed': 4.710438954317994,\n",
       " 'sufficient': 4.710438954317994,\n",
       " 'financial': 4.728958002085232,\n",
       " 'deterioration': 4.738347742435071,\n",
       " 'insufficient': 4.701306470754722,\n",
       " 'damascus': 4.890927330030288,\n",
       " 'search': 4.767057848317502,\n",
       " 'little': 4.738347742435071,\n",
       " 'sustained': 4.747826486389615,\n",
       " 'largely': 4.728958002085232,\n",
       " 'common': 4.738347742435071,\n",
       " 'reduce': 4.786666319705878,\n",
       " 'considered': 4.816819357876566,\n",
       " 'partially': 4.786666319705878,\n",
       " 'air': 4.890927330030288,\n",
       " 'must': 4.776814023262867,\n",
       " 'experience': 4.796616650559047,\n",
       " 'makeshift': 4.890927330030288,\n",
       " 'money': 4.767057848317502,\n",
       " 'yobe': 5.045078009857546,\n",
       " 'authority': 4.827075858043755,\n",
       " 'abuse': 4.757395937405765,\n",
       " 'places': 4.796616650559047,\n",
       " 'included': 4.757395937405765,\n",
       " 'costs': 4.796616650559047,\n",
       " 'reduction': 4.757395937405765,\n",
       " 'torture': 4.869187343393882,\n",
       " 'deteriorate': 4.786666319705878,\n",
       " 'strip': 4.924450022068932,\n",
       " 'relatively': 4.796616650559047,\n",
       " 'show': 4.776814023262867,\n",
       " 'towards': 4.757395937405765,\n",
       " 'alone': 4.719655609422918,\n",
       " 'citizens': 4.913150466814998,\n",
       " 'address': 4.747826486389615,\n",
       " 'leaders': 4.837438645079302,\n",
       " 'tonnes': 5.139106959505822,\n",
       " 'legal': 4.890927330030288,\n",
       " 'equipment': 4.776814023262867,\n",
       " 'shows': 4.806666986412548,\n",
       " 'meanwhile': 4.747826486389615,\n",
       " 'purchasing': 4.786666319705878,\n",
       " 'governments': 4.757395937405765,\n",
       " 'documented': 4.901977166216873,\n",
       " 'news': 4.827075858043755,\n",
       " 'freedom': 4.901977166216873,\n",
       " 'c': 5.045078009857546,\n",
       " 'rising': 4.767057848317502,\n",
       " 'longer': 4.738347742435071,\n",
       " 'improvement': 4.847909944946597,\n",
       " 'incidence': 4.924450022068932,\n",
       " 'evacuation': 4.970970037703824,\n",
       " 'feeding': 5.0073376818746995,\n",
       " 'admitted': 4.913150466814998,\n",
       " 'saturday': 4.816819357876566,\n",
       " 'making': 4.776814023262867,\n",
       " 'forecast': 4.924450022068932,\n",
       " 'set': 4.786666319705878,\n",
       " 'problem': 4.847909944946597,\n",
       " 'jonglei': 4.970970037703824,\n",
       " 'chad': 5.045078009857546,\n",
       " 'upper': 4.935878717892554,\n",
       " 'growing': 4.806666986412548,\n",
       " 'fews': 5.032338984080116,\n",
       " 'disrupted': 4.816819357876566,\n",
       " 'treated': 4.847909944946597,\n",
       " 'wounded': 4.901977166216873,\n",
       " 'male': 4.94743954029363,\n",
       " 'threats': 4.869187343393882,\n",
       " 'consecutive': 4.776814023262867,\n",
       " 'alert': 4.94743954029363,\n",
       " 'palestinians': 4.970970037703824,\n",
       " 'noted': 4.796616650559047,\n",
       " 'court': 4.959135580056821,\n",
       " 'issue': 4.816819357876566,\n",
       " 'planting': 4.98294622875054,\n",
       " 'came': 4.806666986412548,\n",
       " 'way': 4.796616650559047,\n",
       " 'surveyed': 5.111327395398747,\n",
       " 'targeting': 4.879998259498097,\n",
       " 'bahr': 5.1251207175310824,\n",
       " 'minimum': 4.901977166216873,\n",
       " 'psychosocial': 4.924450022068932,\n",
       " 'persist': 4.806666986412548,\n",
       " 'cut': 4.827075858043755,\n",
       " 'afghanistan': 5.084298723010828,\n",
       " 'fields': 4.970970037703824,\n",
       " 'island': 4.924450022068932,\n",
       " 'islamic': 4.901977166216873,\n",
       " 'multiple': 4.827075858043755,\n",
       " 'mental': 5.0977217433429685,\n",
       " 'reasons': 4.879998259498097,\n",
       " 'fiveyear': 4.94743954029363,\n",
       " 'coast': 4.924450022068932,\n",
       " 'strategies': 4.995067589282884,\n",
       " 'armyworm': 4.959135580056821,\n",
       " 'unity': 4.98294622875054,\n",
       " 'turkana': 5.153291594497779,\n",
       " 'commissioner': 4.890927330030288,\n",
       " 'earthquake': 5.071053496260807,\n",
       " 'housing': 4.995067589282884,\n",
       " 'territory': 4.970970037703824,\n",
       " 'suffer': 4.827075858043755,\n",
       " 'destruction': 4.806666986412548,\n",
       " 'short': 4.827075858043755,\n",
       " 'threat': 4.869187343393882,\n",
       " 'constraints': 4.869187343393882,\n",
       " 'nearby': 4.816819357876566,\n",
       " 'established': 4.816819357876566,\n",
       " 'practices': 4.816819357876566,\n",
       " 'therefore': 4.837438645079302,\n",
       " 'de': 4.995067589282884,\n",
       " 'strong': 4.869187343393882,\n",
       " 'clean': 4.858492054277134,\n",
       " 'strike': 5.212132094520713,\n",
       " 'cooking': 5.0073376818746995,\n",
       " 'thus': 4.847909944946597,\n",
       " 'imported': 5.071053496260807,\n",
       " 'occupied': 4.94743954029363,\n",
       " 'ten': 4.858492054277134,\n",
       " 'direct': 4.858492054277134,\n",
       " 'adamawa': 5.111327395398747,\n",
       " 'recruitment': 4.94743954029363,\n",
       " 'boko': 5.111327395398747,\n",
       " 'rape': 5.032338984080116,\n",
       " 'crossing': 4.98294622875054,\n",
       " 'urgently': 4.806666986412548,\n",
       " 'yesterday': 4.858492054277134,\n",
       " 'condition': 4.913150466814998,\n",
       " 'commercial': 4.995067589282884,\n",
       " 'flee': 4.869187343393882,\n",
       " 'missing': 4.94743954029363,\n",
       " 'affect': 4.890927330030288,\n",
       " 'trends': 4.924450022068932,\n",
       " 'monthly': 4.94743954029363,\n",
       " 'includes': 4.901977166216873,\n",
       " 'lebanon': 5.0977217433429685,\n",
       " 'hepatitis': 5.182279131371032,\n",
       " 'cover': 4.879998259498097,\n",
       " 'weekly': 4.995067589282884,\n",
       " 'assessments': 4.94743954029363,\n",
       " 'regular': 4.913150466814998,\n",
       " 'network': 4.98294622875054,\n",
       " 'brought': 4.869187343393882,\n",
       " 'opened': 4.98294622875054,\n",
       " 'personnel': 4.959135580056821,\n",
       " 'aboveaverage': 4.924450022068932,\n",
       " 'come': 4.858492054277134,\n",
       " 'called': 4.858492054277134,\n",
       " 'localized': 4.858492054277134,\n",
       " 'peak': 4.890927330030288,\n",
       " 'northwest': 4.98294622875054,\n",
       " 'transport': 4.94743954029363,\n",
       " 'mass': 4.913150466814998,\n",
       " 'losses': 5.0073376818746995,\n",
       " 'kits': 5.1251207175310824,\n",
       " 'drc': 5.0579814146934545,\n",
       " 'locality': 5.071053496260807,\n",
       " 'infections': 4.94743954029363,\n",
       " 'fleeing': 4.858492054277134,\n",
       " 'indicates': 4.869187343393882,\n",
       " ...}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create BM25 matrix\n",
    "def create_bm25_matrix(tokenized_text_list, dictionary, idf):\n",
    "    \"\"\"\n",
    "    Create BM25 matrix\n",
    "    :param tokenized_text_list: list of tokenized texts, list\n",
    "    :param dictionary: dictionary, dict\n",
    "    :param idf: idf for all words in dictionary, dict\n",
    "    :return: BM25 matrix, np.array\n",
    "    \"\"\"\n",
    "    bm25_matrix = np.zeros((len(tokenized_text_list), len(dictionary)))\n",
    "    for i, tokenized_text in enumerate(tqdm(tokenized_text_list, total=len(tokenized_text_list))):\n",
    "        for j, word in enumerate(dictionary):\n",
    "            bm25_matrix[i, j] = bm25(word, tokenized_text, tokenized_text_list, idf)\n",
    "    return bm25_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77f88fc033d43df86159730ab21d113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a8fc1ecbd84c4a90613959b7ea0721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2595 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf67c0ae4cf40f3b94d426e18d2816b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2596 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bm25_matrix_train = create_bm25_matrix(train_df['tokenized_text'], dictionary, idf)\n",
    "bm25_matrix_test = create_bm25_matrix(test_df['tokenized_text'], dictionary, idf)\n",
    "bm25_matrix_val = create_bm25_matrix(val_df['tokenized_text'], dictionary, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle as pkl\n",
    "pkl.dump(bm25_matrix_train, open('bm25_matrix_train.pkl', 'wb'))\n",
    "pkl.dump(bm25_matrix_test, open('bm25_matrix_test.pkl', 'wb'))\n",
    "pkl.dump(bm25_matrix_val, open('bm25_matrix_val.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How often the difference equals 0: 98834136\n",
      "How often the corresponding values of tf_idf and bm25 are equal but are not equal to zero: 0\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "diff_matrix = tf_idf_matrix_train - bm25_matrix_train\n",
    "diff_matrix_where_both_not_zero = diff_matrix[(tf_idf_matrix_train != 0.0) & (bm25_matrix_train != 0.0)]\n",
    "zero_diff_count = len(diff_matrix[diff_matrix == 0.0])\n",
    "zero_diff_count_where_both_not_zero = len(diff_matrix_where_both_not_zero[diff_matrix_where_both_not_zero == 0.0])\n",
    "\n",
    "print(f\"How often the difference equals 0: {zero_diff_count}\")\n",
    "print(f\"How often the corresponding values of tf_idf and bm25 are equal but are not equal to zero: {zero_diff_count_where_both_not_zero}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.05604833369230368, 1.1319777004100788)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(diff_matrix_where_both_not_zero), np.mean(np.abs(diff_matrix_where_both_not_zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0129877558701921, 0.013217793530761881)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tf_idf_matrix_train), np.mean(bm25_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22325402288504834, 0.2384040880711321)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(tf_idf_matrix_train), np.std(bm25_matrix_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of tf-idf matrix, train set:  0.9958957270374426\n",
      "Sparsity of tf-idf matrix, validation set:  0.9959790770237404\n",
      "Sparsity of tf-idf matrix, test set:  0.9959919637073689\n"
     ]
    }
   ],
   "source": [
    "# Calculate and report the sparsity rate of the vectors of train, validation, and test sets,\n",
    "# namely what percentages of the vectors in each set are filled with zeros.\n",
    "\n",
    "# sparsity of tf-idf\n",
    "print('Sparsity of tf-idf matrix, train set: ', 1 - np.count_nonzero(tf_idf_matrix_train) / tf_idf_matrix_train.size)\n",
    "print('Sparsity of tf-idf matrix, validation set: ', 1 - np.count_nonzero(tf_idf_matrix_val) / tf_idf_matrix_val.size)\n",
    "print('Sparsity of tf-idf matrix, test set: ', 1 - np.count_nonzero(tf_idf_matrix_test) / tf_idf_matrix_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of BM25 matrix, train set:  0.9958957270374426\n",
      "Sparsity of BM25 matrix, validation set:  0.9959790770237404\n",
      "Sparsity of BM25 matrix, test set:  0.9959919637073689\n"
     ]
    }
   ],
   "source": [
    "# sparsity of BM25\n",
    "print('Sparsity of BM25 matrix, train set: ', 1 - np.count_nonzero(bm25_matrix_train) / bm25_matrix_train.size)\n",
    "print('Sparsity of BM25 matrix, validation set: ', 1 - np.count_nonzero(bm25_matrix_val) / bm25_matrix_val.size)\n",
    "print('Sparsity of BM25 matrix, test set: ', 1 - np.count_nonzero(bm25_matrix_test) / bm25_matrix_test.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=100, n_iter=7, random_state=42)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce vectors’ dimensions by applying Latent Semantic Analysis (LSA) to the vectors of both variations.\n",
    "# Report the sparsity rate of the vectors of train, validation, and test sets after dimensionality reduction.\n",
    "\n",
    "# LSA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "k_svd = 100\n",
    "svd = TruncatedSVD(n_components=k_svd, n_iter=7, random_state=42)\n",
    "svd.fit(tf_idf_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf_matrix_train_lsa = svd.transform(tf_idf_matrix_train)\n",
    "tf_idf_matrix_val_lsa = svd.transform(tf_idf_matrix_val)\n",
    "tf_idf_matrix_test_lsa = svd.transform(tf_idf_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69575791, -0.22465822, -0.08211086, ..., -0.1408919 ,\n",
       "         0.0149918 , -0.23613306],\n",
       "       [ 2.7280114 , -1.02332209,  1.19054081, ...,  0.77271061,\n",
       "        -0.40028264,  0.38921487],\n",
       "       [ 4.43634533,  2.26420834, -0.60081245, ...,  0.79922035,\n",
       "        -2.37792393,  1.21858432],\n",
       "       ...,\n",
       "       [ 1.03428372, -0.38884351,  0.88744841, ..., -0.37965635,\n",
       "         0.24784295, -0.29555749],\n",
       "       [ 1.79399647,  0.07872835,  1.41394841, ..., -0.37099971,\n",
       "         0.39624708, -0.30420575],\n",
       "       [ 2.32308256, -0.17807667,  0.30095219, ..., -0.46450006,\n",
       "        -0.07726602,  0.31003912]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_matrix_test_lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of tf-idf matrix after LSA, train set:  0.0\n",
      "Sparsity of tf-idf matrix after LSA, validation set:  0.0\n",
      "Sparsity of tf-idf matrix after LSA, test set:  0.0\n"
     ]
    }
   ],
   "source": [
    "# sparsity of tf-idf after LSA\n",
    "print('Sparsity of tf-idf matrix after LSA, train set: ', 1 - np.count_nonzero(tf_idf_matrix_train_lsa) / tf_idf_matrix_train_lsa.size)\n",
    "print('Sparsity of tf-idf matrix after LSA, validation set: ', 1 - np.count_nonzero(tf_idf_matrix_val_lsa) / tf_idf_matrix_val_lsa.size)\n",
    "print('Sparsity of tf-idf matrix after LSA, test set: ', 1 - np.count_nonzero(tf_idf_matrix_test_lsa) / tf_idf_matrix_test_lsa.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(n_components=100, n_iter=7, random_state=42)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BM25 after LSA\n",
    "k_svd = 100\n",
    "svd = TruncatedSVD(n_components=k_svd, n_iter=7, random_state=42)\n",
    "svd.fit(bm25_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bm25_matrix_train_lsa = svd.transform(bm25_matrix_train)\n",
    "bm25_matrix_val_lsa = svd.transform(bm25_matrix_val)\n",
    "bm25_matrix_test_lsa = svd.transform(bm25_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity of BM25 matrix after LSA, train set:  0.0\n",
      "Sparsity of BM25 matrix after LSA, validation set:  0.0\n",
      "Sparsity of BM25 matrix after LSA, test set:  0.0\n"
     ]
    }
   ],
   "source": [
    "# sparsity of BM25 after LSA\n",
    "print('Sparsity of BM25 matrix after LSA, train set: ', 1 - np.count_nonzero(bm25_matrix_train_lsa) / bm25_matrix_train_lsa.size)\n",
    "print('Sparsity of BM25 matrix after LSA, validation set: ', 1 - np.count_nonzero(bm25_matrix_val_lsa) / bm25_matrix_val_lsa.size)\n",
    "print('Sparsity of BM25 matrix after LSA, test set: ', 1 - np.count_nonzero(bm25_matrix_test_lsa) / bm25_matrix_test_lsa.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"section-training\"></a><h2 style=\"color:rgb(0,120,170)\">Task B: Training and Results Analysis (15 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"background-color:rgb(224, 243, 255)\">\n",
    "\n",
    "To evaluate the models, use <ins>accuracy</ins> as the metric throughout the task. \n",
    "\n",
    "**Dummy baseline (2 points).** Create one dummy baseline classifier that predicts the validation/test labels only based on the distribution of the labels in the training set (without any use of the feature vectors). This is a weak baseline and acts as a sanity check for the actual classifiers.\n",
    "\n",
    "**Training and tuning classifiers (5 points).** Select at least <ins>two classification algorithms</ins> from standard machine learning classifiers. Using each classification algorithm, train a machine learning model on each of the variations of feature vectors. This should result in <ins>eight experiment sets</ins> (4 variations of feature vectors × 2 classification algorithms). The ML model in each of the experiments possibly have several involving hyper-parameters (also keep in mind the dimensionality size $k$ in the low-dimensional vectors as yet another hyper-parameter). For each experiment, select <ins>one of the hyper-parameters and tune its value</ins>. The tuning process is done by first assigning at least <ins>five different values</ins> to the hyper-parameter, then training separate models based on each value, and finally using the evaluation results on the validation set to select the best-performing model. Report the studied hyper-parameters, the evaluation results of each on validation set, and finally the selected value of the hyper-parameter. \n",
    "\n",
    "**Evaluation, reporting results, and discussion (3 point).** Evaluate the selected model of the eight experiment on the test set and report the results of the experiments on <ins>both validation and test sets (side by side) in one table as well as in one plot</ins>. Compare different models. Are the test results lower(/higher) than the validation results? If it is the case, where can it be rooted from? Among all these models and variations, what are the most important factors improving the classification results?\n",
    "\n",
    "**Confusion matrix (2 point).** Select the best performing model among the experiments and use it to create a confusion matrix. The matrix shows the predicted versus true results per each label. Explain your observations on the matrix. Across which classes do you observe significant confusions?\n",
    "\n",
    "**Features visualization (3 point).** Continue with the best performing model and now take its feature vectors for the *dataitems in the test set*. Project these feature vectors to a 2-dimensional space using the TSNE method.  Using these 2-dimensional vectors, create two plots where the dataitems are shown as points (small circles) on the plots. The plots look exactly the same but only differ in the coloring of the data points. The first plot colors every dataitem with its *true label*, while the second one colors each according to its *predicted label by the model*. Keep in mind to assign the same colors to the classes of the plots, so that the plots are visually comparable. Put these two plots side by side, observe the differences, and compare the results. Report your observations.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dummy baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_train = train_df['label']\n",
    "y_val = val_df['label']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4     2829\n",
       "9     2657\n",
       "3     2079\n",
       "10     947\n",
       "11     754\n",
       "2      602\n",
       "8      564\n",
       "5      559\n",
       "1      500\n",
       "0      238\n",
       "7      226\n",
       "6      155\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the distribution of labels in the training set\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Health         2829\n",
       "Protection     2657\n",
       "Food           2079\n",
       "Shelter         947\n",
       "WASH            754\n",
       "Education       602\n",
       "Nutrition       564\n",
       "Livelihood      559\n",
       "Cross           500\n",
       "Agriculture     238\n",
       "NFI             226\n",
       "Logistic        155\n",
       "Name: caption, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['caption'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['caption'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>caption</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5446</td>\n",
       "      <td>In addition to the immediate life-saving inter...</td>\n",
       "      <td>9</td>\n",
       "      <td>Protection</td>\n",
       "      <td>in addition to the immediate lifesaving interv...</td>\n",
       "      <td>in addition to the immediate lifesaving interv...</td>\n",
       "      <td>[addition, immediate, lifesaving, intervention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8812</td>\n",
       "      <td>There are approximately 2.6 million people cla...</td>\n",
       "      <td>3</td>\n",
       "      <td>Food</td>\n",
       "      <td>there are approximately &lt; num &gt; million people...</td>\n",
       "      <td>there are approximately &lt; num &gt; million people...</td>\n",
       "      <td>[approximately, &lt;, num, &gt;, million, people, cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16709</td>\n",
       "      <td>While aid imports have held up recently, comme...</td>\n",
       "      <td>5</td>\n",
       "      <td>Livelihood</td>\n",
       "      <td>while aid imports have held up recently commer...</td>\n",
       "      <td>while aid import have held up recently commerc...</td>\n",
       "      <td>[aid, imports, held, recently, commercial, foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3526</td>\n",
       "      <td>Heavy rainfalls as well as onrush of water fro...</td>\n",
       "      <td>0</td>\n",
       "      <td>Agriculture</td>\n",
       "      <td>heavy rainfalls as well as onrush of water fro...</td>\n",
       "      <td>heavy rainfall a well a onrush of water from t...</td>\n",
       "      <td>[heavy, rainfalls, well, onrush, water, upstre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4928</td>\n",
       "      <td>Based on field reports 9 , the main production...</td>\n",
       "      <td>3</td>\n",
       "      <td>Food</td>\n",
       "      <td>based on field reports &lt; num &gt;  the main produ...</td>\n",
       "      <td>based on field report &lt; num &gt; the main product...</td>\n",
       "      <td>[based, field, reports, &lt;, num, &gt;, main, produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12105</th>\n",
       "      <td>12744</td>\n",
       "      <td>The total gap in the number of people who requ...</td>\n",
       "      <td>8</td>\n",
       "      <td>Nutrition</td>\n",
       "      <td>the total gap in the number of people who requ...</td>\n",
       "      <td>the total gap in the number of people who requ...</td>\n",
       "      <td>[total, gap, number, people, require, assistan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12106</th>\n",
       "      <td>9655</td>\n",
       "      <td>A food crisis is looming in the country with t...</td>\n",
       "      <td>0</td>\n",
       "      <td>Agriculture</td>\n",
       "      <td>a food crisis is looming in the country with t...</td>\n",
       "      <td>a food crisis is looming in the country with t...</td>\n",
       "      <td>[food, crisis, looming, country, seasons, maiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12107</th>\n",
       "      <td>6963</td>\n",
       "      <td>? Acute watery diarrhoea (AWD) continues to be...</td>\n",
       "      <td>4</td>\n",
       "      <td>Health</td>\n",
       "      <td>acute watery diarrhoea awd continues to be re...</td>\n",
       "      <td>acute watery diarrhoea awd continues to be rep...</td>\n",
       "      <td>[acute, watery, diarrhoea, awd, continues, rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12108</th>\n",
       "      <td>923</td>\n",
       "      <td>As South India grapples with drought and water...</td>\n",
       "      <td>11</td>\n",
       "      <td>WASH</td>\n",
       "      <td>as south india grapples with drought and water...</td>\n",
       "      <td>a south india grapple with drought and water s...</td>\n",
       "      <td>[south, india, grapples, drought, water, short...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12109</th>\n",
       "      <td>15880</td>\n",
       "      <td>Mirroring trends in South Africa, the main sou...</td>\n",
       "      <td>3</td>\n",
       "      <td>Food</td>\n",
       "      <td>mirroring trends in south africa the main sour...</td>\n",
       "      <td>mirroring trend in south africa the main sourc...</td>\n",
       "      <td>[mirroring, trends, south, africa, main, sourc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12110 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_id                                               text  label  \\\n",
       "0             5446  In addition to the immediate life-saving inter...      9   \n",
       "1             8812  There are approximately 2.6 million people cla...      3   \n",
       "2            16709  While aid imports have held up recently, comme...      5   \n",
       "3             3526  Heavy rainfalls as well as onrush of water fro...      0   \n",
       "4             4928  Based on field reports 9 , the main production...      3   \n",
       "...            ...                                                ...    ...   \n",
       "12105        12744  The total gap in the number of people who requ...      8   \n",
       "12106         9655  A food crisis is looming in the country with t...      0   \n",
       "12107         6963  ? Acute watery diarrhoea (AWD) continues to be...      4   \n",
       "12108          923  As South India grapples with drought and water...     11   \n",
       "12109        15880  Mirroring trends in South Africa, the main sou...      3   \n",
       "\n",
       "           caption                                       cleaned_text  \\\n",
       "0       Protection  in addition to the immediate lifesaving interv...   \n",
       "1             Food  there are approximately < num > million people...   \n",
       "2       Livelihood  while aid imports have held up recently commer...   \n",
       "3      Agriculture  heavy rainfalls as well as onrush of water fro...   \n",
       "4             Food  based on field reports < num >  the main produ...   \n",
       "...            ...                                                ...   \n",
       "12105    Nutrition  the total gap in the number of people who requ...   \n",
       "12106  Agriculture  a food crisis is looming in the country with t...   \n",
       "12107       Health   acute watery diarrhoea awd continues to be re...   \n",
       "12108         WASH  as south india grapples with drought and water...   \n",
       "12109         Food  mirroring trends in south africa the main sour...   \n",
       "\n",
       "                                         normalized_text  \\\n",
       "0      in addition to the immediate lifesaving interv...   \n",
       "1      there are approximately < num > million people...   \n",
       "2      while aid import have held up recently commerc...   \n",
       "3      heavy rainfall a well a onrush of water from t...   \n",
       "4      based on field report < num > the main product...   \n",
       "...                                                  ...   \n",
       "12105  the total gap in the number of people who requ...   \n",
       "12106  a food crisis is looming in the country with t...   \n",
       "12107  acute watery diarrhoea awd continues to be rep...   \n",
       "12108  a south india grapple with drought and water s...   \n",
       "12109  mirroring trend in south africa the main sourc...   \n",
       "\n",
       "                                          tokenized_text  \n",
       "0      [addition, immediate, lifesaving, intervention...  \n",
       "1      [approximately, <, num, >, million, people, cl...  \n",
       "2      [aid, imports, held, recently, commercial, foo...  \n",
       "3      [heavy, rainfalls, well, onrush, water, upstre...  \n",
       "4      [based, field, reports, <, num, >, main, produ...  \n",
       "...                                                  ...  \n",
       "12105  [total, gap, number, people, require, assistan...  \n",
       "12106  [food, crisis, looming, country, seasons, maiz...  \n",
       "12107  [acute, watery, diarrhoea, awd, continues, rep...  \n",
       "12108  [south, india, grapples, drought, water, short...  \n",
       "12109  [mirroring, trends, south, africa, main, sourc...  \n",
       "\n",
       "[12110 rows x 7 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(strategy='most_frequent')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(tf_idf_matrix_train_lsa, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.2561633281972265\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on validation set\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dummy_clf.predict(tf_idf_matrix_val_lsa)\n",
    "print('Accuracy on validation set: ', accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training and tuning classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Select at least  two classification algorithms  from standard machine learning classifiers. Using each classification algorithm, train a machine learning model on each of the variations of feature vectors. This should result in  eight experiment sets  (4 variations of feature vectors × 2 classification algorithms). The ML model in each of the experiments possibly have several involving hyper-parameters (also keep in mind the dimensionality size   in the low-dimensional vectors as yet another hyper-parameter). For each experiment, select  one of the hyper-parameters and tune its value . The tuning process is done by first assigning at least  five different values  to the hyper-parameter, then training separate models based on each value, and finally using the evaluation results on the validation set to select the best-performing model. Report the studied hyper-parameters, the evaluation results of each on validation set, and finally the selected value of the hyper-parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clfs = np.array([None] * 8)\n",
    "val_accs = np.array([0.0] * 8)\n",
    "y_preds = np.array([None] * 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Algorithm 1: Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Experiment 1: TF-IDF, high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier with alpha: 0.01\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 3.84, NNZs: 35, Bias: -2.811899, T: 12110, Avg. loss: 0.094533\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.09, NNZs: 28, Bias: -6.616447, T: 12110, Avg. loss: 0.112654\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.89, NNZs: 67, Bias: -1.591763, T: 12110, Avg. loss: 0.235519\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.12, NNZs: 40, Bias: -3.415152, T: 12110, Avg. loss: 0.087871\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.54, NNZs: 25, Bias: -2.553433, T: 12110, Avg. loss: 0.066481\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.20, NNZs: 180, Bias: -1.137275, T: 12110, Avg. loss: 0.347427\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.14, NNZs: 51, Bias: -2.407204, T: 12110, Avg. loss: 0.090791\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.38, NNZs: 149, Bias: -1.415115, T: 12110, Avg. loss: 0.208460\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.26, NNZs: 409, Bias: -0.851784, T: 12110, Avg. loss: 0.420609\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.37, NNZs: 259, Bias: -0.978253, T: 12110, Avg. loss: 0.251159\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.95, NNZs: 93, Bias: -1.379334, T: 12110, Avg. loss: 0.202014\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.51, NNZs: 90, Bias: -1.143960, T: 12110, Avg. loss: 0.135864\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.82, NNZs: 36, Bias: -1.987078, T: 24220, Avg. loss: 0.050540\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.32, NNZs: 19, Bias: -1.012377, T: 24220, Avg. loss: 0.087692\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.14, NNZs: 42, Bias: -2.571513, T: 24220, Avg. loss: 0.057071\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.23, NNZs: 27, Bias: -5.797547, T: 24220, Avg. loss: 0.074389\n",
      "Total training time: 1.35 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.60, NNZs: 22, Bias: -1.913324, T: 24220, Avg. loss: 0.040360\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.15, NNZs: 49, Bias: -1.665164, T: 24220, Avg. loss: 0.046763\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.55, NNZs: 196, Bias: -1.042216, T: 24220, Avg. loss: 0.138859\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.21, NNZs: 102, Bias: -1.036387, T: 24220, Avg. loss: 0.071270\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.86, NNZs: 153, Bias: -1.202459, T: 24220, Avg. loss: 0.108974\n",
      "Total training time: 1.54 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.45, NNZs: 70, Bias: -1.016064, T: 24220, Avg. loss: 0.091091\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.04, NNZs: 454, Bias: -0.810553, T: 24220, Avg. loss: 0.205917\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.62, NNZs: 253, Bias: -0.978753, T: 24220, Avg. loss: 0.125633\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.58, NNZs: 16, Bias: -1.001823, T: 36330, Avg. loss: 0.083134\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.70, NNZs: 45, Bias: -2.132754, T: 36330, Avg. loss: 0.050781\n",
      "Total training time: 1.92 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.35, NNZs: 33, Bias: -1.568000, T: 36330, Avg. loss: 0.044168\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.22, NNZs: 21, Bias: -1.647704, T: 36330, Avg. loss: 0.036676\n",
      "Total training time: 2.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.95, NNZs: 26, Bias: -5.327668, T: 36330, Avg. loss: 0.067189\n",
      "Total training time: 2.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.69, NNZs: 51, Bias: -1.340954, T: 36330, Avg. loss: 0.041506\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.77, NNZs: 204, Bias: -1.008566, T: 36330, Avg. loss: 0.134916\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.16, NNZs: 161, Bias: -1.089997, T: 36330, Avg. loss: 0.107723\n",
      "Total training time: 2.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.61, NNZs: 110, Bias: -1.003252, T: 36330, Avg. loss: 0.068401\n",
      "Total training time: 2.23 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.84, NNZs: 265, Bias: -0.982856, T: 36330, Avg. loss: 0.120945\n",
      "Total training time: 2.51 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.73, NNZs: 66, Bias: -1.028256, T: 36330, Avg. loss: 0.088843\n",
      "Total training time: 2.53 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.43, NNZs: 53, Bias: -1.912162, T: 48440, Avg. loss: 0.048413\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.05, NNZs: 490, Bias: -0.813256, T: 36330, Avg. loss: 0.197160\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.13, NNZs: 22, Bias: -1.002525, T: 48440, Avg. loss: 0.083043\n",
      "Total training time: 2.64 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.06, NNZs: 35, Bias: -1.289731, T: 48440, Avg. loss: 0.041422\n",
      "Total training time: 2.77 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.79, NNZs: 27, Bias: -4.989190, T: 48440, Avg. loss: 0.061836\n",
      "Total training time: 2.83 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.45, NNZs: 53, Bias: -1.172865, T: 48440, Avg. loss: 0.039183\n",
      "Total training time: 2.84 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.00, NNZs: 21, Bias: -1.487153, T: 48440, Avg. loss: 0.036074\n",
      "Total training time: 2.86 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.29, NNZs: 217, Bias: -1.031275, T: 48440, Avg. loss: 0.132153\n",
      "Total training time: 2.86 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.72, NNZs: 174, Bias: -1.110586, T: 48440, Avg. loss: 0.104981\n",
      "Total training time: 2.88 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.26, NNZs: 120, Bias: -1.001031, T: 48440, Avg. loss: 0.066397\n",
      "Total training time: 2.93 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.40, NNZs: 287, Bias: -0.967084, T: 48440, Avg. loss: 0.119771\n",
      "Total training time: 3.19 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.25, NNZs: 54, Bias: -1.734195, T: 60550, Avg. loss: 0.046392\n",
      "Total training time: 3.27 seconds.\n",
      "Norm: 3.29, NNZs: 61, Bias: -1.010012, T: 48440, Avg. loss: 0.088411\n",
      "Total training time: 3.31 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.82, NNZs: 16, Bias: -1.002681, T: 60550, Avg. loss: 0.082899\n",
      "Total training time: 3.38 seconds.\n",
      "Norm: 5.46, NNZs: 521, Bias: -0.799400, T: 48440, Avg. loss: 0.191227\n",
      "Total training time: 3.37 seconds.\n",
      "-- Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    3.4s remaining:   16.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1.69, NNZs: 27, Bias: -4.729142, T: 60550, Avg. loss: 0.059068\n",
      "Total training time: 3.42 seconds.\n",
      "Norm: 3.97, NNZs: 230, Bias: -1.030105, T: 60550, Avg. loss: 0.130959\n",
      "Total training time: 3.50 seconds.\n",
      "Norm: 1.83, NNZs: 22, Bias: -1.408565, T: 60550, Avg. loss: 0.035533\n",
      "Total training time: 3.51 seconds.\n",
      "Norm: 1.87, NNZs: 32, Bias: -1.156278, T: 60550, Avg. loss: 0.039757\n",
      "Total training time: 3.52 seconds.\n",
      "Norm: 2.27, NNZs: 55, Bias: -1.108242, T: 60550, Avg. loss: 0.038603\n",
      "Total training time: 3.53 seconds.\n",
      "Norm: 3.43, NNZs: 182, Bias: -1.092912, T: 60550, Avg. loss: 0.103824\n",
      "Total training time: 3.55 seconds.\n",
      "Norm: 3.01, NNZs: 126, Bias: -1.002142, T: 60550, Avg. loss: 0.065641\n",
      "Total training time: 3.58 seconds.\n",
      "Norm: 4.10, NNZs: 294, Bias: -0.975202, T: 60550, Avg. loss: 0.117505\n",
      "Total training time: 3.74 seconds.\n",
      "Norm: 2.98, NNZs: 62, Bias: -1.002952, T: 60550, Avg. loss: 0.088048\n",
      "Total training time: 3.81 seconds.\n",
      "Norm: 5.08, NNZs: 541, Bias: -0.797026, T: 60550, Avg. loss: 0.188749\n",
      "Total training time: 3.90 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7862095531587057\n",
      "--------------------\n",
      "Classifier with alpha: 0.005\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 7.18, NNZs: 181, Bias: -4.817813, T: 12110, Avg. loss: 0.153233\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.56, NNZs: 137, Bias: -4.379946, T: 12110, Avg. loss: 0.107127\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.24, NNZs: 189, Bias: -4.248526, T: 12110, Avg. loss: 0.110996\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.20, NNZs: 174, Bias: -3.209203, T: 12110, Avg. loss: 0.125193\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.77, NNZs: 122, Bias: -8.777472, T: 12110, Avg. loss: 0.145994\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.70, NNZs: 409, Bias: -2.343858, T: 12110, Avg. loss: 0.340233\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12.02, NNZs: 625, Bias: -1.103988, T: 12110, Avg. loss: 0.398854\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.25, NNZs: 346, Bias: -2.154657, T: 12110, Avg. loss: 0.207827\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.09, NNZs: 447, Bias: -3.117999, T: 12110, Avg. loss: 0.410796\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12.32, NNZs: 545, Bias: -1.670653, T: 12110, Avg. loss: 0.572417\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.11, NNZs: 467, Bias: -2.196752, T: 12110, Avg. loss: 0.333610\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 15.19, NNZs: 932, Bias: -0.923937, T: 12110, Avg. loss: 0.649692\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.22, NNZs: 116, Bias: -3.300109, T: 24220, Avg. loss: 0.062406\n",
      "Total training time: 1.24 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.32, NNZs: 135, Bias: -2.924587, T: 24220, Avg. loss: 0.047889\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.76, NNZs: 88, Bias: -3.113240, T: 24220, Avg. loss: 0.047394\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.42, NNZs: 133, Bias: -1.909459, T: 24220, Avg. loss: 0.042254\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.16, NNZs: 297, Bias: -1.498102, T: 24220, Avg. loss: 0.103317\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.43, NNZs: 90, Bias: -7.303732, T: 24220, Avg. loss: 0.080075\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.94, NNZs: 490, Bias: -0.964737, T: 24220, Avg. loss: 0.109151\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.94, NNZs: 278, Bias: -1.191856, T: 24220, Avg. loss: 0.064454\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.29, NNZs: 464, Bias: -1.054253, T: 24220, Avg. loss: 0.130348\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.93, NNZs: 287, Bias: -1.294390, T: 24220, Avg. loss: 0.092023\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.18, NNZs: 381, Bias: -1.393324, T: 24220, Avg. loss: 0.099771\n",
      "Total training time: 1.64 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.39, NNZs: 107, Bias: -2.482478, T: 36330, Avg. loss: 0.050399\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.46, NNZs: 132, Bias: -2.269556, T: 36330, Avg. loss: 0.040294\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.33, NNZs: 848, Bias: -0.806732, T: 24220, Avg. loss: 0.179589\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.81, NNZs: 270, Bias: -1.208778, T: 36330, Avg. loss: 0.089328\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.02, NNZs: 80, Bias: -2.468821, T: 36330, Avg. loss: 0.039250\n",
      "Total training time: 2.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.83, NNZs: 92, Bias: -6.492157, T: 36330, Avg. loss: 0.068648\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.62, NNZs: 497, Bias: -0.935890, T: 36330, Avg. loss: 0.103951\n",
      "Total training time: 2.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.52, NNZs: 121, Bias: -1.394381, T: 36330, Avg. loss: 0.035613\n",
      "Total training time: 2.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.88, NNZs: 268, Bias: -1.048244, T: 36330, Avg. loss: 0.057089\n",
      "Total training time: 2.33 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.69, NNZs: 295, Bias: -1.176966, T: 36330, Avg. loss: 0.084907\n",
      "Total training time: 2.34 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.93, NNZs: 466, Bias: -1.041210, T: 36330, Avg. loss: 0.118946\n",
      "Total training time: 2.41 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.02, NNZs: 373, Bias: -1.206662, T: 36330, Avg. loss: 0.094162\n",
      "Total training time: 2.41 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.88, NNZs: 110, Bias: -1.977506, T: 48440, Avg. loss: 0.044038\n",
      "Total training time: 2.53 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.68, NNZs: 887, Bias: -0.743488, T: 36330, Avg. loss: 0.161109\n",
      "Total training time: 2.73 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.98, NNZs: 134, Bias: -1.913081, T: 48440, Avg. loss: 0.036884\n",
      "Total training time: 2.81 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.56, NNZs: 87, Bias: -2.060941, T: 48440, Avg. loss: 0.035443\n",
      "Total training time: 2.83 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.85, NNZs: 504, Bias: -0.923455, T: 48440, Avg. loss: 0.100967\n",
      "Total training time: 2.90 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.97, NNZs: 250, Bias: -1.149147, T: 48440, Avg. loss: 0.086121\n",
      "Total training time: 2.92 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.44, NNZs: 90, Bias: -5.948202, T: 48440, Avg. loss: 0.061262\n",
      "Total training time: 2.92 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.04, NNZs: 129, Bias: -1.206441, T: 48440, Avg. loss: 0.033069\n",
      "Total training time: 2.96 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.25, NNZs: 277, Bias: -1.040476, T: 48440, Avg. loss: 0.055849\n",
      "Total training time: 2.97 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.94, NNZs: 304, Bias: -1.154302, T: 48440, Avg. loss: 0.081384\n",
      "Total training time: 3.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.08, NNZs: 476, Bias: -1.054859, T: 48440, Avg. loss: 0.113891\n",
      "Total training time: 3.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.55, NNZs: 110, Bias: -1.679995, T: 60550, Avg. loss: 0.040762\n",
      "Total training time: 3.13 seconds.\n",
      "Norm: 6.25, NNZs: 386, Bias: -1.190158, T: 48440, Avg. loss: 0.089775\n",
      "Total training time: 3.18 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.29, NNZs: 90, Bias: -1.829334, T: 60550, Avg. loss: 0.033561\n",
      "Total training time: 3.44 seconds.\n",
      "Norm: 8.66, NNZs: 931, Bias: -0.727096, T: 48440, Avg. loss: 0.153685\n",
      "Total training time: 3.49 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.69, NNZs: 138, Bias: -1.647588, T: 60550, Avg. loss: 0.034813\n",
      "Total training time: 3.50 seconds.\n",
      "Norm: 3.22, NNZs: 93, Bias: -5.482451, T: 60550, Avg. loss: 0.057010\n",
      "Total training time: 3.51 seconds.\n",
      "Norm: 6.31, NNZs: 530, Bias: -0.975221, T: 60550, Avg. loss: 0.097901\n",
      "Total training time: 3.56 seconds.\n",
      "Norm: 5.39, NNZs: 254, Bias: -1.103709, T: 60550, Avg. loss: 0.084860\n",
      "Total training time: 3.58 seconds.\n",
      "Norm: 4.84, NNZs: 288, Bias: -1.028830, T: 60550, Avg. loss: 0.054866\n",
      "Total training time: 3.58 seconds.\n",
      "Norm: 3.71, NNZs: 136, Bias: -1.128825, T: 60550, Avg. loss: 0.032321\n",
      "Total training time: 3.60 seconds.\n",
      "Norm: 5.39, NNZs: 310, Bias: -1.085567, T: 60550, Avg. loss: 0.080528\n",
      "Total training time: 3.65 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    3.5s remaining:   17.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 6.51, NNZs: 496, Bias: -1.070169, T: 60550, Avg. loss: 0.112177\n",
      "Total training time: 3.75 seconds.\n",
      "Norm: 5.74, NNZs: 397, Bias: -1.134569, T: 60550, Avg. loss: 0.088087\n",
      "Total training time: 3.74 seconds.\n",
      "Norm: 8.02, NNZs: 962, Bias: -0.731717, T: 60550, Avg. loss: 0.147903\n",
      "Total training time: 3.98 seconds.\n",
      "Accuracy on validation set:  0.7977657935285054\n",
      "--------------------\n",
      "Classifier with alpha: 0.003\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 10.42, NNZs: 457, Bias: -6.711654, T: 12110, Avg. loss: 0.159927\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.74, NNZs: 535, Bias: -8.091376, T: 12110, Avg. loss: 0.243660\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.19, NNZs: 496, Bias: -5.128978, T: 12110, Avg. loss: 0.182287\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 14.16, NNZs: 829, Bias: -3.746797, T: 12110, Avg. loss: 0.320127\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 17.04, NNZs: 973, Bias: -4.148720, T: 12110, Avg. loss: 0.546358\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.43, NNZs: 390, Bias: -11.637950, T: 12110, Avg. loss: 0.197140\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 17.16, NNZs: 993, Bias: -4.920647, T: 12110, Avg. loss: 0.627009\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.60, NNZs: 499, Bias: -6.727964, T: 12110, Avg. loss: 0.155381\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 16.76, NNZs: 1007, Bias: -4.218785, T: 12110, Avg. loss: 0.523167\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.88, NNZs: 1188, Bias: -3.021067, T: 12110, Avg. loss: 0.879291\n",
      "Total training time: 1.12 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.08, NNZs: 1228, Bias: -1.655469, T: 12110, Avg. loss: 0.594390\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 22.42, NNZs: 1711, Bias: -1.101556, T: 12110, Avg. loss: 1.030681\n",
      "Total training time: 1.26 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.59, NNZs: 282, Bias: -4.747544, T: 24220, Avg. loss: 0.054942\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.40, NNZs: 352, Bias: -5.747086, T: 24220, Avg. loss: 0.081471\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.93, NNZs: 286, Bias: -3.097897, T: 24220, Avg. loss: 0.048321\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.07, NNZs: 711, Bias: -1.954730, T: 24220, Avg. loss: 0.104202\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.01, NNZs: 542, Bias: -1.769442, T: 24220, Avg. loss: 0.062766\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.27, NNZs: 245, Bias: -9.408374, T: 24220, Avg. loss: 0.090467\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.21, NNZs: 328, Bias: -4.651069, T: 24220, Avg. loss: 0.053180\n",
      "Total training time: 1.73 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.20, NNZs: 736, Bias: -2.185099, T: 24220, Avg. loss: 0.117908\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 11.87, NNZs: 731, Bias: -2.025494, T: 24220, Avg. loss: 0.104613\n",
      "Total training time: 1.88 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 13.40, NNZs: 853, Bias: -1.366757, T: 24220, Avg. loss: 0.129055\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.09, NNZs: 319, Bias: -4.435049, T: 36330, Avg. loss: 0.059436\n",
      "Total training time: 2.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.31, NNZs: 248, Bias: -3.688471, T: 36330, Avg. loss: 0.040018\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.02, NNZs: 1407, Bias: -0.881345, T: 24220, Avg. loss: 0.164439\n",
      "Total training time: 2.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.54, NNZs: 261, Bias: -2.154480, T: 36330, Avg. loss: 0.036567\n",
      "Total training time: 2.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 13.00, NNZs: 871, Bias: -1.137976, T: 24220, Avg. loss: 0.103293\n",
      "Total training time: 2.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.22, NNZs: 231, Bias: -8.172102, T: 36330, Avg. loss: 0.071641\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.38, NNZs: 508, Bias: -1.360390, T: 36330, Avg. loss: 0.048460\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 10.07, NNZs: 660, Bias: -1.468299, T: 36330, Avg. loss: 0.079674\n",
      "Total training time: 2.33 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.71, NNZs: 279, Bias: -3.525372, T: 36330, Avg. loss: 0.039663\n",
      "Total training time: 2.42 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 10.13, NNZs: 645, Bias: -1.633291, T: 36330, Avg. loss: 0.088355\n",
      "Total training time: 2.55 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.27, NNZs: 294, Bias: -3.605860, T: 48440, Avg. loss: 0.048128\n",
      "Total training time: 2.76 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.55, NNZs: 231, Bias: -2.988412, T: 48440, Avg. loss: 0.032991\n",
      "Total training time: 2.77 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 10.04, NNZs: 679, Bias: -1.506489, T: 36330, Avg. loss: 0.084669\n",
      "Total training time: 2.80 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.53, NNZs: 216, Bias: -7.316142, T: 48440, Avg. loss: 0.061616\n",
      "Total training time: 2.81 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.79, NNZs: 255, Bias: -1.679523, T: 48440, Avg. loss: 0.029718\n",
      "Total training time: 2.83 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 10.92, NNZs: 840, Bias: -1.027449, T: 36330, Avg. loss: 0.090289\n",
      "Total training time: 2.88 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.30, NNZs: 811, Bias: -1.166834, T: 36330, Avg. loss: 0.109218\n",
      "Total training time: 2.94 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.46, NNZs: 515, Bias: -1.182522, T: 48440, Avg. loss: 0.046199\n",
      "Total training time: 2.96 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.90, NNZs: 280, Bias: -2.875126, T: 48440, Avg. loss: 0.032963\n",
      "Total training time: 3.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.89, NNZs: 661, Bias: -1.364031, T: 48440, Avg. loss: 0.072672\n",
      "Total training time: 3.14 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 13.55, NNZs: 1389, Bias: -0.747508, T: 36330, Avg. loss: 0.138155\n",
      "Total training time: 3.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.04, NNZs: 640, Bias: -1.495929, T: 48440, Avg. loss: 0.083003\n",
      "Total training time: 3.30 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.67, NNZs: 296, Bias: -3.021473, T: 60550, Avg. loss: 0.041629\n",
      "Total training time: 3.43 seconds.\n",
      "Norm: 5.13, NNZs: 218, Bias: -6.653224, T: 60550, Avg. loss: 0.055638\n",
      "Total training time: 3.43 seconds.\n",
      "Norm: 5.26, NNZs: 246, Bias: -1.464988, T: 60550, Avg. loss: 0.027743\n",
      "Total training time: 3.45 seconds.\n",
      "Norm: 5.07, NNZs: 237, Bias: -2.550861, T: 60550, Avg. loss: 0.029402\n",
      "Total training time: 3.49 seconds.\n",
      "Norm: 8.96, NNZs: 659, Bias: -1.403852, T: 48440, Avg. loss: 0.076756\n",
      "Total training time: 3.58 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.73, NNZs: 853, Bias: -0.984734, T: 48440, Avg. loss: 0.087213\n",
      "Total training time: 3.60 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.84, NNZs: 526, Bias: -1.135072, T: 60550, Avg. loss: 0.043917\n",
      "Total training time: 3.62 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    3.5s remaining:   17.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 10.10, NNZs: 812, Bias: -1.121070, T: 48440, Avg. loss: 0.100467\n",
      "Total training time: 3.69 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.40, NNZs: 280, Bias: -2.358862, T: 60550, Avg. loss: 0.029195\n",
      "Total training time: 3.71 seconds.\n",
      "Norm: 8.12, NNZs: 651, Bias: -1.230316, T: 60550, Avg. loss: 0.069974\n",
      "Total training time: 3.85 seconds.\n",
      "Norm: 12.03, NNZs: 1398, Bias: -0.790338, T: 48440, Avg. loss: 0.125342\n",
      "Total training time: 3.86 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.24, NNZs: 638, Bias: -1.448569, T: 60550, Avg. loss: 0.079560\n",
      "Total training time: 3.99 seconds.\n",
      "Norm: 8.89, NNZs: 858, Bias: -0.941777, T: 60550, Avg. loss: 0.082084\n",
      "Total training time: 4.16 seconds.\n",
      "Norm: 8.24, NNZs: 668, Bias: -1.303592, T: 60550, Avg. loss: 0.074313\n",
      "Total training time: 4.16 seconds.\n",
      "Norm: 9.29, NNZs: 839, Bias: -1.157015, T: 60550, Avg. loss: 0.095667\n",
      "Total training time: 4.18 seconds.\n",
      "Norm: 11.05, NNZs: 1422, Bias: -0.741356, T: 60550, Avg. loss: 0.116957\n",
      "Total training time: 4.39 seconds.\n",
      "Accuracy on validation set:  0.7985362095531587\n",
      "--------------------\n",
      "Classifier with alpha: 0.001\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 29.91, NNZs: 1608, Bias: -17.148331, T: 12110, Avg. loss: 0.369213\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 30.23, NNZs: 1647, Bias: -14.874985, T: 12110, Avg. loss: 0.444369\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 31.83, NNZs: 1683, Bias: -17.938434, T: 12110, Avg. loss: 0.668913\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 29.13, NNZs: 1596, Bias: -15.568741, T: 12110, Avg. loss: 0.430151\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 27.44, NNZs: 1604, Bias: -24.318677, T: 12110, Avg. loss: 0.456096\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 37.65, NNZs: 2335, Bias: -11.883140, T: 12110, Avg. loss: 0.807291\n",
      "Total training time: 1.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 50.64, NNZs: 3236, Bias: -9.640378, T: 12110, Avg. loss: 2.246503\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 47.97, NNZs: 3242, Bias: -5.636092, T: 12110, Avg. loss: 1.579759\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 47.24, NNZs: 3056, Bias: -16.404782, T: 12110, Avg. loss: 1.763037\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 45.85, NNZs: 2811, Bias: -12.702698, T: 12110, Avg. loss: 1.467065\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 44.81, NNZs: 2761, Bias: -12.658533, T: 12110, Avg. loss: 1.391841\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 59.22, NNZs: 4074, Bias: -3.694113, T: 12110, Avg. loss: 2.789862\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 20.07, NNZs: 1414, Bias: -9.962316, T: 24220, Avg. loss: 0.072920\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 20.04, NNZs: 1395, Bias: -12.690260, T: 24220, Avg. loss: 0.072083\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 19.55, NNZs: 1379, Bias: -11.555974, T: 24220, Avg. loss: 0.066818\n",
      "Total training time: 1.89 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 19.89, NNZs: 1374, Bias: -19.148353, T: 24220, Avg. loss: 0.121457\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 21.93, NNZs: 1520, Bias: -13.042210, T: 24220, Avg. loss: 0.111312\n",
      "Total training time: 1.97 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 24.34, NNZs: 1985, Bias: -6.869413, T: 24220, Avg. loss: 0.090616\n",
      "Total training time: 2.29 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 33.12, NNZs: 2745, Bias: -4.696761, T: 24220, Avg. loss: 0.230866\n",
      "Total training time: 2.40 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 31.27, NNZs: 2679, Bias: -8.666929, T: 24220, Avg. loss: 0.228455\n",
      "Total training time: 2.52 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 30.92, NNZs: 2702, Bias: -3.468162, T: 24220, Avg. loss: 0.161506\n",
      "Total training time: 2.53 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 15.90, NNZs: 1207, Bias: -7.807258, T: 36330, Avg. loss: 0.040822\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.21, NNZs: 1249, Bias: -10.113316, T: 36330, Avg. loss: 0.044209\n",
      "Total training time: 2.64 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 29.33, NNZs: 2375, Bias: -6.899130, T: 24220, Avg. loss: 0.164797\n",
      "Total training time: 2.71 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 16.76, NNZs: 1225, Bias: -16.274866, T: 36330, Avg. loss: 0.076674\n",
      "Total training time: 2.80 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 29.91, NNZs: 2394, Bias: -7.312323, T: 24220, Avg. loss: 0.188168\n",
      "Total training time: 2.78 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 15.72, NNZs: 1233, Bias: -9.450584, T: 36330, Avg. loss: 0.040709\n",
      "Total training time: 2.89 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 17.76, NNZs: 1380, Bias: -10.569963, T: 36330, Avg. loss: 0.063871\n",
      "Total training time: 2.97 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 13.65, NNZs: 1047, Bias: -6.453786, T: 48440, Avg. loss: 0.028346\n",
      "Total training time: 3.30 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 19.08, NNZs: 1758, Bias: -4.467125, T: 36330, Avg. loss: 0.043762\n",
      "Total training time: 3.29 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 37.44, NNZs: 3519, Bias: -2.078178, T: 24220, Avg. loss: 0.249510\n",
      "Total training time: 3.33 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 25.98, NNZs: 2445, Bias: -3.101600, T: 36330, Avg. loss: 0.111214\n",
      "Total training time: 3.49 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 13.88, NNZs: 1106, Bias: -8.589698, T: 48440, Avg. loss: 0.027151\n",
      "Total training time: 3.57 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14.66, NNZs: 1095, Bias: -14.487209, T: 48440, Avg. loss: 0.056616\n",
      "Total training time: 3.67 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 24.58, NNZs: 2380, Bias: -5.468420, T: 36330, Avg. loss: 0.100542\n",
      "Total training time: 3.74 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 13.51, NNZs: 1084, Bias: -7.891718, T: 48440, Avg. loss: 0.027086\n",
      "Total training time: 3.73 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 23.04, NNZs: 2152, Bias: -4.335037, T: 36330, Avg. loss: 0.078733\n",
      "Total training time: 3.74 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 23.91, NNZs: 2407, Bias: -2.152248, T: 36330, Avg. loss: 0.077686Norm: 23.44, NNZs: 2175, Bias: -4.458052, T: 36330, Avg. loss: 0.083092\n",
      "Total training time: 3.92 seconds.\n",
      "-- Epoch 4\n",
      "\n",
      "Total training time: 3.95 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 15.36, NNZs: 1234, Bias: -8.911318, T: 48440, Avg. loss: 0.038192\n",
      "Total training time: 3.98 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.20, NNZs: 966, Bias: -5.481031, T: 60550, Avg. loss: 0.022081\n",
      "Total training time: 4.09 seconds.\n",
      "Norm: 16.04, NNZs: 1555, Bias: -3.547499, T: 48440, Avg. loss: 0.025928\n",
      "Total training time: 4.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.43, NNZs: 987, Bias: -7.433711, T: 60550, Avg. loss: 0.020269\n",
      "Total training time: 4.35 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    4.4s remaining:   21.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 13.41, NNZs: 1030, Bias: -13.057757, T: 60550, Avg. loss: 0.044838\n",
      "Total training time: 4.47 seconds.\n",
      "Norm: 22.07, NNZs: 2251, Bias: -2.057628, T: 48440, Avg. loss: 0.069292\n",
      "Total training time: 4.49 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.23, NNZs: 1001, Bias: -6.838246, T: 60550, Avg. loss: 0.020236\n",
      "Total training time: 4.52 seconds.\n",
      "Norm: 20.84, NNZs: 2190, Bias: -4.158861, T: 48440, Avg. loss: 0.067301\n",
      "Total training time: 4.70 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 19.47, NNZs: 1957, Bias: -3.292124, T: 48440, Avg. loss: 0.052059\n",
      "Total training time: 4.68 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 13.77, NNZs: 1158, Bias: -7.704421, T: 60550, Avg. loss: 0.030458\n",
      "Total training time: 4.72 seconds.\n",
      "Norm: 29.17, NNZs: 3182, Bias: -1.473976, T: 36330, Avg. loss: 0.110303\n",
      "Total training time: 4.79 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 20.55, NNZs: 2231, Bias: -1.722990, T: 48440, Avg. loss: 0.058543\n",
      "Total training time: 4.81 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 19.93, NNZs: 2003, Bias: -3.412737, T: 48440, Avg. loss: 0.049446\n",
      "Total training time: 4.83 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14.20, NNZs: 1393, Bias: -2.854338, T: 60550, Avg. loss: 0.020164\n",
      "Total training time: 4.85 seconds.\n",
      "Norm: 19.63, NNZs: 2119, Bias: -1.946734, T: 60550, Avg. loss: 0.057419\n",
      "Total training time: 5.16 seconds.\n",
      "Norm: 18.47, NNZs: 2072, Bias: -3.197623, T: 60550, Avg. loss: 0.051518\n",
      "Total training time: 5.36 seconds.\n",
      "Norm: 17.38, NNZs: 1818, Bias: -2.491839, T: 60550, Avg. loss: 0.042448\n",
      "Total training time: 5.42 seconds.\n",
      "Norm: 17.62, NNZs: 1883, Bias: -2.443380, T: 60550, Avg. loss: 0.038722\n",
      "Total training time: 5.45 seconds.\n",
      "Norm: 18.20, NNZs: 2095, Bias: -1.442477, T: 60550, Avg. loss: 0.046175\n",
      "Total training time: 5.47 seconds.\n",
      "Norm: 24.75, NNZs: 2904, Bias: -1.090755, T: 48440, Avg. loss: 0.079024\n",
      "Total training time: 5.75 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 22.00, NNZs: 2760, Bias: -0.989983, T: 60550, Avg. loss: 0.060472\n",
      "Total training time: 6.57 seconds.\n",
      "Accuracy on validation set:  0.7761941448382126\n",
      "--------------------\n",
      "Classifier with alpha: 0.0005\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    6.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 60.23, NNZs: 2386, Bias: -31.067146, T: 12110, Avg. loss: 1.188602\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 53.01, NNZs: 2147, Bias: -31.459736, T: 12110, Avg. loss: 0.755753\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 56.88, NNZs: 2363, Bias: -25.805044, T: 12110, Avg. loss: 0.730138\n",
      "Total training time: 0.99 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 56.79, NNZs: 2407, Bias: -26.551566, T: 12110, Avg. loss: 0.640879\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 55.38, NNZs: 2261, Bias: -26.544274, T: 12110, Avg. loss: 0.817546\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 71.94, NNZs: 3203, Bias: -23.599053, T: 12110, Avg. loss: 1.506799\n",
      "Total training time: 1.14 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 95.75, NNZs: 4145, Bias: -21.016909, T: 12110, Avg. loss: 4.010774\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 95.05, NNZs: 4298, Bias: -18.102397, T: 12110, Avg. loss: 3.140092\n",
      "Total training time: 1.19 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 89.04, NNZs: 4139, Bias: -27.243055, T: 12110, Avg. loss: 3.219783\n",
      "Total training time: 1.20 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 84.54, NNZs: 3725, Bias: -20.168948, T: 12110, Avg. loss: 2.630293\n",
      "Total training time: 1.21 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 83.46, NNZs: 3775, Bias: -22.234434, T: 12110, Avg. loss: 2.448992\n",
      "Total training time: 1.36 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 110.24, NNZs: 5007, Bias: -9.404535, T: 12110, Avg. loss: 5.099092\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 40.87, NNZs: 2277, Bias: -23.309442, T: 24220, Avg. loss: 0.170961\n",
      "Total training time: 1.87 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 35.74, NNZs: 2059, Bias: -24.255084, T: 24220, Avg. loss: 0.102196\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 35.96, NNZs: 2242, Bias: -20.914406, T: 24220, Avg. loss: 0.080592\n",
      "Total training time: 2.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 35.66, NNZs: 2126, Bias: -18.809012, T: 24220, Avg. loss: 0.102688\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 36.35, NNZs: 2225, Bias: -19.923586, T: 24220, Avg. loss: 0.082310\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 46.56, NNZs: 3026, Bias: -14.844220, T: 24220, Avg. loss: 0.148355\n",
      "Total training time: 2.33 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 61.11, NNZs: 3912, Bias: -10.757348, T: 24220, Avg. loss: 0.439294\n",
      "Total training time: 2.55 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 59.08, NNZs: 4059, Bias: -8.645898, T: 24220, Avg. loss: 0.310688\n",
      "Total training time: 2.64 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 57.97, NNZs: 3886, Bias: -17.002803, T: 24220, Avg. loss: 0.383513\n",
      "Total training time: 2.66 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 54.50, NNZs: 3546, Bias: -11.766066, T: 24220, Avg. loss: 0.309513\n",
      "Total training time: 2.85 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 53.98, NNZs: 3538, Bias: -13.225228, T: 24220, Avg. loss: 0.274985\n",
      "Total training time: 2.85 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 32.09, NNZs: 2141, Bias: -19.359503, T: 36330, Avg. loss: 0.071134\n",
      "Total training time: 2.94 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 69.39, NNZs: 4668, Bias: -4.124194, T: 24220, Avg. loss: 0.458521\n",
      "Total training time: 3.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 27.90, NNZs: 1965, Bias: -21.159365, T: 36330, Avg. loss: 0.047895\n",
      "Total training time: 3.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 27.86, NNZs: 2010, Bias: -15.076599, T: 36330, Avg. loss: 0.049092\n",
      "Total training time: 3.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 28.43, NNZs: 2071, Bias: -17.491129, T: 36330, Avg. loss: 0.038858\n",
      "Total training time: 3.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 28.67, NNZs: 2087, Bias: -16.379747, T: 36330, Avg. loss: 0.040818\n",
      "Total training time: 3.29 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 35.60, NNZs: 2861, Bias: -11.103624, T: 36330, Avg. loss: 0.046682\n",
      "Total training time: 3.48 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 46.63, NNZs: 3684, Bias: -7.779912, T: 36330, Avg. loss: 0.159756\n",
      "Total training time: 3.94 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 27.42, NNZs: 2001, Bias: -16.850299, T: 48440, Avg. loss: 0.041150\n",
      "Total training time: 3.96 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 23.36, NNZs: 1870, Bias: -13.149839, T: 48440, Avg. loss: 0.031165\n",
      "Total training time: 4.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 24.30, NNZs: 1834, Bias: -18.628808, T: 48440, Avg. loss: 0.032868\n",
      "Total training time: 4.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 44.44, NNZs: 3814, Bias: -5.777656, T: 36330, Avg. loss: 0.093811\n",
      "Total training time: 4.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 45.06, NNZs: 3695, Bias: -11.367046, T: 36330, Avg. loss: 0.144420\n",
      "Total training time: 4.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 24.28, NNZs: 1926, Bias: -15.214256, T: 48440, Avg. loss: 0.023595\n",
      "Total training time: 4.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 23.99, NNZs: 1940, Bias: -14.475169, T: 48440, Avg. loss: 0.021413\n",
      "Total training time: 4.35 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 52.38, NNZs: 4438, Bias: -3.090428, T: 36330, Avg. loss: 0.145743\n",
      "Total training time: 4.40 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 41.92, NNZs: 3319, Bias: -8.049079, T: 36330, Avg. loss: 0.102168\n",
      "Total training time: 4.54 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 29.20, NNZs: 2627, Bias: -9.183157, T: 48440, Avg. loss: 0.021603\n",
      "Total training time: 4.53 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 41.46, NNZs: 3323, Bias: -8.800406, T: 36330, Avg. loss: 0.104788\n",
      "Total training time: 4.55 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 24.00, NNZs: 1890, Bias: -15.249770, T: 60550, Avg. loss: 0.027725\n",
      "Total training time: 4.92 seconds.\n",
      "Norm: 22.13, NNZs: 1726, Bias: -16.801073, T: 60550, Avg. loss: 0.025293\n",
      "Total training time: 4.94 seconds.\n",
      "Norm: 21.18, NNZs: 1772, Bias: -11.139476, T: 60550, Avg. loss: 0.024570\n",
      "Total training time: 5.04 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    5.0s remaining:   24.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 21.30, NNZs: 1813, Bias: -12.980795, T: 60550, Avg. loss: 0.016845\n",
      "Total training time: 5.22 seconds.\n",
      "Norm: 37.22, NNZs: 3456, Bias: -8.839055, T: 48440, Avg. loss: 0.067921\n",
      "Total training time: 5.28 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 21.49, NNZs: 1791, Bias: -13.471469, T: 60550, Avg. loss: 0.015693\n",
      "Total training time: 5.30 seconds.\n",
      "Norm: 36.69, NNZs: 3566, Bias: -4.131590, T: 48440, Avg. loss: 0.055573\n",
      "Total training time: 5.29 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 39.09, NNZs: 3511, Bias: -5.205605, T: 48440, Avg. loss: 0.087082\n",
      "Total training time: 5.32 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 25.22, NNZs: 2472, Bias: -7.701146, T: 60550, Avg. loss: 0.013086\n",
      "Total training time: 5.49 seconds.\n",
      "Norm: 34.59, NNZs: 3152, Bias: -6.031619, T: 48440, Avg. loss: 0.047022\n",
      "Total training time: 5.68 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 43.35, NNZs: 4202, Bias: -2.343756, T: 48440, Avg. loss: 0.077357\n",
      "Total training time: 5.73 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 34.08, NNZs: 3089, Bias: -6.839724, T: 48440, Avg. loss: 0.054086\n",
      "Total training time: 5.79 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 32.17, NNZs: 3288, Bias: -7.096160, T: 60550, Avg. loss: 0.039147\n",
      "Total training time: 6.15 seconds.\n",
      "Norm: 33.81, NNZs: 3353, Bias: -4.545136, T: 60550, Avg. loss: 0.054837\n",
      "Total training time: 6.19 seconds.\n",
      "Norm: 31.53, NNZs: 3393, Bias: -3.483967, T: 60550, Avg. loss: 0.033428\n",
      "Total training time: 6.20 seconds.\n",
      "Norm: 30.02, NNZs: 3037, Bias: -4.920980, T: 60550, Avg. loss: 0.030678\n",
      "Total training time: 6.50 seconds.\n",
      "Norm: 29.42, NNZs: 2926, Bias: -5.307663, T: 60550, Avg. loss: 0.036041\n",
      "Total training time: 6.58 seconds.\n",
      "Norm: 37.23, NNZs: 3995, Bias: -1.944848, T: 60550, Avg. loss: 0.044677\n",
      "Total training time: 6.69 seconds.\n",
      "Accuracy on validation set:  0.7669491525423728\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    6.7s finished\n"
     ]
    }
   ],
   "source": [
    "# tune hyper-parameter alpha\n",
    "hyper_param_values = [1e-2, 5e-3, 3e-3, 1e-3, 5e-4]\n",
    "accuracies = []\n",
    "\n",
    "for hyper_param_value in hyper_param_values:\n",
    "    \n",
    "    print(f\"Classifier with alpha: {hyper_param_value}\")\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = SGDClassifier(loss='hinge', penalty='elasticnet', alpha=hyper_param_value, random_state=42, max_iter=5, tol=None, n_jobs=-1, verbose=1)\n",
    "    clf.fit(tf_idf_matrix_train, y_train)\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    y_pred = clf.predict(tf_idf_matrix_val)\n",
    "    acc_score = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(acc_score)\n",
    "    print('Accuracy on validation set: ', acc_score)\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studied hyper-parameters and the corresponding accuracies on the validation set: \n",
      "Alpha: 0.0100, Accuracy: 0.7862\n",
      "Alpha: 0.0050, Accuracy: 0.7978\n",
      "Alpha: 0.0030, Accuracy: 0.7985\n",
      "Alpha: 0.0010, Accuracy: 0.7762\n",
      "Alpha: 0.0005, Accuracy: 0.7669\n"
     ]
    }
   ],
   "source": [
    "print(\"Studied hyper-parameters and the corresponding accuracies on the validation set: \")\n",
    "_ = [print(f\"Alpha: {param:.4f}, Accuracy: {acc:.4f}\") for param, acc in zip(hyper_param_values, accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected alpha: 0.003\n"
     ]
    }
   ],
   "source": [
    "best_alpha = hyper_param_values[np.argmax(accuracies)]\n",
    "print(f\"Selected alpha: {best_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 10.42, NNZs: 457, Bias: -6.711654, T: 12110, Avg. loss: 0.159927\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.19, NNZs: 496, Bias: -5.128978, T: 12110, Avg. loss: 0.182287\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.74, NNZs: 535, Bias: -8.091376, T: 12110, Avg. loss: 0.243660\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.43, NNZs: 390, Bias: -11.637950, T: 12110, Avg. loss: 0.197140\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.60, NNZs: 499, Bias: -6.727964, T: 12110, Avg. loss: 0.155381\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 14.16, NNZs: 829, Bias: -3.746797, T: 12110, Avg. loss: 0.320127\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 17.16, NNZs: 993, Bias: -4.920647, T: 12110, Avg. loss: 0.627009\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 16.76, NNZs: 1007, Bias: -4.218785, T: 12110, Avg. loss: 0.523167\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 17.04, NNZs: 973, Bias: -4.148720, T: 12110, Avg. loss: 0.546358\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.08, NNZs: 1228, Bias: -1.655469, T: 12110, Avg. loss: 0.594390\n",
      "Total training time: 0.96 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 22.42, NNZs: 1711, Bias: -1.101556, T: 12110, Avg. loss: 1.030681\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.88, NNZs: 1188, Bias: -3.021067, T: 12110, Avg. loss: 0.879291\n",
      "Total training time: 1.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8.40, NNZs: 352, Bias: -5.747086, T: 24220, Avg. loss: 0.081471\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.93, NNZs: 286, Bias: -3.097897, T: 24220, Avg. loss: 0.048321\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.01, NNZs: 542, Bias: -1.769442, T: 24220, Avg. loss: 0.062766\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.27, NNZs: 245, Bias: -9.408374, T: 24220, Avg. loss: 0.090467\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.21, NNZs: 328, Bias: -4.651069, T: 24220, Avg. loss: 0.053180\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.20, NNZs: 736, Bias: -2.185099, T: 24220, Avg. loss: 0.117908\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 11.87, NNZs: 731, Bias: -2.025494, T: 24220, Avg. loss: 0.104613\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.59, NNZs: 282, Bias: -4.747544, T: 24220, Avg. loss: 0.054942\n",
      "Total training time: 1.62 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.07, NNZs: 711, Bias: -1.954730, T: 24220, Avg. loss: 0.104202\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 13.00, NNZs: 871, Bias: -1.137976, T: 24220, Avg. loss: 0.103293\n",
      "Total training time: 1.79 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 16.02, NNZs: 1407, Bias: -0.881345, T: 24220, Avg. loss: 0.164439\n",
      "Total training time: 1.92 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 13.40, NNZs: 853, Bias: -1.366757, T: 24220, Avg. loss: 0.129055\n",
      "Total training time: 2.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.09, NNZs: 319, Bias: -4.435049, T: 36330, Avg. loss: 0.059436\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.54, NNZs: 261, Bias: -2.154480, T: 36330, Avg. loss: 0.036567\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.38, NNZs: 508, Bias: -1.360390, T: 36330, Avg. loss: 0.048460\n",
      "Total training time: 2.21 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.71, NNZs: 279, Bias: -3.525372, T: 36330, Avg. loss: 0.039663\n",
      "Total training time: 2.26 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.22, NNZs: 231, Bias: -8.172102, T: 36330, Avg. loss: 0.071641\n",
      "Total training time: 2.25 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.31, NNZs: 248, Bias: -3.688471, T: 36330, Avg. loss: 0.040018\n",
      "Total training time: 2.25 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 10.13, NNZs: 645, Bias: -1.633291, T: 36330, Avg. loss: 0.088355\n",
      "Total training time: 2.35 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 10.04, NNZs: 679, Bias: -1.506489, T: 36330, Avg. loss: 0.084669\n",
      "Total training time: 2.39 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 10.07, NNZs: 660, Bias: -1.468299, T: 36330, Avg. loss: 0.079674\n",
      "Total training time: 2.43 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 10.92, NNZs: 840, Bias: -1.027449, T: 36330, Avg. loss: 0.090289\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.27, NNZs: 294, Bias: -3.605860, T: 48440, Avg. loss: 0.048128\n",
      "Total training time: 2.72 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.79, NNZs: 255, Bias: -1.679523, T: 48440, Avg. loss: 0.029718\n",
      "Total training time: 2.82 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 13.55, NNZs: 1389, Bias: -0.747508, T: 36330, Avg. loss: 0.138155\n",
      "Total training time: 2.83 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.46, NNZs: 515, Bias: -1.182522, T: 48440, Avg. loss: 0.046199\n",
      "Total training time: 2.88 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.55, NNZs: 231, Bias: -2.988412, T: 48440, Avg. loss: 0.032991\n",
      "Total training time: 2.91 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.90, NNZs: 280, Bias: -2.875126, T: 48440, Avg. loss: 0.032963\n",
      "Total training time: 2.95 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.53, NNZs: 216, Bias: -7.316142, T: 48440, Avg. loss: 0.061616\n",
      "Total training time: 2.96 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11.30, NNZs: 811, Bias: -1.166834, T: 36330, Avg. loss: 0.109218\n",
      "Total training time: 2.99 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.89, NNZs: 661, Bias: -1.364031, T: 48440, Avg. loss: 0.072672\n",
      "Total training time: 3.16 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.04, NNZs: 640, Bias: -1.495929, T: 48440, Avg. loss: 0.083003\n",
      "Total training time: 3.18 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.96, NNZs: 659, Bias: -1.403852, T: 48440, Avg. loss: 0.076756\n",
      "Total training time: 3.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.67, NNZs: 296, Bias: -3.021473, T: 60550, Avg. loss: 0.041629\n",
      "Total training time: 3.33 seconds.\n",
      "Norm: 9.73, NNZs: 853, Bias: -0.984734, T: 48440, Avg. loss: 0.087213\n",
      "Total training time: 3.33 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.26, NNZs: 246, Bias: -1.464988, T: 60550, Avg. loss: 0.027743\n",
      "Total training time: 3.59 seconds.\n",
      "Norm: 5.40, NNZs: 280, Bias: -2.358862, T: 60550, Avg. loss: 0.029195\n",
      "Total training time: 3.61 seconds.\n",
      "Norm: 6.84, NNZs: 526, Bias: -1.135072, T: 60550, Avg. loss: 0.043917\n",
      "Total training time: 3.60 seconds.\n",
      "Norm: 5.07, NNZs: 237, Bias: -2.550861, T: 60550, Avg. loss: 0.029402\n",
      "Total training time: 3.63 seconds.\n",
      "Norm: 5.13, NNZs: 218, Bias: -6.653224, T: 60550, Avg. loss: 0.055638\n",
      "Total training time: 3.64 seconds.\n",
      "Norm: 12.03, NNZs: 1398, Bias: -0.790338, T: 48440, Avg. loss: 0.125342\n",
      "Total training time: 3.76 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 10.10, NNZs: 812, Bias: -1.121070, T: 48440, Avg. loss: 0.100467\n",
      "Total training time: 3.76 seconds.\n",
      "-- Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    3.6s remaining:   18.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 8.12, NNZs: 651, Bias: -1.230316, T: 60550, Avg. loss: 0.069974\n",
      "Total training time: 3.80 seconds.\n",
      "Norm: 8.24, NNZs: 638, Bias: -1.448569, T: 60550, Avg. loss: 0.079560\n",
      "Total training time: 3.89 seconds.\n",
      "Norm: 8.24, NNZs: 668, Bias: -1.303592, T: 60550, Avg. loss: 0.074313\n",
      "Total training time: 3.92 seconds.\n",
      "Norm: 8.89, NNZs: 858, Bias: -0.941777, T: 60550, Avg. loss: 0.082084\n",
      "Total training time: 3.97 seconds.\n",
      "Norm: 9.29, NNZs: 839, Bias: -1.157015, T: 60550, Avg. loss: 0.095667\n",
      "Total training time: 4.24 seconds.\n",
      "Norm: 11.05, NNZs: 1422, Bias: -0.741356, T: 60550, Avg. loss: 0.116957\n",
      "Total training time: 4.30 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.003, max_iter=5, n_jobs=-1, penalty='elasticnet',\n",
       "              random_state=42, tol=None, verbose=1)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit classifier with best hyper-parameter\n",
    "clfs[0] = SGDClassifier(loss='hinge', penalty='elasticnet', alpha=best_alpha, random_state=42, max_iter=5, tol=None, n_jobs=-1, verbose=1)\n",
    "clfs[0].fit(tf_idf_matrix_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7985362095531587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.19      0.27        53\n",
      "           1       0.52      0.22      0.31       122\n",
      "           2       0.90      0.87      0.89       140\n",
      "           3       0.80      0.86      0.83       420\n",
      "           4       0.90      0.91      0.90       665\n",
      "           5       0.57      0.52      0.54       113\n",
      "           6       0.33      0.04      0.06        28\n",
      "           7       0.81      0.66      0.72        38\n",
      "           8       0.86      0.84      0.85       102\n",
      "           9       0.76      0.87      0.81       546\n",
      "          10       0.71      0.75      0.73       195\n",
      "          11       0.78      0.87      0.83       174\n",
      "\n",
      "    accuracy                           0.80      2596\n",
      "   macro avg       0.70      0.63      0.65      2596\n",
      "weighted avg       0.78      0.80      0.78      2596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on validation set\n",
    "y_preds[0] = clfs[0].predict(tf_idf_matrix_val)\n",
    "val_accs[0] = accuracy_score(y_val, y_preds[0])\n",
    "print('Accuracy on validation set: ', val_accs[0])\n",
    "\n",
    "# additionally, we check the classification report\n",
    "# look at F1-Score, Precision, Recall, and relationship between Support & metrics\n",
    "print(classification_report(y_val, y_preds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Experiment 2: TF-IDF, low-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier with 5 maximum iterations.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 4.28, NNZs: 52, Bias: -2.054124, T: 12110, Avg. loss: 0.175307\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 4.56, NNZs: 49, Bias: -2.214191, T: 12110, Avg. loss: 0.085581\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.81, NNZs: 48, Bias: -1.008379, T: 12110, Avg. loss: 0.382438\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.83, NNZs: 52, Bias: -0.992930, T: 12110, Avg. loss: 0.556248\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.86, NNZs: 40, Bias: -1.060754, T: 24220, Avg. loss: 0.053242\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.86, NNZs: 47, Bias: -0.925892, T: 12110, Avg. loss: 0.321753\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.74, NNZs: 43, Bias: -1.202278, T: 12110, Avg. loss: 0.340742\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.72, NNZs: 46, Bias: -4.284778, T: 12110, Avg. loss: 0.135340\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.44, NNZs: 55, Bias: -1.079152, T: 12110, Avg. loss: 0.184828\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.87, NNZs: 47, Bias: -2.013513, T: 12110, Avg. loss: 0.125654\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 2.34, NNZs: 38, Bias: -1.037291, T: 36330, Avg. loss: 0.045373\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.29, NNZs: 61, Bias: -0.792364, T: 24220, Avg. loss: 0.141946\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.09, NNZs: 42, Bias: -1.323091, T: 24220, Avg. loss: 0.039793\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.65, NNZs: 41, Bias: -2.261134, T: 24220, Avg. loss: 0.048649\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.57, NNZs: 50, Bias: -0.855506, T: 24220, Avg. loss: 0.157832\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.05, NNZs: 57, Bias: -1.354272, T: 12110, Avg. loss: 0.290237\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.69, NNZs: 35, Bias: -1.107219, T: 24220, Avg. loss: 0.106101\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.06, NNZs: 42, Bias: -1.045242, T: 48440, Avg. loss: 0.043258\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.60, NNZs: 44, Bias: -1.102872, T: 36330, Avg. loss: 0.036529\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.08, NNZs: 43, Bias: -1.221114, T: 12110, Avg. loss: 0.114648\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.76, NNZs: 66, Bias: -0.841685, T: 36330, Avg. loss: 0.133055\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.75, NNZs: 56, Bias: -1.010818, T: 24220, Avg. loss: 0.116790\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.30, NNZs: 42, Bias: -1.079097, T: 24220, Avg. loss: 0.046647\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.17, NNZs: 34, Bias: -1.238338, T: 36330, Avg. loss: 0.033684\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.20, NNZs: 64, Bias: -0.729534, T: 12110, Avg. loss: 0.555616\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.86, NNZs: 36, Bias: -1.001036, T: 60550, Avg. loss: 0.042964\n",
      "Total training time: 0.06 seconds.\n",
      "Norm: 3.13, NNZs: 30, Bias: -1.081421, T: 36330, Avg. loss: 0.095605\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.42, NNZs: 45, Bias: -1.072519, T: 48440, Avg. loss: 0.036381\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.80, NNZs: 63, Bias: -1.179646, T: 24220, Avg. loss: 0.123296\n",
      "Total training time: 0.06 seconds.\n",
      "Norm: 2.63, NNZs: 47, Bias: -1.105905, T: 36330, Avg. loss: 0.040833\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.53, NNZs: 70, Bias: -0.850087, T: 48440, Avg. loss: 0.130571\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "-- Epoch 3\n",
      "Norm: 3.32, NNZs: 64, Bias: -1.057457, T: 24220, Avg. loss: 0.077719\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.78, NNZs: 40, Bias: -1.005224, T: 24220, Avg. loss: 0.040646\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.30, NNZs: 44, Bias: -1.026056, T: 60550, Avg. loss: 0.035923\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 3.16, NNZs: 56, Bias: -1.042360, T: 36330, Avg. loss: 0.108318\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.78, NNZs: 35, Bias: -1.054504, T: 48440, Avg. loss: 0.091993\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.42, NNZs: 52, Bias: -1.041421, T: 48440, Avg. loss: 0.040565\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.79, NNZs: 29, Bias: -1.014494, T: 48440, Avg. loss: 0.027998\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.31, NNZs: 72, Bias: -0.844808, T: 60550, Avg. loss: 0.128293\n",
      "Total training time: 0.08 seconds.\n",
      "Norm: 2.95, NNZs: 66, Bias: -0.871479, T: 36330, Avg. loss: 0.072530\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.31, NNZs: 67, Bias: -1.188895, T: 36330, Avg. loss: 0.119376\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.53, NNZs: 33, Bias: -1.018872, T: 60550, Avg. loss: 0.090407\n",
      "Total training time: 0.08 seconds.\n",
      "Norm: 2.27, NNZs: 55, Bias: -1.068663, T: 60550, Avg. loss: 0.038757\n",
      "Total training time: 0.09 seconds.\n",
      "Norm: 4.84, NNZs: 71, Bias: -0.505935, T: 24220, Avg. loss: 0.232047\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.79, NNZs: 55, Bias: -1.030513, T: 48440, Avg. loss: 0.104199\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.34, NNZs: 43, Bias: -0.953127, T: 36330, Avg. loss: 0.038707\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.92, NNZs: 61, Bias: -0.828342, T: 36330, Avg. loss: 0.151219\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.70, NNZs: 67, Bias: -0.895361, T: 48440, Avg. loss: 0.071055\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.60, NNZs: 32, Bias: -0.992656, T: 60550, Avg. loss: 0.026884\n",
      "Total training time: 0.10 seconds.\n",
      "Norm: 3.04, NNZs: 68, Bias: -1.103322, T: 48440, Avg. loss: 0.116470\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.48, NNZs: 57, Bias: -0.993187, T: 60550, Avg. loss: 0.101880\n",
      "Total training time: 0.10 seconds.\n",
      "Norm: 4.21, NNZs: 74, Bias: -0.516020, T: 36330, Avg. loss: 0.220745\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.09, NNZs: 40, Bias: -1.075941, T: 48440, Avg. loss: 0.037203\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.53, NNZs: 68, Bias: -0.927839, T: 60550, Avg. loss: 0.069667\n",
      "Total training time: 0.10 seconds.\n",
      "Norm: 3.56, NNZs: 59, Bias: -0.949453, T: 48440, Avg. loss: 0.146809\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.81, NNZs: 69, Bias: -1.026670, T: 60550, Avg. loss: 0.114669\n",
      "Total training time: 0.11 seconds.\n",
      "Norm: 1.90, NNZs: 44, Bias: -1.069048, T: 60550, Avg. loss: 0.037275\n",
      "Total training time: 0.10 seconds.\n",
      "Norm: 3.86, NNZs: 70, Bias: -0.519921, T: 48440, Avg. loss: 0.216367\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.29, NNZs: 60, Bias: -0.978914, T: 60550, Avg. loss: 0.143817\n",
      "Total training time: 0.12 seconds.\n",
      "Norm: 3.69, NNZs: 76, Bias: -0.528953, T: 60550, Avg. loss: 0.211507\n",
      "Total training time: 0.10 seconds.\n",
      "Accuracy on validation set:  0.7865947611710323\n",
      "--------------------\n",
      "Classifier with 10 maximum iterations.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.1s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 5.83, NNZs: 52, Bias: -0.992930, T: 12110, Avg. loss: 0.556248\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 3.72, NNZs: 46, Bias: -4.284778, T: 12110, Avg. loss: 0.135340\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.81, NNZs: 48, Bias: -1.008379, T: 12110, Avg. loss: 0.382438\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.57, NNZs: 50, Bias: -0.855506, T: 24220, Avg. loss: 0.157832\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.08, NNZs: 43, Bias: -1.221114, T: 12110, Avg. loss: 0.114648\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.74, NNZs: 43, Bias: -1.202278, T: 12110, Avg. loss: 0.340742\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 4.28, NNZs: 52, Bias: -2.054124, T: 12110, Avg. loss: 0.175307\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 4.86, NNZs: 47, Bias: -0.925892, T: 12110, Avg. loss: 0.321753\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 2.65, NNZs: 41, Bias: -2.261134, T: 24220, Avg. loss: 0.048649\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.92, NNZs: 61, Bias: -0.828342, T: 36330, Avg. loss: 0.151219\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.56, NNZs: 49, Bias: -2.214191, T: 12110, Avg. loss: 0.085581\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.78, NNZs: 40, Bias: -1.005224, T: 24220, Avg. loss: 0.040646\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.86, NNZs: 40, Bias: -1.060754, T: 24220, Avg. loss: 0.053242\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.69, NNZs: 35, Bias: -1.107219, T: 24220, Avg. loss: 0.106101\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.17, NNZs: 34, Bias: -1.238338, T: 36330, Avg. loss: 0.033684\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.20, NNZs: 64, Bias: -0.729534, T: 12110, Avg. loss: 0.555616\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.87, NNZs: 47, Bias: -2.013513, T: 12110, Avg. loss: 0.125654\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.44, NNZs: 55, Bias: -1.079152, T: 12110, Avg. loss: 0.184828\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.75, NNZs: 56, Bias: -1.010818, T: 24220, Avg. loss: 0.116790\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.34, NNZs: 38, Bias: -1.037291, T: 36330, Avg. loss: 0.045373\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.56, NNZs: 59, Bias: -0.949453, T: 48440, Avg. loss: 0.146809\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.34, NNZs: 43, Bias: -0.953127, T: 36330, Avg. loss: 0.038707\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.29, NNZs: 61, Bias: -0.792364, T: 24220, Avg. loss: 0.141946\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.13, NNZs: 30, Bias: -1.081421, T: 36330, Avg. loss: 0.095605\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.79, NNZs: 29, Bias: -1.014494, T: 48440, Avg. loss: 0.027998\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.05, NNZs: 57, Bias: -1.354272, T: 12110, Avg. loss: 0.290237\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.09, NNZs: 42, Bias: -1.323091, T: 24220, Avg. loss: 0.039793\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.06, NNZs: 42, Bias: -1.045242, T: 48440, Avg. loss: 0.043258\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.30, NNZs: 42, Bias: -1.079097, T: 24220, Avg. loss: 0.046647\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.84, NNZs: 71, Bias: -0.505935, T: 24220, Avg. loss: 0.232047\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.09, NNZs: 40, Bias: -1.075941, T: 48440, Avg. loss: 0.037203\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.60, NNZs: 32, Bias: -0.992656, T: 60550, Avg. loss: 0.026884\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.29, NNZs: 60, Bias: -0.978914, T: 60550, Avg. loss: 0.143817\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.78, NNZs: 35, Bias: -1.054504, T: 48440, Avg. loss: 0.091993\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.60, NNZs: 44, Bias: -1.102872, T: 36330, Avg. loss: 0.036529\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.86, NNZs: 36, Bias: -1.001036, T: 60550, Avg. loss: 0.042964\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.80, NNZs: 63, Bias: -1.179646, T: 24220, Avg. loss: 0.123296\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.63, NNZs: 47, Bias: -1.105905, T: 36330, Avg. loss: 0.040833\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.16, NNZs: 56, Bias: -1.042360, T: 36330, Avg. loss: 0.108318\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.44, NNZs: 31, Bias: -1.013834, T: 72660, Avg. loss: 0.026457\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.90, NNZs: 44, Bias: -1.069048, T: 60550, Avg. loss: 0.037275\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.42, NNZs: 45, Bias: -1.072519, T: 48440, Avg. loss: 0.036381\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.68, NNZs: 36, Bias: -1.007472, T: 72660, Avg. loss: 0.042250\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.32, NNZs: 64, Bias: -1.057457, T: 24220, Avg. loss: 0.077719\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.12, NNZs: 63, Bias: -0.955785, T: 72660, Avg. loss: 0.142822\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.53, NNZs: 33, Bias: -1.018872, T: 60550, Avg. loss: 0.090407\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.76, NNZs: 66, Bias: -0.841685, T: 36330, Avg. loss: 0.133055\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.31, NNZs: 29, Bias: -1.006196, T: 84770, Avg. loss: 0.026293\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.57, NNZs: 40, Bias: -0.992631, T: 84770, Avg. loss: 0.041841\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.30, NNZs: 44, Bias: -1.026056, T: 60550, Avg. loss: 0.035923\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 4.21, NNZs: 74, Bias: -0.516020, T: 36330, Avg. loss: 0.220745\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.42, NNZs: 52, Bias: -1.041421, T: 48440, Avg. loss: 0.040565\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.75, NNZs: 44, Bias: -1.021122, T: 72660, Avg. loss: 0.037006\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.95, NNZs: 66, Bias: -0.871479, T: 36330, Avg. loss: 0.072530\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.79, NNZs: 55, Bias: -1.030513, T: 48440, Avg. loss: 0.104199\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.47, NNZs: 42, Bias: -0.997501, T: 96880, Avg. loss: 0.041429\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.25, NNZs: 45, Bias: -1.030285, T: 72660, Avg. loss: 0.035956\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.21, NNZs: 30, Bias: -0.999730, T: 96880, Avg. loss: 0.026225\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.00, NNZs: 67, Bias: -0.926494, T: 84770, Avg. loss: 0.141550\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.34, NNZs: 40, Bias: -1.038846, T: 72660, Avg. loss: 0.088419\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.53, NNZs: 70, Bias: -0.850087, T: 48440, Avg. loss: 0.130571\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.27, NNZs: 55, Bias: -1.068663, T: 60550, Avg. loss: 0.038757\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.86, NNZs: 70, Bias: -0.519921, T: 48440, Avg. loss: 0.216367\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.40, NNZs: 39, Bias: -1.020571, T: 108990, Avg. loss: 0.041290\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.17, NNZs: 45, Bias: -1.018767, T: 84770, Avg. loss: 0.035531\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.14, NNZs: 31, Bias: -0.988095, T: 108990, Avg. loss: 0.026166\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.70, NNZs: 67, Bias: -0.895361, T: 48440, Avg. loss: 0.071055\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.48, NNZs: 57, Bias: -0.993187, T: 60550, Avg. loss: 0.101880\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.31, NNZs: 67, Bias: -1.188895, T: 36330, Avg. loss: 0.119376\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.90, NNZs: 69, Bias: -0.966763, T: 96880, Avg. loss: 0.140123\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.17, NNZs: 33, Bias: -1.030518, T: 84770, Avg. loss: 0.087498\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.15, NNZs: 53, Bias: -1.022542, T: 72660, Avg. loss: 0.038452\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.31, NNZs: 72, Bias: -0.844808, T: 60550, Avg. loss: 0.128293\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.32, NNZs: 36, Bias: -1.009139, T: 121100, Avg. loss: 0.041097\n",
      "Total training time: 0.14 seconds.\n",
      "Norm: 1.65, NNZs: 46, Bias: -1.036209, T: 84770, Avg. loss: 0.036689\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.69, NNZs: 76, Bias: -0.528953, T: 60550, Avg. loss: 0.211507\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.07, NNZs: 24, Bias: -1.003548, T: 121100, Avg. loss: 0.026123\n",
      "Total training time: 0.12 seconds.\n",
      "Norm: 2.11, NNZs: 46, Bias: -1.003805, T: 96880, Avg. loss: 0.035623\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.53, NNZs: 68, Bias: -0.927839, T: 60550, Avg. loss: 0.069667\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.57, NNZs: 48, Bias: -1.018049, T: 96880, Avg. loss: 0.036686\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.04, NNZs: 41, Bias: -1.036385, T: 96880, Avg. loss: 0.086820\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.21, NNZs: 73, Bias: -0.823912, T: 72660, Avg. loss: 0.128315\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.04, NNZs: 68, Bias: -1.103322, T: 48440, Avg. loss: 0.116470\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.27, NNZs: 56, Bias: -1.006930, T: 72660, Avg. loss: 0.099942\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.81, NNZs: 70, Bias: -0.979311, T: 108990, Avg. loss: 0.139017\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.08, NNZs: 55, Bias: -1.036068, T: 84770, Avg. loss: 0.038292\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.49, NNZs: 51, Bias: -1.033835, T: 108990, Avg. loss: 0.036291\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.07, NNZs: 45, Bias: -1.015746, T: 108990, Avg. loss: 0.035560\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.46, NNZs: 71, Bias: -0.963728, T: 72660, Avg. loss: 0.069238\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.15, NNZs: 76, Bias: -0.747406, T: 84770, Avg. loss: 0.126531\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.94, NNZs: 34, Bias: -1.010666, T: 108990, Avg. loss: 0.086693\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 3.50, NNZs: 78, Bias: -0.470003, T: 72660, Avg. loss: 0.212411\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.44, NNZs: 49, Bias: -1.016712, T: 121100, Avg. loss: 0.036352\n",
      "Total training time: 0.14 seconds.\n",
      "Norm: 2.81, NNZs: 69, Bias: -1.026670, T: 60550, Avg. loss: 0.114669\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.02, NNZs: 54, Bias: -1.041072, T: 96880, Avg. loss: 0.038121\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.13, NNZs: 57, Bias: -0.999164, T: 84770, Avg. loss: 0.099295\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.76, NNZs: 69, Bias: -0.944319, T: 121100, Avg. loss: 0.139540\n",
      "Total training time: 0.19 seconds.\n",
      "Norm: 2.03, NNZs: 44, Bias: -1.041720, T: 121100, Avg. loss: 0.035482\n",
      "Total training time: 0.13 seconds.\n",
      "Norm: 1.85, NNZs: 31, Bias: -1.016580, T: 121100, Avg. loss: 0.085922\n",
      "Total training time: 0.19 seconds.\n",
      "Norm: 2.69, NNZs: 74, Bias: -1.096765, T: 72660, Avg. loss: 0.113239\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.40, NNZs: 78, Bias: -0.532759, T: 84770, Avg. loss: 0.210830\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.08, NNZs: 75, Bias: -0.780608, T: 96880, Avg. loss: 0.126697\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.38, NNZs: 73, Bias: -0.938465, T: 84770, Avg. loss: 0.069091\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.97, NNZs: 52, Bias: -1.057309, T: 108990, Avg. loss: 0.037853\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.99, NNZs: 56, Bias: -1.028445, T: 96880, Avg. loss: 0.098096\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.59, NNZs: 74, Bias: -1.087181, T: 84770, Avg. loss: 0.112497\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.93, NNZs: 55, Bias: -1.045578, T: 121100, Avg. loss: 0.037783\n",
      "Total training time: 0.14 seconds.\n",
      "Norm: 3.31, NNZs: 80, Bias: -0.547962, T: 96880, Avg. loss: 0.209632\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.90, NNZs: 53, Bias: -1.032778, T: 108990, Avg. loss: 0.097100\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 3.02, NNZs: 75, Bias: -0.798918, T: 108990, Avg. loss: 0.125416\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.29, NNZs: 74, Bias: -0.975806, T: 96880, Avg. loss: 0.068464\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.51, NNZs: 75, Bias: -1.101516, T: 96880, Avg. loss: 0.112656\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.24, NNZs: 80, Bias: -0.564505, T: 108990, Avg. loss: 0.209220\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.2s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1.80, NNZs: 55, Bias: -1.036172, T: 121100, Avg. loss: 0.096767\n",
      "Total training time: 0.20 seconds.\n",
      "Norm: 2.96, NNZs: 75, Bias: -0.836133, T: 121100, Avg. loss: 0.125539\n",
      "Total training time: 0.21 seconds.\n",
      "Norm: 3.19, NNZs: 81, Bias: -0.536156, T: 121100, Avg. loss: 0.208217\n",
      "Total training time: 0.16 seconds.\n",
      "Norm: 2.44, NNZs: 75, Bias: -1.077080, T: 108990, Avg. loss: 0.112154\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.23, NNZs: 73, Bias: -0.943043, T: 108990, Avg. loss: 0.068686\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.41, NNZs: 75, Bias: -1.046030, T: 121100, Avg. loss: 0.111645\n",
      "Total training time: 0.17 seconds.\n",
      "Norm: 2.19, NNZs: 75, Bias: -0.965431, T: 121100, Avg. loss: 0.068220\n",
      "Total training time: 0.17 seconds.\n",
      "Accuracy on validation set:  0.7885208012326657\n",
      "--------------------\n",
      "Classifier with 15 maximum iterations.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 4.28, NNZs: 52, Bias: -2.054124, T: 12110, Avg. loss: 0.175307\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 2.86, NNZs: 40, Bias: -1.060754, T: 24220, Avg. loss: 0.053242\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 5.83, NNZs: 52, Bias: -0.992930, T: 12110, Avg. loss: 0.556248\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.56, NNZs: 49, Bias: -2.214191, T: 12110, Avg. loss: 0.085581\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.81, NNZs: 48, Bias: -1.008379, T: 12110, Avg. loss: 0.382438\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.08, NNZs: 43, Bias: -1.221114, T: 12110, Avg. loss: 0.114648\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 4.74, NNZs: 43, Bias: -1.202278, T: 12110, Avg. loss: 0.340742\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.29, NNZs: 61, Bias: -0.792364, T: 24220, Avg. loss: 0.141946\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.34, NNZs: 38, Bias: -1.037291, T: 36330, Avg. loss: 0.045373\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.44, NNZs: 55, Bias: -1.079152, T: 12110, Avg. loss: 0.184828\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.05, NNZs: 57, Bias: -1.354272, T: 12110, Avg. loss: 0.290237\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.87, NNZs: 47, Bias: -2.013513, T: 12110, Avg. loss: 0.125654\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.76, NNZs: 66, Bias: -0.841685, T: 36330, Avg. loss: 0.133055\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.06, NNZs: 42, Bias: -1.045242, T: 48440, Avg. loss: 0.043258\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.09, NNZs: 42, Bias: -1.323091, T: 24220, Avg. loss: 0.039793\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.86, NNZs: 47, Bias: -0.925892, T: 12110, Avg. loss: 0.321753\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.72, NNZs: 46, Bias: -4.284778, T: 12110, Avg. loss: 0.135340\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.80, NNZs: 63, Bias: -1.179646, T: 24220, Avg. loss: 0.123296\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.78, NNZs: 40, Bias: -1.005224, T: 24220, Avg. loss: 0.040646\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.30, NNZs: 42, Bias: -1.079097, T: 24220, Avg. loss: 0.046647\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.57, NNZs: 50, Bias: -0.855506, T: 24220, Avg. loss: 0.157832\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.69, NNZs: 35, Bias: -1.107219, T: 24220, Avg. loss: 0.106101\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.86, NNZs: 36, Bias: -1.001036, T: 60550, Avg. loss: 0.042964\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.53, NNZs: 70, Bias: -0.850087, T: 48440, Avg. loss: 0.130571\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.32, NNZs: 64, Bias: -1.057457, T: 24220, Avg. loss: 0.077719\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.65, NNZs: 41, Bias: -2.261134, T: 24220, Avg. loss: 0.048649\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.20, NNZs: 64, Bias: -0.729534, T: 12110, Avg. loss: 0.555616\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.34, NNZs: 43, Bias: -0.953127, T: 36330, Avg. loss: 0.038707\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.31, NNZs: 67, Bias: -1.188895, T: 36330, Avg. loss: 0.119376\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.63, NNZs: 47, Bias: -1.105905, T: 36330, Avg. loss: 0.040833\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.68, NNZs: 36, Bias: -1.007472, T: 72660, Avg. loss: 0.042250\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.31, NNZs: 72, Bias: -0.844808, T: 60550, Avg. loss: 0.128293\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.13, NNZs: 30, Bias: -1.081421, T: 36330, Avg. loss: 0.095605\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.60, NNZs: 44, Bias: -1.102872, T: 36330, Avg. loss: 0.036529\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.17, NNZs: 34, Bias: -1.238338, T: 36330, Avg. loss: 0.033684\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.95, NNZs: 66, Bias: -0.871479, T: 36330, Avg. loss: 0.072530\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.57, NNZs: 40, Bias: -0.992631, T: 84770, Avg. loss: 0.041841\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.42, NNZs: 52, Bias: -1.041421, T: 48440, Avg. loss: 0.040565\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.04, NNZs: 68, Bias: -1.103322, T: 48440, Avg. loss: 0.116470\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.92, NNZs: 61, Bias: -0.828342, T: 36330, Avg. loss: 0.151219\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.21, NNZs: 73, Bias: -0.823912, T: 72660, Avg. loss: 0.128315\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.78, NNZs: 35, Bias: -1.054504, T: 48440, Avg. loss: 0.091993\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.79, NNZs: 29, Bias: -1.014494, T: 48440, Avg. loss: 0.027998\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.70, NNZs: 67, Bias: -0.895361, T: 48440, Avg. loss: 0.071055\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.75, NNZs: 56, Bias: -1.010818, T: 24220, Avg. loss: 0.116790\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.47, NNZs: 42, Bias: -0.997501, T: 96880, Avg. loss: 0.041429\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 4.84, NNZs: 71, Bias: -0.505935, T: 24220, Avg. loss: 0.232047\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.27, NNZs: 55, Bias: -1.068663, T: 60550, Avg. loss: 0.038757\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.09, NNZs: 40, Bias: -1.075941, T: 48440, Avg. loss: 0.037203\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.56, NNZs: 59, Bias: -0.949453, T: 48440, Avg. loss: 0.146809\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.15, NNZs: 76, Bias: -0.747406, T: 84770, Avg. loss: 0.126531\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.81, NNZs: 69, Bias: -1.026670, T: 60550, Avg. loss: 0.114669\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.60, NNZs: 32, Bias: -0.992656, T: 60550, Avg. loss: 0.026884\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.53, NNZs: 33, Bias: -1.018872, T: 60550, Avg. loss: 0.090407\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.53, NNZs: 68, Bias: -0.927839, T: 60550, Avg. loss: 0.069667\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.40, NNZs: 39, Bias: -1.020571, T: 108990, Avg. loss: 0.041290\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.42, NNZs: 45, Bias: -1.072519, T: 48440, Avg. loss: 0.036381\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.16, NNZs: 56, Bias: -1.042360, T: 36330, Avg. loss: 0.108318\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.90, NNZs: 44, Bias: -1.069048, T: 60550, Avg. loss: 0.037275\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 4.21, NNZs: 74, Bias: -0.516020, T: 36330, Avg. loss: 0.220745\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.08, NNZs: 75, Bias: -0.780608, T: 96880, Avg. loss: 0.126697\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.46, NNZs: 71, Bias: -0.963728, T: 72660, Avg. loss: 0.069238\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.44, NNZs: 31, Bias: -1.013834, T: 72660, Avg. loss: 0.026457\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.32, NNZs: 36, Bias: -1.009139, T: 121100, Avg. loss: 0.041097\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.34, NNZs: 40, Bias: -1.038846, T: 72660, Avg. loss: 0.088419\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.69, NNZs: 74, Bias: -1.096765, T: 72660, Avg. loss: 0.113239\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.15, NNZs: 53, Bias: -1.022542, T: 72660, Avg. loss: 0.038452\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.79, NNZs: 55, Bias: -1.030513, T: 48440, Avg. loss: 0.104199\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.02, NNZs: 75, Bias: -0.798918, T: 108990, Avg. loss: 0.125416\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.75, NNZs: 44, Bias: -1.021122, T: 72660, Avg. loss: 0.037006\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.86, NNZs: 70, Bias: -0.519921, T: 48440, Avg. loss: 0.216367\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.38, NNZs: 73, Bias: -0.938465, T: 84770, Avg. loss: 0.069091\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.26, NNZs: 41, Bias: -1.017228, T: 133210, Avg. loss: 0.040744\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 3.29, NNZs: 60, Bias: -0.978914, T: 60550, Avg. loss: 0.143817\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.31, NNZs: 29, Bias: -1.006196, T: 84770, Avg. loss: 0.026293\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.17, NNZs: 33, Bias: -1.030518, T: 84770, Avg. loss: 0.087498\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.30, NNZs: 44, Bias: -1.026056, T: 60550, Avg. loss: 0.035923\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.59, NNZs: 74, Bias: -1.087181, T: 84770, Avg. loss: 0.112497\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.96, NNZs: 75, Bias: -0.836133, T: 121100, Avg. loss: 0.125539\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.65, NNZs: 46, Bias: -1.036209, T: 84770, Avg. loss: 0.036689\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.29, NNZs: 74, Bias: -0.975806, T: 96880, Avg. loss: 0.068464\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.08, NNZs: 55, Bias: -1.036068, T: 84770, Avg. loss: 0.038292\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.48, NNZs: 57, Bias: -0.993187, T: 60550, Avg. loss: 0.101880\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.69, NNZs: 76, Bias: -0.528953, T: 60550, Avg. loss: 0.211507\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.21, NNZs: 30, Bias: -0.999730, T: 96880, Avg. loss: 0.026225\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.04, NNZs: 41, Bias: -1.036385, T: 96880, Avg. loss: 0.086820\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.25, NNZs: 45, Bias: -1.030285, T: 72660, Avg. loss: 0.035956\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.94, NNZs: 77, Bias: -0.805667, T: 133210, Avg. loss: 0.125270\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.21, NNZs: 42, Bias: -1.007549, T: 145320, Avg. loss: 0.040789\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.23, NNZs: 73, Bias: -0.943043, T: 108990, Avg. loss: 0.068686\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.51, NNZs: 75, Bias: -1.101516, T: 96880, Avg. loss: 0.112656\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.57, NNZs: 48, Bias: -1.018049, T: 96880, Avg. loss: 0.036686\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.50, NNZs: 78, Bias: -0.470003, T: 72660, Avg. loss: 0.212411\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.27, NNZs: 56, Bias: -1.006930, T: 72660, Avg. loss: 0.099942\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.12, NNZs: 63, Bias: -0.955785, T: 72660, Avg. loss: 0.142822\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.14, NNZs: 31, Bias: -0.988095, T: 108990, Avg. loss: 0.026166\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.94, NNZs: 34, Bias: -1.010666, T: 108990, Avg. loss: 0.086693\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.02, NNZs: 54, Bias: -1.041072, T: 96880, Avg. loss: 0.038121\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.92, NNZs: 77, Bias: -0.791824, T: 145320, Avg. loss: 0.125140\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.16, NNZs: 44, Bias: -1.005795, T: 157430, Avg. loss: 0.040743\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.19, NNZs: 75, Bias: -0.965431, T: 121100, Avg. loss: 0.068220\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.49, NNZs: 51, Bias: -1.033835, T: 108990, Avg. loss: 0.036291\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.07, NNZs: 24, Bias: -1.003548, T: 121100, Avg. loss: 0.026123\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 3.40, NNZs: 78, Bias: -0.532759, T: 84770, Avg. loss: 0.210830\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.44, NNZs: 75, Bias: -1.077080, T: 108990, Avg. loss: 0.112154\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.13, NNZs: 57, Bias: -0.999164, T: 84770, Avg. loss: 0.099295\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.85, NNZs: 31, Bias: -1.016580, T: 121100, Avg. loss: 0.085922\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.11, NNZs: 42, Bias: -1.015892, T: 169540, Avg. loss: 0.040446\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.88, NNZs: 78, Bias: -0.811754, T: 157430, Avg. loss: 0.124998\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.16, NNZs: 75, Bias: -0.965210, T: 133210, Avg. loss: 0.068020\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.17, NNZs: 45, Bias: -1.018767, T: 84770, Avg. loss: 0.035531\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.00, NNZs: 67, Bias: -0.926494, T: 84770, Avg. loss: 0.141550\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.44, NNZs: 49, Bias: -1.016712, T: 121100, Avg. loss: 0.036352\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.99, NNZs: 56, Bias: -1.028445, T: 96880, Avg. loss: 0.098096\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.14, NNZs: 74, Bias: -0.937262, T: 145320, Avg. loss: 0.067834\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.77, NNZs: 35, Bias: -1.006814, T: 133210, Avg. loss: 0.085567\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.41, NNZs: 75, Bias: -1.046030, T: 121100, Avg. loss: 0.111645\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.07, NNZs: 43, Bias: -1.013638, T: 181650, Avg. loss: 0.040463\n",
      "Total training time: 0.20 seconds.\n",
      "Norm: 1.97, NNZs: 52, Bias: -1.057309, T: 108990, Avg. loss: 0.037853\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.88, NNZs: 78, Bias: -0.813071, T: 169540, Avg. loss: 0.125118\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.02, NNZs: 28, Bias: -1.000981, T: 133210, Avg. loss: 0.026069\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.90, NNZs: 69, Bias: -0.966763, T: 96880, Avg. loss: 0.140123\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.31, NNZs: 80, Bias: -0.547962, T: 96880, Avg. loss: 0.209632\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.11, NNZs: 46, Bias: -1.003805, T: 96880, Avg. loss: 0.035623\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.11, NNZs: 73, Bias: -0.957678, T: 157430, Avg. loss: 0.067718\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.90, NNZs: 53, Bias: -1.032778, T: 108990, Avg. loss: 0.097100\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.36, NNZs: 75, Bias: -1.116236, T: 133210, Avg. loss: 0.110938\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.39, NNZs: 50, Bias: -1.008981, T: 133210, Avg. loss: 0.036558\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.71, NNZs: 35, Bias: -1.005183, T: 145320, Avg. loss: 0.085485\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.97, NNZs: 24, Bias: -0.996613, T: 145320, Avg. loss: 0.026004\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.86, NNZs: 78, Bias: -0.807838, T: 181650, Avg. loss: 0.124525\n",
      "Total training time: 0.20 seconds.\n",
      "Norm: 2.81, NNZs: 70, Bias: -0.979311, T: 108990, Avg. loss: 0.139017\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.07, NNZs: 45, Bias: -1.015746, T: 108990, Avg. loss: 0.035560\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.10, NNZs: 73, Bias: -0.943310, T: 169540, Avg. loss: 0.067422\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 3.24, NNZs: 80, Bias: -0.564505, T: 108990, Avg. loss: 0.209220\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.93, NNZs: 55, Bias: -1.045578, T: 121100, Avg. loss: 0.037783\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.80, NNZs: 55, Bias: -1.036172, T: 121100, Avg. loss: 0.096767\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.65, NNZs: 36, Bias: -1.022872, T: 157430, Avg. loss: 0.085026\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.35, NNZs: 52, Bias: -1.028703, T: 145320, Avg. loss: 0.036308\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.94, NNZs: 29, Bias: -0.999026, T: 157430, Avg. loss: 0.025970\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.32, NNZs: 76, Bias: -1.050108, T: 145320, Avg. loss: 0.110462\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.76, NNZs: 69, Bias: -0.944319, T: 121100, Avg. loss: 0.139540\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.03, NNZs: 44, Bias: -1.041720, T: 121100, Avg. loss: 0.035482\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.07, NNZs: 74, Bias: -0.952777, T: 181650, Avg. loss: 0.067443\n",
      "Total training time: 0.19 seconds.\n",
      "Norm: 1.90, NNZs: 55, Bias: -1.045465, T: 133210, Avg. loss: 0.037841\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.72, NNZs: 62, Bias: -1.000891, T: 133210, Avg. loss: 0.096542\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.59, NNZs: 40, Bias: -1.028303, T: 169540, Avg. loss: 0.084897\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.31, NNZs: 55, Bias: -1.030567, T: 157430, Avg. loss: 0.036159\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.90, NNZs: 30, Bias: -0.999376, T: 169540, Avg. loss: 0.026007\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.30, NNZs: 75, Bias: -1.093916, T: 157430, Avg. loss: 0.110729\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.01, NNZs: 45, Bias: -1.032897, T: 133210, Avg. loss: 0.035358\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.89, NNZs: 55, Bias: -1.040757, T: 145320, Avg. loss: 0.037567\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.68, NNZs: 74, Bias: -0.936910, T: 133210, Avg. loss: 0.139404\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 3.19, NNZs: 81, Bias: -0.536156, T: 121100, Avg. loss: 0.208217\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.65, NNZs: 60, Bias: -1.012936, T: 145320, Avg. loss: 0.096138\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.87, NNZs: 27, Bias: -0.998110, T: 181650, Avg. loss: 0.025948\n",
      "Total training time: 0.24 seconds.\n",
      "Norm: 1.54, NNZs: 39, Bias: -1.012643, T: 181650, Avg. loss: 0.085162\n",
      "Total training time: 0.25 seconds.\n",
      "Norm: 2.30, NNZs: 76, Bias: -1.092114, T: 169540, Avg. loss: 0.110487\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.28, NNZs: 53, Bias: -1.028136, T: 169540, Avg. loss: 0.036075\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.86, NNZs: 55, Bias: -1.047591, T: 157430, Avg. loss: 0.037396\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.00, NNZs: 44, Bias: -1.027712, T: 145320, Avg. loss: 0.035209\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.64, NNZs: 74, Bias: -0.938908, T: 145320, Avg. loss: 0.139302\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.59, NNZs: 55, Bias: -1.008191, T: 157430, Avg. loss: 0.095480\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.25, NNZs: 57, Bias: -1.025548, T: 181650, Avg. loss: 0.036206\n",
      "Total training time: 0.24 seconds.\n",
      "Norm: 1.84, NNZs: 57, Bias: -1.039209, T: 169540, Avg. loss: 0.037449\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.27, NNZs: 76, Bias: -1.098423, T: 181650, Avg. loss: 0.110225\n",
      "Total training time: 0.24 seconds.\n",
      "Norm: 1.98, NNZs: 46, Bias: -1.017412, T: 157430, Avg. loss: 0.035332\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 3.16, NNZs: 82, Bias: -0.539414, T: 133210, Avg. loss: 0.208382\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.59, NNZs: 73, Bias: -0.945351, T: 157430, Avg. loss: 0.138325\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.83, NNZs: 56, Bias: -1.024891, T: 181650, Avg. loss: 0.037359\n",
      "Total training time: 0.25 seconds.\n",
      "Norm: 1.54, NNZs: 59, Bias: -1.003879, T: 169540, Avg. loss: 0.095758\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.97, NNZs: 47, Bias: -1.005632, T: 169540, Avg. loss: 0.035135\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 3.12, NNZs: 82, Bias: -0.531233, T: 145320, Avg. loss: 0.207566\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.56, NNZs: 75, Bias: -0.938965, T: 169540, Avg. loss: 0.138232\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.49, NNZs: 59, Bias: -1.007808, T: 181650, Avg. loss: 0.095483\n",
      "Total training time: 0.26 seconds.\n",
      "Norm: 1.96, NNZs: 46, Bias: -1.016598, T: 181650, Avg. loss: 0.034739\n",
      "Total training time: 0.28 seconds.\n",
      "Norm: 3.09, NNZs: 82, Bias: -0.514113, T: 157430, Avg. loss: 0.207142\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.54, NNZs: 74, Bias: -0.938472, T: 181650, Avg. loss: 0.138194\n",
      "Total training time: 0.29 seconds.\n",
      "Norm: 3.07, NNZs: 82, Bias: -0.539099, T: 169540, Avg. loss: 0.206576\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 3.07, NNZs: 83, Bias: -0.544754, T: 181650, Avg. loss: 0.206445\n",
      "Total training time: 0.28 seconds.\n",
      "Accuracy on validation set:  0.7804314329738059\n",
      "--------------------\n",
      "Classifier with 30 maximum iterations.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 4.28, NNZs: 52, Bias: -2.054124, T: 12110, Avg. loss: 0.175307\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.3s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 2.86, NNZs: 40, Bias: -1.060754, T: 24220, Avg. loss: 0.053242\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.83, NNZs: 52, Bias: -0.992930, T: 12110, Avg. loss: 0.556248\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 4.44, NNZs: 55, Bias: -1.079152, T: 12110, Avg. loss: 0.184828\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 6.20, NNZs: 64, Bias: -0.729534, T: 12110, Avg. loss: 0.555616\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.08, NNZs: 43, Bias: -1.221114, T: 12110, Avg. loss: 0.114648\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.87, NNZs: 47, Bias: -2.013513, T: 12110, Avg. loss: 0.125654\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.05, NNZs: 57, Bias: -1.354272, T: 12110, Avg. loss: 0.290237\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.86, NNZs: 47, Bias: -0.925892, T: 12110, Avg. loss: 0.321753\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.34, NNZs: 38, Bias: -1.037291, T: 36330, Avg. loss: 0.045373\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.72, NNZs: 46, Bias: -4.284778, T: 12110, Avg. loss: 0.135340\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.56, NNZs: 49, Bias: -2.214191, T: 12110, Avg. loss: 0.085581\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.74, NNZs: 43, Bias: -1.202278, T: 12110, Avg. loss: 0.340742\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.78, NNZs: 40, Bias: -1.005224, T: 24220, Avg. loss: 0.040646\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.81, NNZs: 48, Bias: -1.008379, T: 12110, Avg. loss: 0.382438\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.30, NNZs: 42, Bias: -1.079097, T: 24220, Avg. loss: 0.046647\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.32, NNZs: 64, Bias: -1.057457, T: 24220, Avg. loss: 0.077719\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.57, NNZs: 50, Bias: -0.855506, T: 24220, Avg. loss: 0.157832\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.75, NNZs: 56, Bias: -1.010818, T: 24220, Avg. loss: 0.116790\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.84, NNZs: 71, Bias: -0.505935, T: 24220, Avg. loss: 0.232047\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.06, NNZs: 42, Bias: -1.045242, T: 48440, Avg. loss: 0.043258\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.69, NNZs: 35, Bias: -1.107219, T: 24220, Avg. loss: 0.106101\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.09, NNZs: 42, Bias: -1.323091, T: 24220, Avg. loss: 0.039793\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.34, NNZs: 43, Bias: -0.953127, T: 36330, Avg. loss: 0.038707\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.63, NNZs: 47, Bias: -1.105905, T: 36330, Avg. loss: 0.040833\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.95, NNZs: 66, Bias: -0.871479, T: 36330, Avg. loss: 0.072530\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.29, NNZs: 61, Bias: -0.792364, T: 24220, Avg. loss: 0.141946\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.86, NNZs: 36, Bias: -1.001036, T: 60550, Avg. loss: 0.042964\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 4.21, NNZs: 74, Bias: -0.516020, T: 36330, Avg. loss: 0.220745\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.92, NNZs: 61, Bias: -0.828342, T: 36330, Avg. loss: 0.151219\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.16, NNZs: 56, Bias: -1.042360, T: 36330, Avg. loss: 0.108318\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.13, NNZs: 30, Bias: -1.081421, T: 36330, Avg. loss: 0.095605\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.60, NNZs: 44, Bias: -1.102872, T: 36330, Avg. loss: 0.036529\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.65, NNZs: 41, Bias: -2.261134, T: 24220, Avg. loss: 0.048649\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.80, NNZs: 63, Bias: -1.179646, T: 24220, Avg. loss: 0.123296\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.42, NNZs: 52, Bias: -1.041421, T: 48440, Avg. loss: 0.040565\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.76, NNZs: 66, Bias: -0.841685, T: 36330, Avg. loss: 0.133055\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.86, NNZs: 70, Bias: -0.519921, T: 48440, Avg. loss: 0.216367\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.70, NNZs: 67, Bias: -0.895361, T: 48440, Avg. loss: 0.071055\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.56, NNZs: 59, Bias: -0.949453, T: 48440, Avg. loss: 0.146809\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.42, NNZs: 45, Bias: -1.072519, T: 48440, Avg. loss: 0.036381\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.68, NNZs: 36, Bias: -1.007472, T: 72660, Avg. loss: 0.042250\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.27, NNZs: 55, Bias: -1.068663, T: 60550, Avg. loss: 0.038757\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.79, NNZs: 55, Bias: -1.030513, T: 48440, Avg. loss: 0.104199\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.78, NNZs: 35, Bias: -1.054504, T: 48440, Avg. loss: 0.091993\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.31, NNZs: 67, Bias: -1.188895, T: 36330, Avg. loss: 0.119376\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.09, NNZs: 40, Bias: -1.075941, T: 48440, Avg. loss: 0.037203\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.53, NNZs: 70, Bias: -0.850087, T: 48440, Avg. loss: 0.130571\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.69, NNZs: 76, Bias: -0.528953, T: 60550, Avg. loss: 0.211507\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.57, NNZs: 40, Bias: -0.992631, T: 84770, Avg. loss: 0.041841\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.29, NNZs: 60, Bias: -0.978914, T: 60550, Avg. loss: 0.143817\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.17, NNZs: 34, Bias: -1.238338, T: 36330, Avg. loss: 0.033684\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.15, NNZs: 53, Bias: -1.022542, T: 72660, Avg. loss: 0.038452\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.53, NNZs: 33, Bias: -1.018872, T: 60550, Avg. loss: 0.090407\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.48, NNZs: 57, Bias: -0.993187, T: 60550, Avg. loss: 0.101880\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.04, NNZs: 68, Bias: -1.103322, T: 48440, Avg. loss: 0.116470\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.31, NNZs: 72, Bias: -0.844808, T: 60550, Avg. loss: 0.128293\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.47, NNZs: 42, Bias: -0.997501, T: 96880, Avg. loss: 0.041429\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.50, NNZs: 78, Bias: -0.470003, T: 72660, Avg. loss: 0.212411\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.53, NNZs: 68, Bias: -0.927839, T: 60550, Avg. loss: 0.069667\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.12, NNZs: 63, Bias: -0.955785, T: 72660, Avg. loss: 0.142822\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.08, NNZs: 55, Bias: -1.036068, T: 84770, Avg. loss: 0.038292\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.30, NNZs: 44, Bias: -1.026056, T: 60550, Avg. loss: 0.035923\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.40, NNZs: 39, Bias: -1.020571, T: 108990, Avg. loss: 0.041290\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.90, NNZs: 44, Bias: -1.069048, T: 60550, Avg. loss: 0.037275\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.27, NNZs: 56, Bias: -1.006930, T: 72660, Avg. loss: 0.099942\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.81, NNZs: 69, Bias: -1.026670, T: 60550, Avg. loss: 0.114669\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.21, NNZs: 73, Bias: -0.823912, T: 72660, Avg. loss: 0.128315\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.34, NNZs: 40, Bias: -1.038846, T: 72660, Avg. loss: 0.088419\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.40, NNZs: 78, Bias: -0.532759, T: 84770, Avg. loss: 0.210830\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.32, NNZs: 36, Bias: -1.009139, T: 121100, Avg. loss: 0.041097\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.02, NNZs: 54, Bias: -1.041072, T: 96880, Avg. loss: 0.038121\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.00, NNZs: 67, Bias: -0.926494, T: 84770, Avg. loss: 0.141550\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.46, NNZs: 71, Bias: -0.963728, T: 72660, Avg. loss: 0.069238\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.79, NNZs: 29, Bias: -1.014494, T: 48440, Avg. loss: 0.027998\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.75, NNZs: 44, Bias: -1.021122, T: 72660, Avg. loss: 0.037006\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.31, NNZs: 80, Bias: -0.547962, T: 96880, Avg. loss: 0.209632\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.69, NNZs: 74, Bias: -1.096765, T: 72660, Avg. loss: 0.113239\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.25, NNZs: 45, Bias: -1.030285, T: 72660, Avg. loss: 0.035956\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.26, NNZs: 41, Bias: -1.017228, T: 133210, Avg. loss: 0.040744\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.13, NNZs: 57, Bias: -0.999164, T: 84770, Avg. loss: 0.099295\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.15, NNZs: 76, Bias: -0.747406, T: 84770, Avg. loss: 0.126531\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.17, NNZs: 33, Bias: -1.030518, T: 84770, Avg. loss: 0.087498\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.60, NNZs: 32, Bias: -0.992656, T: 60550, Avg. loss: 0.026884\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.97, NNZs: 52, Bias: -1.057309, T: 108990, Avg. loss: 0.037853\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.90, NNZs: 69, Bias: -0.966763, T: 96880, Avg. loss: 0.140123\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.38, NNZs: 73, Bias: -0.938465, T: 84770, Avg. loss: 0.069091\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.24, NNZs: 80, Bias: -0.564505, T: 108990, Avg. loss: 0.209220\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.65, NNZs: 46, Bias: -1.036209, T: 84770, Avg. loss: 0.036689\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.17, NNZs: 45, Bias: -1.018767, T: 84770, Avg. loss: 0.035531\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.59, NNZs: 74, Bias: -1.087181, T: 84770, Avg. loss: 0.112497\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.99, NNZs: 56, Bias: -1.028445, T: 96880, Avg. loss: 0.098096\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.08, NNZs: 75, Bias: -0.780608, T: 96880, Avg. loss: 0.126697\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.21, NNZs: 42, Bias: -1.007549, T: 145320, Avg. loss: 0.040789\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.04, NNZs: 41, Bias: -1.036385, T: 96880, Avg. loss: 0.086820\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.44, NNZs: 31, Bias: -1.013834, T: 72660, Avg. loss: 0.026457\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.93, NNZs: 55, Bias: -1.045578, T: 121100, Avg. loss: 0.037783\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.29, NNZs: 74, Bias: -0.975806, T: 96880, Avg. loss: 0.068464\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.81, NNZs: 70, Bias: -0.979311, T: 108990, Avg. loss: 0.139017\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 3.19, NNZs: 81, Bias: -0.536156, T: 121100, Avg. loss: 0.208217\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.11, NNZs: 46, Bias: -1.003805, T: 96880, Avg. loss: 0.035623\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.31, NNZs: 29, Bias: -1.006196, T: 84770, Avg. loss: 0.026293\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.57, NNZs: 48, Bias: -1.018049, T: 96880, Avg. loss: 0.036686\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.51, NNZs: 75, Bias: -1.101516, T: 96880, Avg. loss: 0.112656\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.90, NNZs: 53, Bias: -1.032778, T: 108990, Avg. loss: 0.097100\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.94, NNZs: 34, Bias: -1.010666, T: 108990, Avg. loss: 0.086693\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 3.02, NNZs: 75, Bias: -0.798918, T: 108990, Avg. loss: 0.125416\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.16, NNZs: 44, Bias: -1.005795, T: 157430, Avg. loss: 0.040743\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.90, NNZs: 55, Bias: -1.045465, T: 133210, Avg. loss: 0.037841\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.23, NNZs: 73, Bias: -0.943043, T: 108990, Avg. loss: 0.068686\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 3.16, NNZs: 82, Bias: -0.539414, T: 133210, Avg. loss: 0.208382\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.21, NNZs: 30, Bias: -0.999730, T: 96880, Avg. loss: 0.026225\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.07, NNZs: 45, Bias: -1.015746, T: 108990, Avg. loss: 0.035560\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.44, NNZs: 75, Bias: -1.077080, T: 108990, Avg. loss: 0.112154\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.11, NNZs: 42, Bias: -1.015892, T: 169540, Avg. loss: 0.040446\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.85, NNZs: 31, Bias: -1.016580, T: 121100, Avg. loss: 0.085922\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.80, NNZs: 55, Bias: -1.036172, T: 121100, Avg. loss: 0.096767\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.49, NNZs: 51, Bias: -1.033835, T: 108990, Avg. loss: 0.036291\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.76, NNZs: 69, Bias: -0.944319, T: 121100, Avg. loss: 0.139540\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.89, NNZs: 55, Bias: -1.040757, T: 145320, Avg. loss: 0.037567\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.14, NNZs: 31, Bias: -0.988095, T: 108990, Avg. loss: 0.026166\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.96, NNZs: 75, Bias: -0.836133, T: 121100, Avg. loss: 0.125539\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.19, NNZs: 75, Bias: -0.965431, T: 121100, Avg. loss: 0.068220\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 3.12, NNZs: 82, Bias: -0.531233, T: 145320, Avg. loss: 0.207566\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.03, NNZs: 44, Bias: -1.041720, T: 121100, Avg. loss: 0.035482\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.07, NNZs: 43, Bias: -1.013638, T: 181650, Avg. loss: 0.040463\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.72, NNZs: 62, Bias: -1.000891, T: 133210, Avg. loss: 0.096542\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.44, NNZs: 49, Bias: -1.016712, T: 121100, Avg. loss: 0.036352\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.77, NNZs: 35, Bias: -1.006814, T: 133210, Avg. loss: 0.085567\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.86, NNZs: 55, Bias: -1.047591, T: 157430, Avg. loss: 0.037396\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.07, NNZs: 24, Bias: -1.003548, T: 121100, Avg. loss: 0.026123\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.16, NNZs: 75, Bias: -0.965210, T: 133210, Avg. loss: 0.068020\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 3.09, NNZs: 82, Bias: -0.514113, T: 157430, Avg. loss: 0.207142\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.04, NNZs: 42, Bias: -0.999367, T: 193760, Avg. loss: 0.040381\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.01, NNZs: 45, Bias: -1.032897, T: 133210, Avg. loss: 0.035358\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.68, NNZs: 74, Bias: -0.936910, T: 133210, Avg. loss: 0.139404\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.94, NNZs: 77, Bias: -0.805667, T: 133210, Avg. loss: 0.125270\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.02, NNZs: 28, Bias: -1.000981, T: 133210, Avg. loss: 0.026069\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.39, NNZs: 50, Bias: -1.008981, T: 133210, Avg. loss: 0.036558\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.65, NNZs: 60, Bias: -1.012936, T: 145320, Avg. loss: 0.096138\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.41, NNZs: 75, Bias: -1.046030, T: 121100, Avg. loss: 0.111645\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.71, NNZs: 35, Bias: -1.005183, T: 145320, Avg. loss: 0.085485\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.01, NNZs: 48, Bias: -1.004482, T: 205870, Avg. loss: 0.040286\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.84, NNZs: 57, Bias: -1.039209, T: 169540, Avg. loss: 0.037449\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 3.07, NNZs: 82, Bias: -0.539099, T: 169540, Avg. loss: 0.206576\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.64, NNZs: 74, Bias: -0.938908, T: 145320, Avg. loss: 0.139302\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.97, NNZs: 24, Bias: -0.996613, T: 145320, Avg. loss: 0.026004\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.98, NNZs: 46, Bias: -1.005950, T: 217980, Avg. loss: 0.040245\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.14, NNZs: 74, Bias: -0.937262, T: 145320, Avg. loss: 0.067834\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.35, NNZs: 52, Bias: -1.028703, T: 145320, Avg. loss: 0.036308\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.92, NNZs: 77, Bias: -0.791824, T: 145320, Avg. loss: 0.125140\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.65, NNZs: 36, Bias: -1.022872, T: 157430, Avg. loss: 0.085026\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.36, NNZs: 75, Bias: -1.116236, T: 133210, Avg. loss: 0.110938\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.83, NNZs: 56, Bias: -1.024891, T: 181650, Avg. loss: 0.037359\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.59, NNZs: 55, Bias: -1.008191, T: 157430, Avg. loss: 0.095480\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 3.07, NNZs: 83, Bias: -0.544754, T: 181650, Avg. loss: 0.206445\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.94, NNZs: 29, Bias: -0.999026, T: 157430, Avg. loss: 0.025970\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.95, NNZs: 47, Bias: -1.008617, T: 230090, Avg. loss: 0.040283\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.59, NNZs: 73, Bias: -0.945351, T: 157430, Avg. loss: 0.138325\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.31, NNZs: 55, Bias: -1.030567, T: 157430, Avg. loss: 0.036159\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.82, NNZs: 58, Bias: -1.044200, T: 193760, Avg. loss: 0.037239\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.11, NNZs: 73, Bias: -0.957678, T: 157430, Avg. loss: 0.067718\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.54, NNZs: 59, Bias: -1.003879, T: 169540, Avg. loss: 0.095758\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 3.04, NNZs: 85, Bias: -0.532697, T: 193760, Avg. loss: 0.206389\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.59, NNZs: 40, Bias: -1.028303, T: 169540, Avg. loss: 0.084897\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.32, NNZs: 76, Bias: -1.050108, T: 145320, Avg. loss: 0.110462\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.90, NNZs: 30, Bias: -0.999376, T: 169540, Avg. loss: 0.026007\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.92, NNZs: 39, Bias: -1.003117, T: 242200, Avg. loss: 0.040139\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.88, NNZs: 78, Bias: -0.811754, T: 157430, Avg. loss: 0.124998\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.56, NNZs: 75, Bias: -0.938965, T: 169540, Avg. loss: 0.138232\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.28, NNZs: 53, Bias: -1.028136, T: 169540, Avg. loss: 0.036075\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.80, NNZs: 59, Bias: -1.031721, T: 205870, Avg. loss: 0.037126\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.87, NNZs: 27, Bias: -0.998110, T: 181650, Avg. loss: 0.025948\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 3.03, NNZs: 85, Bias: -0.513373, T: 205870, Avg. loss: 0.205788\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.49, NNZs: 59, Bias: -1.007808, T: 181650, Avg. loss: 0.095483\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.54, NNZs: 39, Bias: -1.012643, T: 181650, Avg. loss: 0.085162\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.10, NNZs: 73, Bias: -0.943310, T: 169540, Avg. loss: 0.067422\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.90, NNZs: 45, Bias: -1.001921, T: 254310, Avg. loss: 0.040145\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.30, NNZs: 75, Bias: -1.093916, T: 157430, Avg. loss: 0.110729\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.79, NNZs: 59, Bias: -1.049065, T: 217980, Avg. loss: 0.037345\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.84, NNZs: 26, Bias: -0.996661, T: 193760, Avg. loss: 0.025888\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.54, NNZs: 74, Bias: -0.938472, T: 181650, Avg. loss: 0.138194\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 3.02, NNZs: 86, Bias: -0.556943, T: 217980, Avg. loss: 0.205318\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.88, NNZs: 41, Bias: -1.000727, T: 266420, Avg. loss: 0.040113\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.88, NNZs: 78, Bias: -0.813071, T: 169540, Avg. loss: 0.125118\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.44, NNZs: 61, Bias: -1.014309, T: 193760, Avg. loss: 0.095587\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.50, NNZs: 39, Bias: -1.017648, T: 193760, Avg. loss: 0.084563\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.30, NNZs: 76, Bias: -1.092114, T: 169540, Avg. loss: 0.110487\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.00, NNZs: 44, Bias: -1.027712, T: 145320, Avg. loss: 0.035209\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.81, NNZs: 29, Bias: -1.009931, T: 205870, Avg. loss: 0.025899\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.79, NNZs: 58, Bias: -1.056076, T: 230090, Avg. loss: 0.037024\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.07, NNZs: 74, Bias: -0.952777, T: 181650, Avg. loss: 0.067443\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 3.00, NNZs: 85, Bias: -0.533309, T: 230090, Avg. loss: 0.205495\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.51, NNZs: 75, Bias: -0.932428, T: 193760, Avg. loss: 0.137517\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.25, NNZs: 57, Bias: -1.025548, T: 181650, Avg. loss: 0.036206\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.40, NNZs: 64, Bias: -1.018757, T: 205870, Avg. loss: 0.095440\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.45, NNZs: 34, Bias: -1.009153, T: 205870, Avg. loss: 0.084548\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.27, NNZs: 76, Bias: -1.098423, T: 181650, Avg. loss: 0.110225\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.79, NNZs: 18, Bias: -1.002026, T: 217980, Avg. loss: 0.025863\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.79, NNZs: 59, Bias: -1.035339, T: 242200, Avg. loss: 0.037169\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.86, NNZs: 35, Bias: -1.002139, T: 278530, Avg. loss: 0.040079\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.86, NNZs: 78, Bias: -0.807838, T: 181650, Avg. loss: 0.124525\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.99, NNZs: 86, Bias: -0.529838, T: 242200, Avg. loss: 0.205034\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.06, NNZs: 75, Bias: -0.954556, T: 193760, Avg. loss: 0.067210\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.98, NNZs: 46, Bias: -1.017412, T: 157430, Avg. loss: 0.035332\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.23, NNZs: 58, Bias: -1.011311, T: 193760, Avg. loss: 0.035928\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.76, NNZs: 21, Bias: -1.004927, T: 230090, Avg. loss: 0.025813\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.37, NNZs: 64, Bias: -1.002682, T: 217980, Avg. loss: 0.094844\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.24, NNZs: 74, Bias: -1.107094, T: 193760, Avg. loss: 0.109645\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.78, NNZs: 59, Bias: -1.041057, T: 254310, Avg. loss: 0.037067\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.42, NNZs: 39, Bias: -1.016794, T: 217980, Avg. loss: 0.084426\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.98, NNZs: 86, Bias: -0.531479, T: 254310, Avg. loss: 0.205261\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.05, NNZs: 77, Bias: -0.967637, T: 205870, Avg. loss: 0.067191\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.74, NNZs: 23, Bias: -1.004845, T: 242200, Avg. loss: 0.025812\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.49, NNZs: 74, Bias: -0.953929, T: 205870, Avg. loss: 0.137682\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.21, NNZs: 58, Bias: -1.029819, T: 205870, Avg. loss: 0.035906\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.85, NNZs: 78, Bias: -0.812496, T: 193760, Avg. loss: 0.124439\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.84, NNZs: 35, Bias: -1.002135, T: 290640, Avg. loss: 0.040014\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.33, NNZs: 64, Bias: -1.010030, T: 230090, Avg. loss: 0.094933\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.77, NNZs: 60, Bias: -1.031071, T: 266420, Avg. loss: 0.036925\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.97, NNZs: 47, Bias: -1.005632, T: 169540, Avg. loss: 0.035135\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.38, NNZs: 36, Bias: -1.007866, T: 230090, Avg. loss: 0.084553\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.72, NNZs: 19, Bias: -1.004702, T: 254310, Avg. loss: 0.025809\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.47, NNZs: 75, Bias: -0.923533, T: 217980, Avg. loss: 0.136841\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.82, NNZs: 45, Bias: -0.998836, T: 302750, Avg. loss: 0.040006\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1.19, NNZs: 59, Bias: -1.010897, T: 217980, Avg. loss: 0.035918\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.23, NNZs: 75, Bias: -1.086289, T: 205870, Avg. loss: 0.109311\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.05, NNZs: 75, Bias: -0.948585, T: 217980, Avg. loss: 0.067277\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.83, NNZs: 78, Bias: -0.790816, T: 205870, Avg. loss: 0.124527\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.76, NNZs: 59, Bias: -1.037624, T: 278530, Avg. loss: 0.037003\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.96, NNZs: 46, Bias: -1.016598, T: 181650, Avg. loss: 0.034739\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.70, NNZs: 22, Bias: -1.009584, T: 266420, Avg. loss: 0.025756\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.29, NNZs: 62, Bias: -1.011425, T: 242200, Avg. loss: 0.094876\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.98, NNZs: 86, Bias: -0.554178, T: 266420, Avg. loss: 0.204601\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.81, NNZs: 38, Bias: -1.000195, T: 314860, Avg. loss: 0.039986\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.35, NNZs: 32, Bias: -1.005211, T: 242200, Avg. loss: 0.084233\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1.17, NNZs: 58, Bias: -1.020012, T: 230090, Avg. loss: 0.035759\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.45, NNZs: 75, Bias: -0.920538, T: 230090, Avg. loss: 0.137189\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.69, NNZs: 23, Bias: -1.002064, T: 278530, Avg. loss: 0.025830\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.75, NNZs: 59, Bias: -1.036448, T: 290640, Avg. loss: 0.037025\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.04, NNZs: 76, Bias: -0.966188, T: 230090, Avg. loss: 0.066831\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.83, NNZs: 80, Bias: -0.820097, T: 217980, Avg. loss: 0.124168\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.79, NNZs: 47, Bias: -1.007535, T: 326970, Avg. loss: 0.039870\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.27, NNZs: 69, Bias: -0.998063, T: 254310, Avg. loss: 0.094360\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.96, NNZs: 86, Bias: -0.533185, T: 278530, Avg. loss: 0.204922\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.21, NNZs: 74, Bias: -1.069396, T: 217980, Avg. loss: 0.109649\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.95, NNZs: 47, Bias: -1.015390, T: 193760, Avg. loss: 0.034837\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.68, NNZs: 32, Bias: -0.999829, T: 290640, Avg. loss: 0.025837\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.32, NNZs: 30, Bias: -1.004066, T: 254310, Avg. loss: 0.084062\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.74, NNZs: 59, Bias: -1.038697, T: 302750, Avg. loss: 0.036972\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.43, NNZs: 75, Bias: -0.913865, T: 242200, Avg. loss: 0.137403\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.01, NNZs: 75, Bias: -0.958677, T: 242200, Avg. loss: 0.066541\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.78, NNZs: 45, Bias: -1.004530, T: 339080, Avg. loss: 0.039944\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.16, NNZs: 59, Bias: -1.023154, T: 242200, Avg. loss: 0.035762\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.97, NNZs: 86, Bias: -0.519184, T: 290640, Avg. loss: 0.204671\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.81, NNZs: 80, Bias: -0.809668, T: 230090, Avg. loss: 0.124100\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.24, NNZs: 65, Bias: -0.993821, T: 266420, Avg. loss: 0.094631\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.66, NNZs: 23, Bias: -1.000976, T: 302750, Avg. loss: 0.025828\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.20, NNZs: 75, Bias: -1.087849, T: 230090, Avg. loss: 0.109549\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.76, NNZs: 47, Bias: -0.999703, T: 351190, Avg. loss: 0.039919\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.74, NNZs: 60, Bias: -1.035614, T: 314860, Avg. loss: 0.036948\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.94, NNZs: 48, Bias: -1.010682, T: 205870, Avg. loss: 0.034857\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.29, NNZs: 39, Bias: -1.001704, T: 266420, Avg. loss: 0.084154\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.41, NNZs: 75, Bias: -0.954903, T: 254310, Avg. loss: 0.137209\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.96, NNZs: 86, Bias: -0.558165, T: 302750, Avg. loss: 0.204860\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.02, NNZs: 75, Bias: -0.962055, T: 254310, Avg. loss: 0.067141\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.65, NNZs: 23, Bias: -1.000987, T: 314860, Avg. loss: 0.025796\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.21, NNZs: 64, Bias: -0.995527, T: 278530, Avg. loss: 0.094536\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.80, NNZs: 81, Bias: -0.787422, T: 242200, Avg. loss: 0.124083\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.75, NNZs: 49, Bias: -1.000783, T: 363300, Avg. loss: 0.039842\n",
      "Total training time: 0.42 seconds.\n",
      "Norm: 2.18, NNZs: 77, Bias: -1.082717, T: 242200, Avg. loss: 0.109418\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1.93, NNZs: 47, Bias: -1.010526, T: 217980, Avg. loss: 0.034846\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.74, NNZs: 60, Bias: -1.048255, T: 326970, Avg. loss: 0.036828\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.14, NNZs: 60, Bias: -1.016360, T: 254310, Avg. loss: 0.035746\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.26, NNZs: 33, Bias: -1.005571, T: 278530, Avg. loss: 0.083929\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.95, NNZs: 86, Bias: -0.533029, T: 314860, Avg. loss: 0.204265\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.64, NNZs: 29, Bias: -1.002032, T: 326970, Avg. loss: 0.025780\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.40, NNZs: 76, Bias: -0.937351, T: 266420, Avg. loss: 0.137089\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.01, NNZs: 75, Bias: -0.939742, T: 266420, Avg. loss: 0.066738\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.78, NNZs: 81, Bias: -0.811795, T: 254310, Avg. loss: 0.124089\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.19, NNZs: 65, Bias: -0.991526, T: 290640, Avg. loss: 0.094303\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.92, NNZs: 48, Bias: -1.010524, T: 230090, Avg. loss: 0.034625\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.18, NNZs: 77, Bias: -1.071419, T: 254310, Avg. loss: 0.109274\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.63, NNZs: 23, Bias: -1.001105, T: 339080, Avg. loss: 0.025788\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.13, NNZs: 60, Bias: -1.023691, T: 266420, Avg. loss: 0.035735\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.94, NNZs: 86, Bias: -0.544569, T: 326970, Avg. loss: 0.204196\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.78, NNZs: 79, Bias: -0.802493, T: 266420, Avg. loss: 0.123945\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.39, NNZs: 76, Bias: -0.939423, T: 278530, Avg. loss: 0.137272\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.00, NNZs: 75, Bias: -0.964134, T: 278530, Avg. loss: 0.066867\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.74, NNZs: 60, Bias: -1.029177, T: 339080, Avg. loss: 0.036811\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.24, NNZs: 37, Bias: -1.018216, T: 290640, Avg. loss: 0.083909\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.61, NNZs: 27, Bias: -1.005843, T: 351190, Avg. loss: 0.025729\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.16, NNZs: 67, Bias: -1.002950, T: 302750, Avg. loss: 0.094500\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.16, NNZs: 78, Bias: -1.099047, T: 266420, Avg. loss: 0.109276\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.94, NNZs: 86, Bias: -0.557628, T: 339080, Avg. loss: 0.204353\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.92, NNZs: 47, Bias: -1.004452, T: 242200, Avg. loss: 0.034644\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.78, NNZs: 81, Bias: -0.796419, T: 278530, Avg. loss: 0.123976\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.73, NNZs: 60, Bias: -1.030443, T: 351190, Avg. loss: 0.036890\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.00, NNZs: 75, Bias: -0.960145, T: 290640, Avg. loss: 0.066803\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.60, NNZs: 28, Bias: -1.004833, T: 363300, Avg. loss: 0.025774\n",
      "Total training time: 0.45 seconds.\n",
      "Norm: 1.14, NNZs: 63, Bias: -1.008387, T: 314860, Avg. loss: 0.094150\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2.94, NNZs: 86, Bias: -0.538033, T: 351190, Avg. loss: 0.204310\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.78, NNZs: 81, Bias: -0.797610, T: 290640, Avg. loss: 0.123847\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.21, NNZs: 45, Bias: -1.004653, T: 302750, Avg. loss: 0.083872\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.15, NNZs: 77, Bias: -1.090645, T: 278530, Avg. loss: 0.109274\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.12, NNZs: 60, Bias: -1.019053, T: 278530, Avg. loss: 0.035712\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.39, NNZs: 76, Bias: -0.904529, T: 290640, Avg. loss: 0.136828\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.91, NNZs: 47, Bias: -1.003100, T: 254310, Avg. loss: 0.034472\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.73, NNZs: 60, Bias: -1.034348, T: 363300, Avg. loss: 0.036855\n",
      "Total training time: 0.46 seconds.\n",
      "Norm: 1.99, NNZs: 75, Bias: -0.954449, T: 302750, Avg. loss: 0.066946\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.93, NNZs: 86, Bias: -0.553552, T: 363300, Avg. loss: 0.204004\n",
      "Total training time: 0.46 seconds.\n",
      "Norm: 1.12, NNZs: 63, Bias: -0.995994, T: 326970, Avg. loss: 0.094293\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.19, NNZs: 33, Bias: -1.004752, T: 314860, Avg. loss: 0.083907\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2.76, NNZs: 81, Bias: -0.807973, T: 302750, Avg. loss: 0.123900\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.14, NNZs: 77, Bias: -1.107755, T: 290640, Avg. loss: 0.108934\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.38, NNZs: 76, Bias: -0.910580, T: 302750, Avg. loss: 0.136920\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1.91, NNZs: 47, Bias: -1.004568, T: 266420, Avg. loss: 0.034641\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.11, NNZs: 61, Bias: -1.015618, T: 290640, Avg. loss: 0.035604\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.99, NNZs: 75, Bias: -0.949328, T: 314860, Avg. loss: 0.066864\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.10, NNZs: 67, Bias: -1.007138, T: 339080, Avg. loss: 0.094088\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2.76, NNZs: 81, Bias: -0.799157, T: 314860, Avg. loss: 0.123761\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.10, NNZs: 61, Bias: -1.019290, T: 302750, Avg. loss: 0.035574\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1.16, NNZs: 32, Bias: -1.009921, T: 326970, Avg. loss: 0.083769\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.90, NNZs: 47, Bias: -1.009363, T: 278530, Avg. loss: 0.034516\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.13, NNZs: 77, Bias: -1.096364, T: 302750, Avg. loss: 0.109009\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.37, NNZs: 76, Bias: -0.921607, T: 314860, Avg. loss: 0.136408\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.09, NNZs: 60, Bias: -1.021398, T: 314860, Avg. loss: 0.035568\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.08, NNZs: 65, Bias: -1.002327, T: 351190, Avg. loss: 0.094157\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.98, NNZs: 76, Bias: -0.957630, T: 326970, Avg. loss: 0.066837\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.90, NNZs: 47, Bias: -1.011466, T: 290640, Avg. loss: 0.034535\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.76, NNZs: 81, Bias: -0.790937, T: 326970, Avg. loss: 0.123568\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.13, NNZs: 77, Bias: -1.099239, T: 314860, Avg. loss: 0.108790\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.14, NNZs: 35, Bias: -1.004909, T: 339080, Avg. loss: 0.083783\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2.36, NNZs: 75, Bias: -0.936798, T: 326970, Avg. loss: 0.136516\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.09, NNZs: 59, Bias: -1.022341, T: 326970, Avg. loss: 0.035614\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.07, NNZs: 63, Bias: -1.003290, T: 363300, Avg. loss: 0.094049\n",
      "Total training time: 0.51 seconds.\n",
      "Norm: 2.12, NNZs: 78, Bias: -1.095604, T: 326970, Avg. loss: 0.108933\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.98, NNZs: 76, Bias: -0.956557, T: 339080, Avg. loss: 0.066708\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.89, NNZs: 47, Bias: -1.021314, T: 302750, Avg. loss: 0.034475\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.35, NNZs: 75, Bias: -0.918052, T: 339080, Avg. loss: 0.136568\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.08, NNZs: 58, Bias: -1.032995, T: 339080, Avg. loss: 0.035506\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.13, NNZs: 36, Bias: -1.004967, T: 351190, Avg. loss: 0.083745\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.12, NNZs: 78, Bias: -1.097190, T: 339080, Avg. loss: 0.108909\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2.75, NNZs: 81, Bias: -0.795496, T: 339080, Avg. loss: 0.123640\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.07, NNZs: 59, Bias: -1.022215, T: 351190, Avg. loss: 0.035503\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.35, NNZs: 75, Bias: -0.915297, T: 351190, Avg. loss: 0.136236\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.89, NNZs: 47, Bias: -1.010341, T: 314860, Avg. loss: 0.034544\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2.11, NNZs: 79, Bias: -1.091589, T: 351190, Avg. loss: 0.108798\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.98, NNZs: 75, Bias: -0.959329, T: 351190, Avg. loss: 0.066640\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.07, NNZs: 59, Bias: -1.022934, T: 363300, Avg. loss: 0.035502\n",
      "Total training time: 0.54 seconds.\n",
      "Norm: 1.11, NNZs: 40, Bias: -1.013300, T: 363300, Avg. loss: 0.083694\n",
      "Total training time: 0.51 seconds.\n",
      "Norm: 2.75, NNZs: 81, Bias: -0.804038, T: 351190, Avg. loss: 0.123566\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.11, NNZs: 79, Bias: -1.090612, T: 363300, Avg. loss: 0.108820\n",
      "Total training time: 0.52 seconds.\n",
      "Norm: 2.34, NNZs: 75, Bias: -0.924659, T: 363300, Avg. loss: 0.136255\n",
      "Total training time: 0.55 seconds.\n",
      "Norm: 1.89, NNZs: 47, Bias: -1.021760, T: 326970, Avg. loss: 0.034483\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.97, NNZs: 75, Bias: -0.958275, T: 363300, Avg. loss: 0.066656\n",
      "Total training time: 0.53 seconds.\n",
      "Norm: 2.74, NNZs: 81, Bias: -0.813104, T: 363300, Avg. loss: 0.123504\n",
      "Total training time: 0.55 seconds.\n",
      "Norm: 1.89, NNZs: 47, Bias: -1.009915, T: 339080, Avg. loss: 0.034500\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.88, NNZs: 47, Bias: -1.007183, T: 351190, Avg. loss: 0.034435\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.88, NNZs: 47, Bias: -1.012863, T: 363300, Avg. loss: 0.034439\n",
      "Total training time: 0.57 seconds.\n",
      "Accuracy on validation set:  0.786979969183359\n",
      "--------------------\n",
      "Classifier with 50 maximum iterations.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.5s remaining:    2.4s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 5.83, NNZs: 52, Bias: -0.992930, T: 12110, Avg. loss: 0.556248\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 4.56, NNZs: 49, Bias: -2.214191, T: 12110, Avg. loss: 0.085581\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 4.28, NNZs: 52, Bias: -2.054124, T: 12110, Avg. loss: 0.175307\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 4.74, NNZs: 43, Bias: -1.202278, T: 12110, Avg. loss: 0.340742\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 3.72, NNZs: 46, Bias: -4.284778, T: 12110, Avg. loss: 0.135340\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.87, NNZs: 47, Bias: -2.013513, T: 12110, Avg. loss: 0.125654\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.05, NNZs: 57, Bias: -1.354272, T: 12110, Avg. loss: 0.290237\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.20, NNZs: 64, Bias: -0.729534, T: 12110, Avg. loss: 0.555616\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.44, NNZs: 55, Bias: -1.079152, T: 12110, Avg. loss: 0.184828\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.86, NNZs: 47, Bias: -0.925892, T: 12110, Avg. loss: 0.321753\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.81, NNZs: 48, Bias: -1.008379, T: 12110, Avg. loss: 0.382438\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.65, NNZs: 41, Bias: -2.261134, T: 24220, Avg. loss: 0.048649\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.09, NNZs: 42, Bias: -1.323091, T: 24220, Avg. loss: 0.039793\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.08, NNZs: 43, Bias: -1.221114, T: 12110, Avg. loss: 0.114648\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.30, NNZs: 42, Bias: -1.079097, T: 24220, Avg. loss: 0.046647\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.69, NNZs: 35, Bias: -1.107219, T: 24220, Avg. loss: 0.106101\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.75, NNZs: 56, Bias: -1.010818, T: 24220, Avg. loss: 0.116790\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.57, NNZs: 50, Bias: -0.855506, T: 24220, Avg. loss: 0.157832\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.17, NNZs: 34, Bias: -1.238338, T: 36330, Avg. loss: 0.033684\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.29, NNZs: 61, Bias: -0.792364, T: 24220, Avg. loss: 0.141946\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.60, NNZs: 44, Bias: -1.102872, T: 36330, Avg. loss: 0.036529\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.84, NNZs: 71, Bias: -0.505935, T: 24220, Avg. loss: 0.232047\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.63, NNZs: 47, Bias: -1.105905, T: 36330, Avg. loss: 0.040833\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.32, NNZs: 64, Bias: -1.057457, T: 24220, Avg. loss: 0.077719\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.78, NNZs: 40, Bias: -1.005224, T: 24220, Avg. loss: 0.040646\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.86, NNZs: 40, Bias: -1.060754, T: 24220, Avg. loss: 0.053242\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.13, NNZs: 30, Bias: -1.081421, T: 36330, Avg. loss: 0.095605\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.16, NNZs: 56, Bias: -1.042360, T: 36330, Avg. loss: 0.108318\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.79, NNZs: 29, Bias: -1.014494, T: 48440, Avg. loss: 0.027998\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.76, NNZs: 66, Bias: -0.841685, T: 36330, Avg. loss: 0.133055\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.42, NNZs: 45, Bias: -1.072519, T: 48440, Avg. loss: 0.036381\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.34, NNZs: 38, Bias: -1.037291, T: 36330, Avg. loss: 0.045373\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.80, NNZs: 63, Bias: -1.179646, T: 24220, Avg. loss: 0.123296\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.34, NNZs: 43, Bias: -0.953127, T: 36330, Avg. loss: 0.038707\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.42, NNZs: 52, Bias: -1.041421, T: 48440, Avg. loss: 0.040565\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.95, NNZs: 66, Bias: -0.871479, T: 36330, Avg. loss: 0.072530\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.79, NNZs: 55, Bias: -1.030513, T: 48440, Avg. loss: 0.104199\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.78, NNZs: 35, Bias: -1.054504, T: 48440, Avg. loss: 0.091993\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.06, NNZs: 42, Bias: -1.045242, T: 48440, Avg. loss: 0.043258\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.60, NNZs: 32, Bias: -0.992656, T: 60550, Avg. loss: 0.026884\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.53, NNZs: 70, Bias: -0.850087, T: 48440, Avg. loss: 0.130571\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.31, NNZs: 67, Bias: -1.188895, T: 36330, Avg. loss: 0.119376\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.30, NNZs: 44, Bias: -1.026056, T: 60550, Avg. loss: 0.035923\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.09, NNZs: 40, Bias: -1.075941, T: 48440, Avg. loss: 0.037203\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.27, NNZs: 55, Bias: -1.068663, T: 60550, Avg. loss: 0.038757\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 4.21, NNZs: 74, Bias: -0.516020, T: 36330, Avg. loss: 0.220745\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.48, NNZs: 57, Bias: -0.993187, T: 60550, Avg. loss: 0.101880\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.92, NNZs: 61, Bias: -0.828342, T: 36330, Avg. loss: 0.151219\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.70, NNZs: 67, Bias: -0.895361, T: 48440, Avg. loss: 0.071055\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.86, NNZs: 36, Bias: -1.001036, T: 60550, Avg. loss: 0.042964\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.53, NNZs: 33, Bias: -1.018872, T: 60550, Avg. loss: 0.090407\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.44, NNZs: 31, Bias: -1.013834, T: 72660, Avg. loss: 0.026457\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.31, NNZs: 72, Bias: -0.844808, T: 60550, Avg. loss: 0.128293\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.25, NNZs: 45, Bias: -1.030285, T: 72660, Avg. loss: 0.035956\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.27, NNZs: 56, Bias: -1.006930, T: 72660, Avg. loss: 0.099942\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.90, NNZs: 44, Bias: -1.069048, T: 60550, Avg. loss: 0.037275\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.15, NNZs: 53, Bias: -1.022542, T: 72660, Avg. loss: 0.038452\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.56, NNZs: 59, Bias: -0.949453, T: 48440, Avg. loss: 0.146809\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.68, NNZs: 36, Bias: -1.007472, T: 72660, Avg. loss: 0.042250\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.53, NNZs: 68, Bias: -0.927839, T: 60550, Avg. loss: 0.069667\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.34, NNZs: 40, Bias: -1.038846, T: 72660, Avg. loss: 0.088419\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.31, NNZs: 29, Bias: -1.006196, T: 84770, Avg. loss: 0.026293\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.75, NNZs: 44, Bias: -1.021122, T: 72660, Avg. loss: 0.037006\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.13, NNZs: 57, Bias: -0.999164, T: 84770, Avg. loss: 0.099295\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.17, NNZs: 45, Bias: -1.018767, T: 84770, Avg. loss: 0.035531\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.21, NNZs: 73, Bias: -0.823912, T: 72660, Avg. loss: 0.128315\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 3.04, NNZs: 68, Bias: -1.103322, T: 48440, Avg. loss: 0.116470\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.86, NNZs: 70, Bias: -0.519921, T: 48440, Avg. loss: 0.216367\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.57, NNZs: 40, Bias: -0.992631, T: 84770, Avg. loss: 0.041841\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.29, NNZs: 60, Bias: -0.978914, T: 60550, Avg. loss: 0.143817\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.08, NNZs: 55, Bias: -1.036068, T: 84770, Avg. loss: 0.038292\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.46, NNZs: 71, Bias: -0.963728, T: 72660, Avg. loss: 0.069238\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.21, NNZs: 30, Bias: -0.999730, T: 96880, Avg. loss: 0.026225\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.17, NNZs: 33, Bias: -1.030518, T: 84770, Avg. loss: 0.087498\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.65, NNZs: 46, Bias: -1.036209, T: 84770, Avg. loss: 0.036689\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.11, NNZs: 46, Bias: -1.003805, T: 96880, Avg. loss: 0.035623\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.69, NNZs: 76, Bias: -0.528953, T: 60550, Avg. loss: 0.211507\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.15, NNZs: 76, Bias: -0.747406, T: 84770, Avg. loss: 0.126531\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.12, NNZs: 63, Bias: -0.955785, T: 72660, Avg. loss: 0.142822\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.81, NNZs: 69, Bias: -1.026670, T: 60550, Avg. loss: 0.114669\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.47, NNZs: 42, Bias: -0.997501, T: 96880, Avg. loss: 0.041429\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.14, NNZs: 31, Bias: -0.988095, T: 108990, Avg. loss: 0.026166\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.04, NNZs: 41, Bias: -1.036385, T: 96880, Avg. loss: 0.086820\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.99, NNZs: 56, Bias: -1.028445, T: 96880, Avg. loss: 0.098096\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.57, NNZs: 48, Bias: -1.018049, T: 96880, Avg. loss: 0.036686\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.50, NNZs: 78, Bias: -0.470003, T: 72660, Avg. loss: 0.212411\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.38, NNZs: 73, Bias: -0.938465, T: 84770, Avg. loss: 0.069091\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.00, NNZs: 67, Bias: -0.926494, T: 84770, Avg. loss: 0.141550\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.07, NNZs: 45, Bias: -1.015746, T: 108990, Avg. loss: 0.035560\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 3.08, NNZs: 75, Bias: -0.780608, T: 96880, Avg. loss: 0.126697\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.69, NNZs: 74, Bias: -1.096765, T: 72660, Avg. loss: 0.113239\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.07, NNZs: 24, Bias: -1.003548, T: 121100, Avg. loss: 0.026123\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.40, NNZs: 39, Bias: -1.020571, T: 108990, Avg. loss: 0.041290\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.49, NNZs: 51, Bias: -1.033835, T: 108990, Avg. loss: 0.036291\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.94, NNZs: 34, Bias: -1.010666, T: 108990, Avg. loss: 0.086693\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.90, NNZs: 53, Bias: -1.032778, T: 108990, Avg. loss: 0.097100\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 3.40, NNZs: 78, Bias: -0.532759, T: 84770, Avg. loss: 0.210830\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.90, NNZs: 69, Bias: -0.966763, T: 96880, Avg. loss: 0.140123\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.03, NNZs: 44, Bias: -1.041720, T: 121100, Avg. loss: 0.035482\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 3.02, NNZs: 75, Bias: -0.798918, T: 108990, Avg. loss: 0.125416\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.29, NNZs: 74, Bias: -0.975806, T: 96880, Avg. loss: 0.068464\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.59, NNZs: 74, Bias: -1.087181, T: 84770, Avg. loss: 0.112497\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.02, NNZs: 28, Bias: -1.000981, T: 133210, Avg. loss: 0.026069\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.32, NNZs: 36, Bias: -1.009139, T: 121100, Avg. loss: 0.041097\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.44, NNZs: 49, Bias: -1.016712, T: 121100, Avg. loss: 0.036352\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 3.31, NNZs: 80, Bias: -0.547962, T: 96880, Avg. loss: 0.209632\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.81, NNZs: 70, Bias: -0.979311, T: 108990, Avg. loss: 0.139017\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.80, NNZs: 55, Bias: -1.036172, T: 121100, Avg. loss: 0.096767\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.97, NNZs: 24, Bias: -0.996613, T: 145320, Avg. loss: 0.026004\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.96, NNZs: 75, Bias: -0.836133, T: 121100, Avg. loss: 0.125539\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.01, NNZs: 45, Bias: -1.032897, T: 133210, Avg. loss: 0.035358\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.26, NNZs: 41, Bias: -1.017228, T: 133210, Avg. loss: 0.040744\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.51, NNZs: 75, Bias: -1.101516, T: 96880, Avg. loss: 0.112656\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.39, NNZs: 50, Bias: -1.008981, T: 133210, Avg. loss: 0.036558\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.02, NNZs: 54, Bias: -1.041072, T: 96880, Avg. loss: 0.038121\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.23, NNZs: 73, Bias: -0.943043, T: 108990, Avg. loss: 0.068686\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.76, NNZs: 69, Bias: -0.944319, T: 121100, Avg. loss: 0.139540\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 3.24, NNZs: 80, Bias: -0.564505, T: 108990, Avg. loss: 0.209220\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.85, NNZs: 31, Bias: -1.016580, T: 121100, Avg. loss: 0.085922\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.72, NNZs: 62, Bias: -1.000891, T: 133210, Avg. loss: 0.096542\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.21, NNZs: 42, Bias: -1.007549, T: 145320, Avg. loss: 0.040789\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.00, NNZs: 44, Bias: -1.027712, T: 145320, Avg. loss: 0.035209\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.35, NNZs: 52, Bias: -1.028703, T: 145320, Avg. loss: 0.036308\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.94, NNZs: 77, Bias: -0.805667, T: 133210, Avg. loss: 0.125270\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.44, NNZs: 75, Bias: -1.077080, T: 108990, Avg. loss: 0.112154\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.97, NNZs: 52, Bias: -1.057309, T: 108990, Avg. loss: 0.037853\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.19, NNZs: 75, Bias: -0.965431, T: 121100, Avg. loss: 0.068220\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.77, NNZs: 35, Bias: -1.006814, T: 133210, Avg. loss: 0.085567\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 3.19, NNZs: 81, Bias: -0.536156, T: 121100, Avg. loss: 0.208217\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.31, NNZs: 55, Bias: -1.030567, T: 157430, Avg. loss: 0.036159\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.68, NNZs: 74, Bias: -0.936910, T: 133210, Avg. loss: 0.139404\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.65, NNZs: 60, Bias: -1.012936, T: 145320, Avg. loss: 0.096138\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.94, NNZs: 29, Bias: -0.999026, T: 157430, Avg. loss: 0.025970\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.92, NNZs: 77, Bias: -0.791824, T: 145320, Avg. loss: 0.125140\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.16, NNZs: 44, Bias: -1.005795, T: 157430, Avg. loss: 0.040743\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.98, NNZs: 46, Bias: -1.017412, T: 157430, Avg. loss: 0.035332\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.41, NNZs: 75, Bias: -1.046030, T: 121100, Avg. loss: 0.111645\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 3.16, NNZs: 82, Bias: -0.539414, T: 133210, Avg. loss: 0.208382\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.71, NNZs: 35, Bias: -1.005183, T: 145320, Avg. loss: 0.085485\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.28, NNZs: 53, Bias: -1.028136, T: 169540, Avg. loss: 0.036075\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.64, NNZs: 74, Bias: -0.938908, T: 145320, Avg. loss: 0.139302\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.11, NNZs: 42, Bias: -1.015892, T: 169540, Avg. loss: 0.040446\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.59, NNZs: 55, Bias: -1.008191, T: 157430, Avg. loss: 0.095480\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.88, NNZs: 78, Bias: -0.811754, T: 157430, Avg. loss: 0.124998\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.93, NNZs: 55, Bias: -1.045578, T: 121100, Avg. loss: 0.037783\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 2.36, NNZs: 75, Bias: -1.116236, T: 133210, Avg. loss: 0.110938\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.97, NNZs: 47, Bias: -1.005632, T: 169540, Avg. loss: 0.035135\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.25, NNZs: 57, Bias: -1.025548, T: 181650, Avg. loss: 0.036206\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.16, NNZs: 75, Bias: -0.965210, T: 133210, Avg. loss: 0.068020\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.65, NNZs: 36, Bias: -1.022872, T: 157430, Avg. loss: 0.085026\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 3.12, NNZs: 82, Bias: -0.531233, T: 145320, Avg. loss: 0.207566\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.59, NNZs: 73, Bias: -0.945351, T: 157430, Avg. loss: 0.138325\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.07, NNZs: 43, Bias: -1.013638, T: 181650, Avg. loss: 0.040463\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.90, NNZs: 30, Bias: -0.999376, T: 169540, Avg. loss: 0.026007\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.88, NNZs: 78, Bias: -0.813071, T: 169540, Avg. loss: 0.125118\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.54, NNZs: 59, Bias: -1.003879, T: 169540, Avg. loss: 0.095758\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.32, NNZs: 76, Bias: -1.050108, T: 145320, Avg. loss: 0.110462\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 1.23, NNZs: 58, Bias: -1.011311, T: 193760, Avg. loss: 0.035928\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.59, NNZs: 40, Bias: -1.028303, T: 169540, Avg. loss: 0.084897\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 1.96, NNZs: 46, Bias: -1.016598, T: 181650, Avg. loss: 0.034739\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.14, NNZs: 74, Bias: -0.937262, T: 145320, Avg. loss: 0.067834\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 2.56, NNZs: 75, Bias: -0.938965, T: 169540, Avg. loss: 0.138232\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.86, NNZs: 78, Bias: -0.807838, T: 181650, Avg. loss: 0.124525\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.90, NNZs: 55, Bias: -1.045465, T: 133210, Avg. loss: 0.037841\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 2.30, NNZs: 75, Bias: -1.093916, T: 157430, Avg. loss: 0.110729\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.21, NNZs: 58, Bias: -1.029819, T: 205870, Avg. loss: 0.035906\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 3.09, NNZs: 82, Bias: -0.514113, T: 157430, Avg. loss: 0.207142\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.54, NNZs: 39, Bias: -1.012643, T: 181650, Avg. loss: 0.085162\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.04, NNZs: 42, Bias: -0.999367, T: 193760, Avg. loss: 0.040381\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.87, NNZs: 27, Bias: -0.998110, T: 181650, Avg. loss: 0.025948\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.49, NNZs: 59, Bias: -1.007808, T: 181650, Avg. loss: 0.095483\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.54, NNZs: 74, Bias: -0.938472, T: 181650, Avg. loss: 0.138194\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.11, NNZs: 73, Bias: -0.957678, T: 157430, Avg. loss: 0.067718\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 1.19, NNZs: 59, Bias: -1.010897, T: 217980, Avg. loss: 0.035918\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 3.07, NNZs: 82, Bias: -0.539099, T: 169540, Avg. loss: 0.206576\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.85, NNZs: 78, Bias: -0.812496, T: 193760, Avg. loss: 0.124439\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 2.30, NNZs: 76, Bias: -1.092114, T: 169540, Avg. loss: 0.110487\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.84, NNZs: 26, Bias: -0.996661, T: 193760, Avg. loss: 0.025888\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.95, NNZs: 47, Bias: -1.015390, T: 193760, Avg. loss: 0.034837\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.01, NNZs: 48, Bias: -1.004482, T: 205870, Avg. loss: 0.040286\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.51, NNZs: 75, Bias: -0.932428, T: 193760, Avg. loss: 0.137517\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.44, NNZs: 61, Bias: -1.014309, T: 193760, Avg. loss: 0.095587\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.50, NNZs: 39, Bias: -1.017648, T: 193760, Avg. loss: 0.084563\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.17, NNZs: 58, Bias: -1.020012, T: 230090, Avg. loss: 0.035759\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 3.07, NNZs: 83, Bias: -0.544754, T: 181650, Avg. loss: 0.206445\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.83, NNZs: 78, Bias: -0.790816, T: 205870, Avg. loss: 0.124527\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.10, NNZs: 73, Bias: -0.943310, T: 169540, Avg. loss: 0.067422\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 2.27, NNZs: 76, Bias: -1.098423, T: 181650, Avg. loss: 0.110225\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.89, NNZs: 55, Bias: -1.040757, T: 145320, Avg. loss: 0.037567\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.81, NNZs: 29, Bias: -1.009931, T: 205870, Avg. loss: 0.025899\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.49, NNZs: 74, Bias: -0.953929, T: 205870, Avg. loss: 0.137682\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.98, NNZs: 46, Bias: -1.005950, T: 217980, Avg. loss: 0.040245\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.40, NNZs: 64, Bias: -1.018757, T: 205870, Avg. loss: 0.095440\n",
      "Total training time: 0.30 seconds.\n",
      "Norm: 1.94, NNZs: 48, Bias: -1.010682, T: 205870, Avg. loss: 0.034857\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 18\n",
      "-- Epoch 18\n",
      "Norm: 1.16, NNZs: 59, Bias: -1.023154, T: 242200, Avg. loss: 0.035762\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.79, NNZs: 18, Bias: -1.002026, T: 217980, Avg. loss: 0.025863\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.86, NNZs: 55, Bias: -1.047591, T: 157430, Avg. loss: 0.037396\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 2.83, NNZs: 80, Bias: -0.820097, T: 217980, Avg. loss: 0.124168\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.24, NNZs: 74, Bias: -1.107094, T: 193760, Avg. loss: 0.109645\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 3.04, NNZs: 85, Bias: -0.532697, T: 193760, Avg. loss: 0.206389\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.45, NNZs: 34, Bias: -1.009153, T: 205870, Avg. loss: 0.084548\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.95, NNZs: 47, Bias: -1.008617, T: 230090, Avg. loss: 0.040283\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.47, NNZs: 75, Bias: -0.923533, T: 217980, Avg. loss: 0.136841\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.07, NNZs: 74, Bias: -0.952777, T: 181650, Avg. loss: 0.067443\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 1.14, NNZs: 60, Bias: -1.016360, T: 254310, Avg. loss: 0.035746\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.76, NNZs: 21, Bias: -1.004927, T: 230090, Avg. loss: 0.025813\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.37, NNZs: 64, Bias: -1.002682, T: 217980, Avg. loss: 0.094844\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.84, NNZs: 57, Bias: -1.039209, T: 169540, Avg. loss: 0.037449\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.92, NNZs: 39, Bias: -1.003117, T: 242200, Avg. loss: 0.040139\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.81, NNZs: 80, Bias: -0.809668, T: 230090, Avg. loss: 0.124100\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.23, NNZs: 75, Bias: -1.086289, T: 205870, Avg. loss: 0.109311\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.93, NNZs: 47, Bias: -1.010526, T: 217980, Avg. loss: 0.034846\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.45, NNZs: 75, Bias: -0.920538, T: 230090, Avg. loss: 0.137189\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.42, NNZs: 39, Bias: -1.016794, T: 217980, Avg. loss: 0.084426\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 3.03, NNZs: 85, Bias: -0.513373, T: 205870, Avg. loss: 0.205788\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.06, NNZs: 75, Bias: -0.954556, T: 193760, Avg. loss: 0.067210\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.13, NNZs: 60, Bias: -1.023691, T: 266420, Avg. loss: 0.035735\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.74, NNZs: 23, Bias: -1.004845, T: 242200, Avg. loss: 0.025812\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.90, NNZs: 45, Bias: -1.001921, T: 254310, Avg. loss: 0.040145\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.33, NNZs: 64, Bias: -1.010030, T: 230090, Avg. loss: 0.094933\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.83, NNZs: 56, Bias: -1.024891, T: 181650, Avg. loss: 0.037359\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 2.80, NNZs: 81, Bias: -0.787422, T: 242200, Avg. loss: 0.124083\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.43, NNZs: 75, Bias: -0.913865, T: 242200, Avg. loss: 0.137403\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.21, NNZs: 74, Bias: -1.069396, T: 217980, Avg. loss: 0.109649\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.92, NNZs: 48, Bias: -1.010524, T: 230090, Avg. loss: 0.034625\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.12, NNZs: 60, Bias: -1.019053, T: 278530, Avg. loss: 0.035712\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.88, NNZs: 41, Bias: -1.000727, T: 266420, Avg. loss: 0.040113\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.72, NNZs: 19, Bias: -1.004702, T: 254310, Avg. loss: 0.025809\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 3.02, NNZs: 86, Bias: -0.556943, T: 217980, Avg. loss: 0.205318\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 1.82, NNZs: 58, Bias: -1.044200, T: 193760, Avg. loss: 0.037239\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 1.38, NNZs: 36, Bias: -1.007866, T: 230090, Avg. loss: 0.084553\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 2.05, NNZs: 77, Bias: -0.967637, T: 205870, Avg. loss: 0.067191\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 1.29, NNZs: 62, Bias: -1.011425, T: 242200, Avg. loss: 0.094876\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.41, NNZs: 75, Bias: -0.954903, T: 254310, Avg. loss: 0.137209\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.78, NNZs: 81, Bias: -0.811795, T: 254310, Avg. loss: 0.124089\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.20, NNZs: 75, Bias: -1.087849, T: 230090, Avg. loss: 0.109549\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.86, NNZs: 35, Bias: -1.002139, T: 278530, Avg. loss: 0.040079\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.11, NNZs: 61, Bias: -1.015618, T: 290640, Avg. loss: 0.035604\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.70, NNZs: 22, Bias: -1.009584, T: 266420, Avg. loss: 0.025756\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.92, NNZs: 47, Bias: -1.004452, T: 242200, Avg. loss: 0.034644\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1.35, NNZs: 32, Bias: -1.005211, T: 242200, Avg. loss: 0.084233\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1.80, NNZs: 59, Bias: -1.031721, T: 205870, Avg. loss: 0.037126\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 2.78, NNZs: 79, Bias: -0.802493, T: 266420, Avg. loss: 0.123945\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 3.00, NNZs: 85, Bias: -0.533309, T: 230090, Avg. loss: 0.205495\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.84, NNZs: 35, Bias: -1.002135, T: 290640, Avg. loss: 0.040014\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.05, NNZs: 75, Bias: -0.948585, T: 217980, Avg. loss: 0.067277\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.18, NNZs: 77, Bias: -1.082717, T: 242200, Avg. loss: 0.109418\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.40, NNZs: 76, Bias: -0.937351, T: 266420, Avg. loss: 0.137089\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.27, NNZs: 69, Bias: -0.998063, T: 254310, Avg. loss: 0.094360\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.69, NNZs: 23, Bias: -1.002064, T: 278530, Avg. loss: 0.025830\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.10, NNZs: 61, Bias: -1.019290, T: 302750, Avg. loss: 0.035574\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1.91, NNZs: 47, Bias: -1.003100, T: 254310, Avg. loss: 0.034472\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.82, NNZs: 45, Bias: -0.998836, T: 302750, Avg. loss: 0.040006\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1.32, NNZs: 30, Bias: -1.004066, T: 254310, Avg. loss: 0.084062\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.79, NNZs: 59, Bias: -1.049065, T: 217980, Avg. loss: 0.037345\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 2.78, NNZs: 81, Bias: -0.796419, T: 278530, Avg. loss: 0.123976\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.18, NNZs: 77, Bias: -1.071419, T: 254310, Avg. loss: 0.109274\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.39, NNZs: 76, Bias: -0.939423, T: 278530, Avg. loss: 0.137272\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.68, NNZs: 32, Bias: -0.999829, T: 290640, Avg. loss: 0.025837\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.04, NNZs: 76, Bias: -0.966188, T: 230090, Avg. loss: 0.066831\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.09, NNZs: 60, Bias: -1.021398, T: 314860, Avg. loss: 0.035568\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.24, NNZs: 65, Bias: -0.993821, T: 266420, Avg. loss: 0.094631\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.99, NNZs: 86, Bias: -0.529838, T: 242200, Avg. loss: 0.205034\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.81, NNZs: 38, Bias: -1.000195, T: 314860, Avg. loss: 0.039986\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.91, NNZs: 47, Bias: -1.004568, T: 266420, Avg. loss: 0.034641\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.79, NNZs: 58, Bias: -1.056076, T: 230090, Avg. loss: 0.037024\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 1.29, NNZs: 39, Bias: -1.001704, T: 266420, Avg. loss: 0.084154\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 2.78, NNZs: 81, Bias: -0.797610, T: 290640, Avg. loss: 0.123847\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.66, NNZs: 23, Bias: -1.000976, T: 302750, Avg. loss: 0.025828\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.16, NNZs: 78, Bias: -1.099047, T: 266420, Avg. loss: 0.109276\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.09, NNZs: 59, Bias: -1.022341, T: 326970, Avg. loss: 0.035614\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.79, NNZs: 47, Bias: -1.007535, T: 326970, Avg. loss: 0.039870\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.01, NNZs: 75, Bias: -0.958677, T: 242200, Avg. loss: 0.066541\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 1.79, NNZs: 59, Bias: -1.035339, T: 242200, Avg. loss: 0.037169\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 2.39, NNZs: 76, Bias: -0.904529, T: 290640, Avg. loss: 0.136828\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.21, NNZs: 64, Bias: -0.995527, T: 278530, Avg. loss: 0.094536\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.65, NNZs: 23, Bias: -1.000987, T: 314860, Avg. loss: 0.025796Norm: 2.15, NNZs: 77, Bias: -1.090645, T: 278530, Avg. loss: 0.109274\n",
      "Total training time: 0.38 seconds.\n",
      "Norm: 2.76, NNZs: 81, Bias: -0.807973, T: 302750, Avg. loss: 0.123900\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 26\n",
      "-- Epoch 24\n",
      "\n",
      "Total training time: 0.40 seconds.\n",
      "Norm: 1.08, NNZs: 58, Bias: -1.032995, T: 339080, Avg. loss: 0.035506\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.26, NNZs: 33, Bias: -1.005571, T: 278530, Avg. loss: 0.083929\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 24\n",
      "-- Epoch 27\n",
      "Norm: 0.78, NNZs: 45, Bias: -1.004530, T: 339080, Avg. loss: 0.039944\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.90, NNZs: 47, Bias: -1.009363, T: 278530, Avg. loss: 0.034516\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 2.98, NNZs: 86, Bias: -0.531479, T: 254310, Avg. loss: 0.205261\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.78, NNZs: 59, Bias: -1.041057, T: 254310, Avg. loss: 0.037067\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 2.38, NNZs: 76, Bias: -0.910580, T: 302750, Avg. loss: 0.136920\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.02, NNZs: 75, Bias: -0.962055, T: 254310, Avg. loss: 0.067141\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 1.19, NNZs: 65, Bias: -0.991526, T: 290640, Avg. loss: 0.094303\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.76, NNZs: 47, Bias: -0.999703, T: 351190, Avg. loss: 0.039919\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 0.64, NNZs: 29, Bias: -1.002032, T: 326970, Avg. loss: 0.025780\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.07, NNZs: 59, Bias: -1.022215, T: 351190, Avg. loss: 0.035503\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.24, NNZs: 37, Bias: -1.018216, T: 290640, Avg. loss: 0.083909\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.76, NNZs: 81, Bias: -0.799157, T: 314860, Avg. loss: 0.123761\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2.14, NNZs: 77, Bias: -1.107755, T: 290640, Avg. loss: 0.108934\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 2.98, NNZs: 86, Bias: -0.554178, T: 266420, Avg. loss: 0.204601\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.90, NNZs: 47, Bias: -1.011466, T: 290640, Avg. loss: 0.034535\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.75, NNZs: 49, Bias: -1.000783, T: 363300, Avg. loss: 0.039842\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 0.63, NNZs: 23, Bias: -1.001105, T: 339080, Avg. loss: 0.025788\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2.01, NNZs: 75, Bias: -0.939742, T: 266420, Avg. loss: 0.066738\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.16, NNZs: 67, Bias: -1.002950, T: 302750, Avg. loss: 0.094500\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1.07, NNZs: 59, Bias: -1.022934, T: 363300, Avg. loss: 0.035502\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2.76, NNZs: 81, Bias: -0.790937, T: 326970, Avg. loss: 0.123568\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.13, NNZs: 77, Bias: -1.096364, T: 302750, Avg. loss: 0.109009\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1.21, NNZs: 45, Bias: -1.004653, T: 302750, Avg. loss: 0.083872\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.37, NNZs: 76, Bias: -0.921607, T: 314860, Avg. loss: 0.136408\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.89, NNZs: 47, Bias: -1.021314, T: 302750, Avg. loss: 0.034475\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 0.74, NNZs: 46, Bias: -1.003515, T: 375410, Avg. loss: 0.039869\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 0.61, NNZs: 27, Bias: -1.005843, T: 351190, Avg. loss: 0.025729\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.96, NNZs: 86, Bias: -0.533185, T: 278530, Avg. loss: 0.204922\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.77, NNZs: 60, Bias: -1.031071, T: 266420, Avg. loss: 0.036925\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 1.06, NNZs: 58, Bias: -1.017404, T: 375410, Avg. loss: 0.035535\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 2.00, NNZs: 75, Bias: -0.964134, T: 278530, Avg. loss: 0.066867\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.14, NNZs: 63, Bias: -1.008387, T: 314860, Avg. loss: 0.094150\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.19, NNZs: 33, Bias: -1.004752, T: 314860, Avg. loss: 0.083907\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2.75, NNZs: 81, Bias: -0.795496, T: 339080, Avg. loss: 0.123640\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 0.72, NNZs: 37, Bias: -0.999286, T: 387520, Avg. loss: 0.039768\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 2.36, NNZs: 75, Bias: -0.936798, T: 326970, Avg. loss: 0.136516\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.13, NNZs: 77, Bias: -1.099239, T: 314860, Avg. loss: 0.108790\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.89, NNZs: 47, Bias: -1.010341, T: 314860, Avg. loss: 0.034544\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.60, NNZs: 28, Bias: -1.004833, T: 363300, Avg. loss: 0.025774\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2.97, NNZs: 86, Bias: -0.519184, T: 290640, Avg. loss: 0.204671\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.05, NNZs: 56, Bias: -1.020857, T: 387520, Avg. loss: 0.035472\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1.12, NNZs: 63, Bias: -0.995994, T: 326970, Avg. loss: 0.094293\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.71, NNZs: 44, Bias: -1.005306, T: 399630, Avg. loss: 0.039786\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 2.00, NNZs: 75, Bias: -0.960145, T: 290640, Avg. loss: 0.066803\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 1.76, NNZs: 59, Bias: -1.037624, T: 278530, Avg. loss: 0.037003\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 1.16, NNZs: 32, Bias: -1.009921, T: 326970, Avg. loss: 0.083769\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.59, NNZs: 32, Bias: -1.007388, T: 375410, Avg. loss: 0.025731\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 2.35, NNZs: 75, Bias: -0.918052, T: 339080, Avg. loss: 0.136568\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2.12, NNZs: 78, Bias: -1.095604, T: 326970, Avg. loss: 0.108933\n",
      "Total training time: 0.44 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.89, NNZs: 47, Bias: -1.021760, T: 326970, Avg. loss: 0.034483\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.75, NNZs: 81, Bias: -0.804038, T: 351190, Avg. loss: 0.123566\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.05, NNZs: 58, Bias: -1.014264, T: 399630, Avg. loss: 0.035531\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 0.70, NNZs: 50, Bias: -1.002849, T: 411740, Avg. loss: 0.039825\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 2.96, NNZs: 86, Bias: -0.558165, T: 302750, Avg. loss: 0.204860\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 1.10, NNZs: 67, Bias: -1.007138, T: 339080, Avg. loss: 0.094088\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.75, NNZs: 59, Bias: -1.036448, T: 290640, Avg. loss: 0.037025\n",
      "Total training time: 0.46 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.58, NNZs: 28, Bias: -1.000271, T: 387520, Avg. loss: 0.025806\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1.99, NNZs: 75, Bias: -0.954449, T: 302750, Avg. loss: 0.066946\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.35, NNZs: 75, Bias: -0.915297, T: 351190, Avg. loss: 0.136236\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.12, NNZs: 78, Bias: -1.097190, T: 339080, Avg. loss: 0.108909\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.89, NNZs: 47, Bias: -1.009915, T: 339080, Avg. loss: 0.034500\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.04, NNZs: 58, Bias: -1.019201, T: 411740, Avg. loss: 0.035529\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 0.69, NNZs: 52, Bias: -0.998104, T: 423850, Avg. loss: 0.039795\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 2.74, NNZs: 81, Bias: -0.813104, T: 363300, Avg. loss: 0.123504\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1.14, NNZs: 35, Bias: -1.004909, T: 339080, Avg. loss: 0.083783\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 0.58, NNZs: 28, Bias: -0.999464, T: 399630, Avg. loss: 0.025762\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1.08, NNZs: 65, Bias: -1.002327, T: 351190, Avg. loss: 0.094157\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.74, NNZs: 59, Bias: -1.038697, T: 302750, Avg. loss: 0.036972\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 2.34, NNZs: 75, Bias: -0.924659, T: 363300, Avg. loss: 0.136255\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2.95, NNZs: 86, Bias: -0.533029, T: 314860, Avg. loss: 0.204265\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 1.99, NNZs: 75, Bias: -0.949328, T: 314860, Avg. loss: 0.066864\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 2.11, NNZs: 79, Bias: -1.091589, T: 351190, Avg. loss: 0.108798\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 0.68, NNZs: 51, Bias: -1.002112, T: 435960, Avg. loss: 0.039819\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1.04, NNZs: 58, Bias: -1.020048, T: 423850, Avg. loss: 0.035470\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 2.74, NNZs: 81, Bias: -0.798571, T: 375410, Avg. loss: 0.123610\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 0.57, NNZs: 21, Bias: -1.000296, T: 411740, Avg. loss: 0.025746\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1.88, NNZs: 47, Bias: -1.007183, T: 351190, Avg. loss: 0.034435\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.13, NNZs: 36, Bias: -1.004967, T: 351190, Avg. loss: 0.083745\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.74, NNZs: 60, Bias: -1.035614, T: 314860, Avg. loss: 0.036948\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.67, NNZs: 46, Bias: -1.002852, T: 448070, Avg. loss: 0.039817\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1.07, NNZs: 63, Bias: -1.003290, T: 363300, Avg. loss: 0.094049\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2.11, NNZs: 79, Bias: -1.090612, T: 363300, Avg. loss: 0.108820\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2.33, NNZs: 75, Bias: -0.928367, T: 375410, Avg. loss: 0.136299\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1.98, NNZs: 76, Bias: -0.957630, T: 326970, Avg. loss: 0.066837\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.03, NNZs: 57, Bias: -1.017772, T: 435960, Avg. loss: 0.035452\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 0.56, NNZs: 32, Bias: -1.001105, T: 423850, Avg. loss: 0.025737\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1.88, NNZs: 47, Bias: -1.012863, T: 363300, Avg. loss: 0.034439\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1.11, NNZs: 40, Bias: -1.013300, T: 363300, Avg. loss: 0.083694\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2.73, NNZs: 81, Bias: -0.814324, T: 387520, Avg. loss: 0.123424\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1.74, NNZs: 60, Bias: -1.048255, T: 326970, Avg. loss: 0.036828\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 2.94, NNZs: 86, Bias: -0.544569, T: 326970, Avg. loss: 0.204196\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 1.05, NNZs: 60, Bias: -1.000615, T: 375410, Avg. loss: 0.093948\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 2.10, NNZs: 79, Bias: -1.106294, T: 375410, Avg. loss: 0.108651\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 0.55, NNZs: 29, Bias: -1.002638, T: 435960, Avg. loss: 0.025725\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 2.33, NNZs: 75, Bias: -0.926661, T: 387520, Avg. loss: 0.136467\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1.98, NNZs: 76, Bias: -0.956557, T: 339080, Avg. loss: 0.066708\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.88, NNZs: 47, Bias: -1.010227, T: 375410, Avg. loss: 0.034448\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1.09, NNZs: 41, Bias: -1.005100, T: 375410, Avg. loss: 0.083642\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1.03, NNZs: 57, Bias: -1.025282, T: 448070, Avg. loss: 0.035453\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.66, NNZs: 44, Bias: -0.997836, T: 460180, Avg. loss: 0.039771\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 2.94, NNZs: 86, Bias: -0.557628, T: 339080, Avg. loss: 0.204353\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 2.73, NNZs: 81, Bias: -0.799891, T: 399630, Avg. loss: 0.123402\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 0.54, NNZs: 24, Bias: -0.999635, T: 448070, Avg. loss: 0.025741\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1.74, NNZs: 60, Bias: -1.029177, T: 339080, Avg. loss: 0.036811\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 1.03, NNZs: 61, Bias: -0.999036, T: 387520, Avg. loss: 0.093915\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 2.10, NNZs: 78, Bias: -1.098562, T: 387520, Avg. loss: 0.108586\n",
      "Total training time: 0.51 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1.98, NNZs: 75, Bias: -0.959329, T: 351190, Avg. loss: 0.066640\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.87, NNZs: 47, Bias: -1.013702, T: 387520, Avg. loss: 0.034445\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1.07, NNZs: 32, Bias: -1.007727, T: 387520, Avg. loss: 0.083613\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 0.65, NNZs: 46, Bias: -1.002173, T: 472290, Avg. loss: 0.039740\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 0.53, NNZs: 29, Bias: -1.004732, T: 460180, Avg. loss: 0.025712\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1.73, NNZs: 60, Bias: -1.030443, T: 351190, Avg. loss: 0.036890\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 2.94, NNZs: 86, Bias: -0.538033, T: 351190, Avg. loss: 0.204310\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 1.02, NNZs: 57, Bias: -1.017182, T: 460180, Avg. loss: 0.035459\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 2.73, NNZs: 81, Bias: -0.801464, T: 411740, Avg. loss: 0.123332\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1.02, NNZs: 64, Bias: -0.997566, T: 399630, Avg. loss: 0.093917\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 2.10, NNZs: 79, Bias: -1.079209, T: 399630, Avg. loss: 0.108556\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1.97, NNZs: 75, Bias: -0.958275, T: 363300, Avg. loss: 0.066656\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 0.65, NNZs: 47, Bias: -1.000103, T: 484400, Avg. loss: 0.039728\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 0.53, NNZs: 29, Bias: -1.003223, T: 472290, Avg. loss: 0.025741\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1.06, NNZs: 35, Bias: -1.004313, T: 399630, Avg. loss: 0.083577\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1.87, NNZs: 47, Bias: -1.017043, T: 399630, Avg. loss: 0.034383\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 2.33, NNZs: 78, Bias: -0.919094, T: 399630, Avg. loss: 0.135924\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1.73, NNZs: 60, Bias: -1.034348, T: 363300, Avg. loss: 0.036855\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 2.72, NNZs: 81, Bias: -0.814213, T: 423850, Avg. loss: 0.123353\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 2.09, NNZs: 79, Bias: -1.109310, T: 411740, Avg. loss: 0.108575\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1.00, NNZs: 66, Bias: -1.006659, T: 411740, Avg. loss: 0.093755\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 2.93, NNZs: 86, Bias: -0.553552, T: 363300, Avg. loss: 0.204004\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 1.02, NNZs: 56, Bias: -1.015787, T: 472290, Avg. loss: 0.035467\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 0.52, NNZs: 27, Bias: -1.000433, T: 484400, Avg. loss: 0.025750\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 0.64, NNZs: 47, Bias: -1.003543, T: 496510, Avg. loss: 0.039745\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1.04, NNZs: 33, Bias: -1.007589, T: 411740, Avg. loss: 0.083589\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1.87, NNZs: 47, Bias: -1.010343, T: 411740, Avg. loss: 0.034418\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1.97, NNZs: 75, Bias: -0.958177, T: 375410, Avg. loss: 0.066623\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 2.32, NNZs: 78, Bias: -0.928651, T: 411740, Avg. loss: 0.136316\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 1.73, NNZs: 60, Bias: -1.044273, T: 375410, Avg. loss: 0.036884\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 2.72, NNZs: 81, Bias: -0.816225, T: 435960, Avg. loss: 0.123393\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 0.51, NNZs: 24, Bias: -1.000445, T: 496510, Avg. loss: 0.025725\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 2.09, NNZs: 79, Bias: -1.093159, T: 423850, Avg. loss: 0.108639\n",
      "Total training time: 0.55 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 0.99, NNZs: 68, Bias: -1.004236, T: 423850, Avg. loss: 0.093848\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1.01, NNZs: 56, Bias: -1.019889, T: 484400, Avg. loss: 0.035386\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 0.63, NNZs: 41, Bias: -1.003574, T: 508620, Avg. loss: 0.039727\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1.03, NNZs: 38, Bias: -1.005971, T: 423850, Avg. loss: 0.083499\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 2.94, NNZs: 86, Bias: -0.518215, T: 375410, Avg. loss: 0.203750\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 1.87, NNZs: 47, Bias: -1.010365, T: 423850, Avg. loss: 0.034435\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 2.31, NNZs: 79, Bias: -0.938937, T: 423850, Avg. loss: 0.136083\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1.96, NNZs: 75, Bias: -0.955626, T: 387520, Avg. loss: 0.066631\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 1.72, NNZs: 60, Bias: -1.045788, T: 387520, Avg. loss: 0.036787\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 0.51, NNZs: 29, Bias: -1.001124, T: 508620, Avg. loss: 0.025756\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 2.09, NNZs: 79, Bias: -1.093914, T: 435960, Avg. loss: 0.108673\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 2.72, NNZs: 81, Bias: -0.791574, T: 448070, Avg. loss: 0.123256\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.62, NNZs: 38, Bias: -0.999726, T: 520730, Avg. loss: 0.039709\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 0.97, NNZs: 66, Bias: -1.005698, T: 435960, Avg. loss: 0.093806\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1.01, NNZs: 33, Bias: -1.000607, T: 435960, Avg. loss: 0.083423\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 2.31, NNZs: 81, Bias: -0.934968, T: 435960, Avg. loss: 0.136083\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1.87, NNZs: 47, Bias: -1.016611, T: 435960, Avg. loss: 0.034392\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 1.01, NNZs: 56, Bias: -1.017783, T: 496510, Avg. loss: 0.035440\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 0.50, NNZs: 26, Bias: -1.002400, T: 520730, Avg. loss: 0.025709\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1.72, NNZs: 60, Bias: -1.049034, T: 399630, Avg. loss: 0.036809\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 1.96, NNZs: 75, Bias: -0.944909, T: 399630, Avg. loss: 0.066507\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 2.92, NNZs: 86, Bias: -0.531703, T: 387520, Avg. loss: 0.204102\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 2.08, NNZs: 79, Bias: -1.098481, T: 448070, Avg. loss: 0.108529\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.61, NNZs: 38, Bias: -1.002915, T: 532840, Avg. loss: 0.039666\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 2.72, NNZs: 81, Bias: -0.800548, T: 460180, Avg. loss: 0.123409\n",
      "Total training time: 0.58 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 0.96, NNZs: 65, Bias: -1.000400, T: 448070, Avg. loss: 0.093602\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.50, NNZs: 26, Bias: -0.999863, T: 532840, Avg. loss: 0.025718\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 2.31, NNZs: 81, Bias: -0.925948, T: 448070, Avg. loss: 0.136033\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1.87, NNZs: 47, Bias: -1.003128, T: 448070, Avg. loss: 0.034355\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 1.00, NNZs: 43, Bias: -1.008891, T: 448070, Avg. loss: 0.083429\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.61, NNZs: 46, Bias: -1.002254, T: 544950, Avg. loss: 0.039697\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 2.08, NNZs: 79, Bias: -1.095428, T: 460180, Avg. loss: 0.108402\n",
      "Total training time: 0.59 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 2.92, NNZs: 86, Bias: -0.536091, T: 399630, Avg. loss: 0.203980\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 0.95, NNZs: 69, Bias: -1.008438, T: 460180, Avg. loss: 0.093619\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 0.49, NNZs: 28, Bias: -0.999285, T: 544950, Avg. loss: 0.025754\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1.01, NNZs: 56, Bias: -1.012569, T: 508620, Avg. loss: 0.035430\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1.72, NNZs: 60, Bias: -1.044102, T: 411740, Avg. loss: 0.036723\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 2.30, NNZs: 81, Bias: -0.931108, T: 460180, Avg. loss: 0.135976\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1.87, NNZs: 47, Bias: -1.013426, T: 460180, Avg. loss: 0.034395\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 1.96, NNZs: 76, Bias: -0.956657, T: 411740, Avg. loss: 0.066580\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 0.60, NNZs: 42, Bias: -1.001067, T: 557060, Avg. loss: 0.039692\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 0.99, NNZs: 37, Bias: -1.005188, T: 460180, Avg. loss: 0.083472\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 0.49, NNZs: 31, Bias: -1.000521, T: 557060, Avg. loss: 0.025706\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 2.08, NNZs: 79, Bias: -1.103876, T: 472290, Avg. loss: 0.108296\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 0.94, NNZs: 66, Bias: -1.006911, T: 472290, Avg. loss: 0.093675\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 2.92, NNZs: 86, Bias: -0.534542, T: 411740, Avg. loss: 0.203913\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 2.71, NNZs: 81, Bias: -0.795725, T: 472290, Avg. loss: 0.123413\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 2.29, NNZs: 81, Bias: -0.933285, T: 472290, Avg. loss: 0.135981\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1.86, NNZs: 47, Bias: -1.011252, T: 472290, Avg. loss: 0.034368\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1.00, NNZs: 56, Bias: -1.021606, T: 520730, Avg. loss: 0.035432\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 0.97, NNZs: 28, Bias: -0.999541, T: 472290, Avg. loss: 0.083385\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1.96, NNZs: 76, Bias: -0.958196, T: 423850, Avg. loss: 0.066581\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 0.48, NNZs: 25, Bias: -1.003459, T: 569170, Avg. loss: 0.025688\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 2.08, NNZs: 79, Bias: -1.089954, T: 484400, Avg. loss: 0.108243\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 0.59, NNZs: 38, Bias: -1.003404, T: 569170, Avg. loss: 0.039669\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 0.93, NNZs: 69, Bias: -1.002721, T: 484400, Avg. loss: 0.093448\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 2.91, NNZs: 86, Bias: -0.530747, T: 423850, Avg. loss: 0.203759\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 2.29, NNZs: 81, Bias: -0.933326, T: 484400, Avg. loss: 0.135988\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 2.71, NNZs: 81, Bias: -0.804110, T: 484400, Avg. loss: 0.123412\n",
      "Total training time: 0.62 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 0.48, NNZs: 27, Bias: -1.000549, T: 581280, Avg. loss: 0.025716\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1.72, NNZs: 61, Bias: -1.044939, T: 423850, Avg. loss: 0.036817\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 1.00, NNZs: 56, Bias: -1.024665, T: 532840, Avg. loss: 0.035406\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1.86, NNZs: 47, Bias: -1.016133, T: 484400, Avg. loss: 0.034289\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 0.96, NNZs: 37, Bias: -1.004477, T: 484400, Avg. loss: 0.083345\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 2.08, NNZs: 79, Bias: -1.097466, T: 496510, Avg. loss: 0.108314\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 0.59, NNZs: 45, Bias: -1.005115, T: 581280, Avg. loss: 0.039624\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 2.91, NNZs: 86, Bias: -0.560839, T: 435960, Avg. loss: 0.203574\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 0.47, NNZs: 23, Bias: -0.999988, T: 593390, Avg. loss: 0.025710\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 2.29, NNZs: 81, Bias: -0.929329, T: 496510, Avg. loss: 0.135855\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1.96, NNZs: 75, Bias: -0.957413, T: 435960, Avg. loss: 0.066587\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 2.71, NNZs: 81, Bias: -0.789951, T: 496510, Avg. loss: 0.123200\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 1.00, NNZs: 56, Bias: -1.018353, T: 544950, Avg. loss: 0.035413\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 0.92, NNZs: 65, Bias: -1.001379, T: 496510, Avg. loss: 0.093525\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 0.95, NNZs: 44, Bias: -1.006472, T: 496510, Avg. loss: 0.083357\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 0.58, NNZs: 48, Bias: -1.002269, T: 593390, Avg. loss: 0.039633\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1.86, NNZs: 47, Bias: -1.008672, T: 496510, Avg. loss: 0.034322\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 2.08, NNZs: 79, Bias: -1.091472, T: 508620, Avg. loss: 0.108331\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 0.47, NNZs: 27, Bias: -1.000010, T: 605500, Avg. loss: 0.025695\n",
      "Total training time: 0.68 seconds.\n",
      "Norm: 2.91, NNZs: 86, Bias: -0.538647, T: 448070, Avg. loss: 0.203815\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 2.71, NNZs: 81, Bias: -0.805730, T: 508620, Avg. loss: 0.123361\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 0.99, NNZs: 57, Bias: -1.019547, T: 557060, Avg. loss: 0.035356\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 0.58, NNZs: 47, Bias: -0.998429, T: 605500, Avg. loss: 0.039655\n",
      "Total training time: 0.72 seconds.\n",
      "Norm: 1.72, NNZs: 61, Bias: -1.038104, T: 435960, Avg. loss: 0.036712\n",
      "Total training time: 0.67 seconds.\n",
      "Norm: 0.94, NNZs: 38, Bias: -1.005115, T: 508620, Avg. loss: 0.083372\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 0.90, NNZs: 69, Bias: -1.007969, T: 508620, Avg. loss: 0.093566\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 43\n",
      "-- Epoch 37\n",
      "Norm: 2.07, NNZs: 79, Bias: -1.102332, T: 520730, Avg. loss: 0.108262\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1.86, NNZs: 48, Bias: -1.016568, T: 508620, Avg. loss: 0.034335\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1.95, NNZs: 75, Bias: -0.960446, T: 448070, Avg. loss: 0.066489\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.99, NNZs: 57, Bias: -1.022507, T: 569170, Avg. loss: 0.035423\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 2.71, NNZs: 81, Bias: -0.808193, T: 520730, Avg. loss: 0.123194\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 2.90, NNZs: 86, Bias: -0.543806, T: 460180, Avg. loss: 0.203667\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 0.93, NNZs: 37, Bias: -1.005743, T: 520730, Avg. loss: 0.083535\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 2.28, NNZs: 80, Bias: -0.939282, T: 508620, Avg. loss: 0.135550\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1.72, NNZs: 61, Bias: -1.045830, T: 448070, Avg. loss: 0.036741\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 2.07, NNZs: 79, Bias: -1.102813, T: 532840, Avg. loss: 0.108214\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1.86, NNZs: 48, Bias: -1.012609, T: 520730, Avg. loss: 0.034320\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 0.89, NNZs: 66, Bias: -1.001497, T: 520730, Avg. loss: 0.093503\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 0.99, NNZs: 56, Bias: -1.019617, T: 581280, Avg. loss: 0.035404\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1.95, NNZs: 75, Bias: -0.960490, T: 460180, Avg. loss: 0.066556\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 0.92, NNZs: 45, Bias: -1.006346, T: 532840, Avg. loss: 0.083293\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 2.71, NNZs: 81, Bias: -0.807485, T: 532840, Avg. loss: 0.123134\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 2.90, NNZs: 86, Bias: -0.537449, T: 472290, Avg. loss: 0.203697\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 1.71, NNZs: 61, Bias: -1.042729, T: 460180, Avg. loss: 0.036691\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 2.28, NNZs: 80, Bias: -0.939334, T: 520730, Avg. loss: 0.135855\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 2.07, NNZs: 79, Bias: -1.092317, T: 544950, Avg. loss: 0.108332\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1.86, NNZs: 48, Bias: -1.012499, T: 532840, Avg. loss: 0.034310\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 0.99, NNZs: 57, Bias: -1.022962, T: 593390, Avg. loss: 0.035366\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 0.91, NNZs: 37, Bias: -1.005701, T: 544950, Avg. loss: 0.083344\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 0.88, NNZs: 67, Bias: -1.000903, T: 532840, Avg. loss: 0.093554\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 1.95, NNZs: 75, Bias: -0.965332, T: 472290, Avg. loss: 0.066449\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 2.71, NNZs: 81, Bias: -0.806773, T: 544950, Avg. loss: 0.123208\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 2.06, NNZs: 79, Bias: -1.094796, T: 557060, Avg. loss: 0.108171\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1.71, NNZs: 61, Bias: -1.044043, T: 472290, Avg. loss: 0.036668\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 0.98, NNZs: 56, Bias: -1.025658, T: 605500, Avg. loss: 0.035371\n",
      "Total training time: 0.72 seconds.\n",
      "Norm: 2.90, NNZs: 86, Bias: -0.542346, T: 484400, Avg. loss: 0.203639\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 1.86, NNZs: 48, Bias: -1.009429, T: 544950, Avg. loss: 0.034285\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 2.28, NNZs: 81, Bias: -0.927405, T: 532840, Avg. loss: 0.135698\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 0.90, NNZs: 40, Bias: -1.004481, T: 557060, Avg. loss: 0.083342\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 0.87, NNZs: 66, Bias: -0.999671, T: 544950, Avg. loss: 0.093591\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1.71, NNZs: 61, Bias: -1.041277, T: 484400, Avg. loss: 0.036659\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 2.06, NNZs: 79, Bias: -1.099425, T: 569170, Avg. loss: 0.108280\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1.95, NNZs: 75, Bias: -0.951370, T: 484400, Avg. loss: 0.066504\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 2.70, NNZs: 81, Bias: -0.796538, T: 557060, Avg. loss: 0.123157\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 2.89, NNZs: 86, Bias: -0.542492, T: 496510, Avg. loss: 0.203312\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 0.89, NNZs: 44, Bias: -1.005614, T: 569170, Avg. loss: 0.083420\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1.86, NNZs: 49, Bias: -1.008253, T: 557060, Avg. loss: 0.034299\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 2.27, NNZs: 81, Bias: -0.932410, T: 544950, Avg. loss: 0.135640\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 2.06, NNZs: 79, Bias: -1.102236, T: 581280, Avg. loss: 0.108202\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 0.86, NNZs: 65, Bias: -1.004512, T: 557060, Avg. loss: 0.093428\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1.70, NNZs: 61, Bias: -1.040027, T: 496510, Avg. loss: 0.036687\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 2.27, NNZs: 81, Bias: -0.928713, T: 557060, Avg. loss: 0.135621\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 2.71, NNZs: 81, Bias: -0.792521, T: 569170, Avg. loss: 0.123007\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 0.88, NNZs: 31, Bias: -1.003882, T: 581280, Avg. loss: 0.083294\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1.95, NNZs: 75, Bias: -0.956764, T: 496510, Avg. loss: 0.066475\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 2.89, NNZs: 86, Bias: -0.541797, T: 508620, Avg. loss: 0.203536\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1.86, NNZs: 50, Bias: -1.009497, T: 569170, Avg. loss: 0.034287\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 2.06, NNZs: 79, Bias: -1.095337, T: 593390, Avg. loss: 0.108170\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1.70, NNZs: 61, Bias: -1.032876, T: 508620, Avg. loss: 0.036806\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 0.86, NNZs: 67, Bias: -1.006229, T: 569170, Avg. loss: 0.093435\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 0.87, NNZs: 34, Bias: -1.003881, T: 593390, Avg. loss: 0.083254\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 2.05, NNZs: 79, Bias: -1.104686, T: 605500, Avg. loss: 0.108145\n",
      "Total training time: 0.75 seconds.\n",
      "Norm: 2.27, NNZs: 81, Bias: -0.940527, T: 569170, Avg. loss: 0.135639\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 2.71, NNZs: 81, Bias: -0.792732, T: 581280, Avg. loss: 0.123008\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1.85, NNZs: 51, Bias: -1.016381, T: 581280, Avg. loss: 0.034233\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1.95, NNZs: 75, Bias: -0.954110, T: 508620, Avg. loss: 0.066443\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 1.70, NNZs: 61, Bias: -1.038717, T: 520730, Avg. loss: 0.036636\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 2.89, NNZs: 86, Bias: -0.535861, T: 520730, Avg. loss: 0.203479\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 2.70, NNZs: 81, Bias: -0.813602, T: 593390, Avg. loss: 0.123102\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 2.26, NNZs: 81, Bias: -0.934069, T: 581280, Avg. loss: 0.135361\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 0.86, NNZs: 39, Bias: -1.002237, T: 605500, Avg. loss: 0.083224\n",
      "Total training time: 0.81 seconds.\n",
      "Norm: 0.85, NNZs: 69, Bias: -1.005031, T: 581280, Avg. loss: 0.093349\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1.95, NNZs: 75, Bias: -0.959377, T: 520730, Avg. loss: 0.066380\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 1.85, NNZs: 51, Bias: -1.012907, T: 593390, Avg. loss: 0.034266\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1.70, NNZs: 61, Bias: -1.041847, T: 532840, Avg. loss: 0.036645\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 2.70, NNZs: 81, Bias: -0.812336, T: 605500, Avg. loss: 0.123024\n",
      "Total training time: 0.76 seconds.\n",
      "Norm: 2.89, NNZs: 86, Bias: -0.540234, T: 532840, Avg. loss: 0.203541\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 2.26, NNZs: 81, Bias: -0.939624, T: 593390, Avg. loss: 0.135603\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1.94, NNZs: 75, Bias: -0.961217, T: 532840, Avg. loss: 0.066430\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 0.84, NNZs: 65, Bias: -1.001640, T: 593390, Avg. loss: 0.093353\n",
      "Total training time: 0.80 seconds.\n",
      "Norm: 1.70, NNZs: 61, Bias: -1.038154, T: 544950, Avg. loss: 0.036697\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 46\n",
      "-- Epoch 50\n",
      "Norm: 1.85, NNZs: 51, Bias: -1.014493, T: 605500, Avg. loss: 0.034229\n",
      "Total training time: 0.82 seconds.\n",
      "Norm: 2.26, NNZs: 81, Bias: -0.932249, T: 605500, Avg. loss: 0.135568\n",
      "Total training time: 0.81 seconds.\n",
      "Norm: 1.94, NNZs: 75, Bias: -0.964872, T: 544950, Avg. loss: 0.066376\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 2.89, NNZs: 86, Bias: -0.540969, T: 544950, Avg. loss: 0.203470\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 1.94, NNZs: 75, Bias: -0.957532, T: 557060, Avg. loss: 0.066290\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1.70, NNZs: 61, Bias: -1.038238, T: 557060, Avg. loss: 0.036615\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 0.83, NNZs: 68, Bias: -1.009366, T: 605500, Avg. loss: 0.093227\n",
      "Total training time: 0.81 seconds.\n",
      "Norm: 2.89, NNZs: 86, Bias: -0.541933, T: 557060, Avg. loss: 0.203284\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 1.94, NNZs: 75, Bias: -0.958730, T: 569170, Avg. loss: 0.066404\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1.70, NNZs: 61, Bias: -1.032970, T: 569170, Avg. loss: 0.036692\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 2.89, NNZs: 86, Bias: -0.534344, T: 569170, Avg. loss: 0.203423\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 1.94, NNZs: 75, Bias: -0.958162, T: 581280, Avg. loss: 0.066396\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1.70, NNZs: 61, Bias: -1.034144, T: 581280, Avg. loss: 0.036630\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 2.88, NNZs: 86, Bias: -0.528072, T: 581280, Avg. loss: 0.203225\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 1.70, NNZs: 61, Bias: -1.041028, T: 593390, Avg. loss: 0.036639\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1.94, NNZs: 75, Bias: -0.963190, T: 593390, Avg. loss: 0.066324\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 2.88, NNZs: 86, Bias: -0.547720, T: 593390, Avg. loss: 0.203231\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 1.94, NNZs: 75, Bias: -0.961060, T: 605500, Avg. loss: 0.066340\n",
      "Total training time: 0.82 seconds.\n",
      "Norm: 1.69, NNZs: 61, Bias: -1.046022, T: 605500, Avg. loss: 0.036645\n",
      "Total training time: 0.83 seconds.\n",
      "Norm: 2.88, NNZs: 86, Bias: -0.542066, T: 605500, Avg. loss: 0.203262\n",
      "Total training time: 0.83 seconds.\n",
      "Accuracy on validation set:  0.786979969183359\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.8s remaining:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "# tune hyper-parameter max_iter\n",
    "hyper_param_values = [5, 10, 15, 30, 50]\n",
    "accuracies = []\n",
    "\n",
    "for hyper_param_value in hyper_param_values:\n",
    "    \n",
    "    print(f\"Classifier with {hyper_param_value} maximum iterations.\")\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = SGDClassifier(loss='hinge', penalty='elasticnet', alpha=3e-3, random_state=42, max_iter=hyper_param_value, tol=None, n_jobs=-1, verbose=1)\n",
    "    clf.fit(tf_idf_matrix_train_lsa, y_train)\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    y_pred = clf.predict(tf_idf_matrix_val_lsa)\n",
    "    acc_score = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(acc_score)\n",
    "    print('Accuracy on validation set: ', acc_score)\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studied hyper-parameters and the corresponding accuracies on the validation set: \n",
      "max_iter: 5, Accuracy: 0.7866\n",
      "max_iter: 10, Accuracy: 0.7885\n",
      "max_iter: 15, Accuracy: 0.7804\n",
      "max_iter: 30, Accuracy: 0.7870\n",
      "max_iter: 50, Accuracy: 0.7870\n"
     ]
    }
   ],
   "source": [
    "print(\"Studied hyper-parameters and the corresponding accuracies on the validation set: \")\n",
    "_ = [print(f\"max_iter: {param:.0f}, Accuracy: {acc:.4f}\") for param, acc in zip(hyper_param_values, accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected max_iter: 10\n"
     ]
    }
   ],
   "source": [
    "best_max_iter = hyper_param_values[np.argmax(accuracies)]\n",
    "print(f\"Selected max_iter: {best_max_iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 4.74, NNZs: 43, Bias: -1.202278, T: 12110, Avg. loss: 0.340742\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 5.83, NNZs: 52, Bias: -0.992930, T: 12110, Avg. loss: 0.556248\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.28, NNZs: 52, Bias: -2.054124, T: 12110, Avg. loss: 0.175307\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.86, NNZs: 47, Bias: -0.925892, T: 12110, Avg. loss: 0.321753\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.56, NNZs: 49, Bias: -2.214191, T: 12110, Avg. loss: 0.085581\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.72, NNZs: 46, Bias: -4.284778, T: 12110, Avg. loss: 0.135340\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.08, NNZs: 43, Bias: -1.221114, T: 12110, Avg. loss: 0.114648\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.81, NNZs: 48, Bias: -1.008379, T: 12110, Avg. loss: 0.382438\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.20, NNZs: 64, Bias: -0.729534, T: 12110, Avg. loss: 0.555616\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.69, NNZs: 35, Bias: -1.107219, T: 24220, Avg. loss: 0.106101\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.75, NNZs: 56, Bias: -1.010818, T: 24220, Avg. loss: 0.116790\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.09, NNZs: 42, Bias: -1.323091, T: 24220, Avg. loss: 0.039793\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.65, NNZs: 41, Bias: -2.261134, T: 24220, Avg. loss: 0.048649\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.57, NNZs: 50, Bias: -0.855506, T: 24220, Avg. loss: 0.157832\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.87, NNZs: 47, Bias: -2.013513, T: 12110, Avg. loss: 0.125654\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.13, NNZs: 30, Bias: -1.081421, T: 36330, Avg. loss: 0.095605\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.78, NNZs: 40, Bias: -1.005224, T: 24220, Avg. loss: 0.040646\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.44, NNZs: 55, Bias: -1.079152, T: 12110, Avg. loss: 0.184828\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.05, NNZs: 57, Bias: -1.354272, T: 12110, Avg. loss: 0.290237\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.60, NNZs: 44, Bias: -1.102872, T: 36330, Avg. loss: 0.036529\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.86, NNZs: 40, Bias: -1.060754, T: 24220, Avg. loss: 0.053242\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.17, NNZs: 34, Bias: -1.238338, T: 36330, Avg. loss: 0.033684\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.29, NNZs: 61, Bias: -0.792364, T: 24220, Avg. loss: 0.141946\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.16, NNZs: 56, Bias: -1.042360, T: 36330, Avg. loss: 0.108318\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.84, NNZs: 71, Bias: -0.505935, T: 24220, Avg. loss: 0.232047\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.78, NNZs: 35, Bias: -1.054504, T: 48440, Avg. loss: 0.091993\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.30, NNZs: 42, Bias: -1.079097, T: 24220, Avg. loss: 0.046647\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.92, NNZs: 61, Bias: -0.828342, T: 36330, Avg. loss: 0.151219\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.34, NNZs: 43, Bias: -0.953127, T: 36330, Avg. loss: 0.038707\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.42, NNZs: 45, Bias: -1.072519, T: 48440, Avg. loss: 0.036381\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.34, NNZs: 38, Bias: -1.037291, T: 36330, Avg. loss: 0.045373\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.79, NNZs: 29, Bias: -1.014494, T: 48440, Avg. loss: 0.027998\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.79, NNZs: 55, Bias: -1.030513, T: 48440, Avg. loss: 0.104199\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.76, NNZs: 66, Bias: -0.841685, T: 36330, Avg. loss: 0.133055\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.63, NNZs: 47, Bias: -1.105905, T: 36330, Avg. loss: 0.040833\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.53, NNZs: 33, Bias: -1.018872, T: 60550, Avg. loss: 0.090407\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.80, NNZs: 63, Bias: -1.179646, T: 24220, Avg. loss: 0.123296\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.30, NNZs: 44, Bias: -1.026056, T: 60550, Avg. loss: 0.035923\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.32, NNZs: 64, Bias: -1.057457, T: 24220, Avg. loss: 0.077719\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.60, NNZs: 32, Bias: -0.992656, T: 60550, Avg. loss: 0.026884\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 4.21, NNZs: 74, Bias: -0.516020, T: 36330, Avg. loss: 0.220745\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.09, NNZs: 40, Bias: -1.075941, T: 48440, Avg. loss: 0.037203\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.56, NNZs: 59, Bias: -0.949453, T: 48440, Avg. loss: 0.146809\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.48, NNZs: 57, Bias: -0.993187, T: 60550, Avg. loss: 0.101880\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.53, NNZs: 70, Bias: -0.850087, T: 48440, Avg. loss: 0.130571\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.06, NNZs: 42, Bias: -1.045242, T: 48440, Avg. loss: 0.043258\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.44, NNZs: 31, Bias: -1.013834, T: 72660, Avg. loss: 0.026457\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.25, NNZs: 45, Bias: -1.030285, T: 72660, Avg. loss: 0.035956\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.95, NNZs: 66, Bias: -0.871479, T: 36330, Avg. loss: 0.072530\n",
      "Total training time: 0.19 seconds.\n",
      "Norm: 3.31, NNZs: 67, Bias: -1.188895, T: 36330, Avg. loss: 0.119376\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.34, NNZs: 40, Bias: -1.038846, T: 72660, Avg. loss: 0.088419\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 7\n",
      "-- Epoch 4\n",
      "Norm: 3.86, NNZs: 70, Bias: -0.519921, T: 48440, Avg. loss: 0.216367\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.29, NNZs: 60, Bias: -0.978914, T: 60550, Avg. loss: 0.143817\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.27, NNZs: 56, Bias: -1.006930, T: 72660, Avg. loss: 0.099942\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.31, NNZs: 29, Bias: -1.006196, T: 84770, Avg. loss: 0.026293\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.17, NNZs: 45, Bias: -1.018767, T: 84770, Avg. loss: 0.035531\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.31, NNZs: 72, Bias: -0.844808, T: 60550, Avg. loss: 0.128293\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.17, NNZs: 33, Bias: -1.030518, T: 84770, Avg. loss: 0.087498\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.90, NNZs: 44, Bias: -1.069048, T: 60550, Avg. loss: 0.037275\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.04, NNZs: 68, Bias: -1.103322, T: 48440, Avg. loss: 0.116470\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.42, NNZs: 52, Bias: -1.041421, T: 48440, Avg. loss: 0.040565\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.86, NNZs: 36, Bias: -1.001036, T: 60550, Avg. loss: 0.042964\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.70, NNZs: 67, Bias: -0.895361, T: 48440, Avg. loss: 0.071055\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.21, NNZs: 30, Bias: -0.999730, T: 96880, Avg. loss: 0.026225\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.69, NNZs: 76, Bias: -0.528953, T: 60550, Avg. loss: 0.211507\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 3.12, NNZs: 63, Bias: -0.955785, T: 72660, Avg. loss: 0.142822\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.11, NNZs: 46, Bias: -1.003805, T: 96880, Avg. loss: 0.035623\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.21, NNZs: 73, Bias: -0.823912, T: 72660, Avg. loss: 0.128315\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.75, NNZs: 44, Bias: -1.021122, T: 72660, Avg. loss: 0.037006\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.04, NNZs: 41, Bias: -1.036385, T: 96880, Avg. loss: 0.086820\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.81, NNZs: 69, Bias: -1.026670, T: 60550, Avg. loss: 0.114669\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.14, NNZs: 31, Bias: -0.988095, T: 108990, Avg. loss: 0.026166\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.68, NNZs: 36, Bias: -1.007472, T: 72660, Avg. loss: 0.042250\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.53, NNZs: 68, Bias: -0.927839, T: 60550, Avg. loss: 0.069667\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.07, NNZs: 45, Bias: -1.015746, T: 108990, Avg. loss: 0.035560\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 3.50, NNZs: 78, Bias: -0.470003, T: 72660, Avg. loss: 0.212411\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.65, NNZs: 46, Bias: -1.036209, T: 84770, Avg. loss: 0.036689\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 3.00, NNZs: 67, Bias: -0.926494, T: 84770, Avg. loss: 0.141550\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.94, NNZs: 34, Bias: -1.010666, T: 108990, Avg. loss: 0.086693\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.07, NNZs: 24, Bias: -1.003548, T: 121100, Avg. loss: 0.026123\n",
      "Total training time: 0.26 seconds.\n",
      "Norm: 2.46, NNZs: 71, Bias: -0.963728, T: 72660, Avg. loss: 0.069238\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.69, NNZs: 74, Bias: -1.096765, T: 72660, Avg. loss: 0.113239\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.57, NNZs: 40, Bias: -0.992631, T: 84770, Avg. loss: 0.041841\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.13, NNZs: 57, Bias: -0.999164, T: 84770, Avg. loss: 0.099295\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.03, NNZs: 44, Bias: -1.041720, T: 121100, Avg. loss: 0.035482\n",
      "Total training time: 0.27 seconds.\n",
      "Norm: 3.15, NNZs: 76, Bias: -0.747406, T: 84770, Avg. loss: 0.126531\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.57, NNZs: 48, Bias: -1.018049, T: 96880, Avg. loss: 0.036686\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.40, NNZs: 78, Bias: -0.532759, T: 84770, Avg. loss: 0.210830\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.90, NNZs: 69, Bias: -0.966763, T: 96880, Avg. loss: 0.140123\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.85, NNZs: 31, Bias: -1.016580, T: 121100, Avg. loss: 0.085922\n",
      "Total training time: 0.28 seconds.\n",
      "Norm: 2.27, NNZs: 55, Bias: -1.068663, T: 60550, Avg. loss: 0.038757\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 2.38, NNZs: 73, Bias: -0.938465, T: 84770, Avg. loss: 0.069091\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.47, NNZs: 42, Bias: -0.997501, T: 96880, Avg. loss: 0.041429\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.59, NNZs: 74, Bias: -1.087181, T: 84770, Avg. loss: 0.112497\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.99, NNZs: 56, Bias: -1.028445, T: 96880, Avg. loss: 0.098096\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.08, NNZs: 75, Bias: -0.780608, T: 96880, Avg. loss: 0.126697\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.49, NNZs: 51, Bias: -1.033835, T: 108990, Avg. loss: 0.036291\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.40, NNZs: 39, Bias: -1.020571, T: 108990, Avg. loss: 0.041290\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.15, NNZs: 53, Bias: -1.022542, T: 72660, Avg. loss: 0.038452\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 2.29, NNZs: 74, Bias: -0.975806, T: 96880, Avg. loss: 0.068464\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.81, NNZs: 70, Bias: -0.979311, T: 108990, Avg. loss: 0.139017\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 3.31, NNZs: 80, Bias: -0.547962, T: 96880, Avg. loss: 0.209632\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 2.51, NNZs: 75, Bias: -1.101516, T: 96880, Avg. loss: 0.112656\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.02, NNZs: 75, Bias: -0.798918, T: 108990, Avg. loss: 0.125416\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.32, NNZs: 36, Bias: -1.009139, T: 121100, Avg. loss: 0.041097\n",
      "Total training time: 0.30 seconds.\n",
      "Norm: 1.44, NNZs: 49, Bias: -1.016712, T: 121100, Avg. loss: 0.036352\n",
      "Total training time: 0.29 seconds.\n",
      "Norm: 1.90, NNZs: 53, Bias: -1.032778, T: 108990, Avg. loss: 0.097100\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.23, NNZs: 73, Bias: -0.943043, T: 108990, Avg. loss: 0.068686\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 2.08, NNZs: 55, Bias: -1.036068, T: 84770, Avg. loss: 0.038292\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 2.76, NNZs: 69, Bias: -0.944319, T: 121100, Avg. loss: 0.139540\n",
      "Total training time: 0.31 seconds.\n",
      "Norm: 2.96, NNZs: 75, Bias: -0.836133, T: 121100, Avg. loss: 0.125539\n",
      "Total training time: 0.31 seconds.\n",
      "Norm: 2.44, NNZs: 75, Bias: -1.077080, T: 108990, Avg. loss: 0.112154\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 3.24, NNZs: 80, Bias: -0.564505, T: 108990, Avg. loss: 0.209220\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.80, NNZs: 55, Bias: -1.036172, T: 121100, Avg. loss: 0.096767\n",
      "Total training time: 0.31 seconds.\n",
      "Norm: 2.19, NNZs: 75, Bias: -0.965431, T: 121100, Avg. loss: 0.068220\n",
      "Total training time: 0.29 seconds.\n",
      "Norm: 2.02, NNZs: 54, Bias: -1.041072, T: 96880, Avg. loss: 0.038121\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 3.19, NNZs: 81, Bias: -0.536156, T: 121100, Avg. loss: 0.208217\n",
      "Total training time: 0.31 seconds.\n",
      "Norm: 2.41, NNZs: 75, Bias: -1.046030, T: 121100, Avg. loss: 0.111645\n",
      "Total training time: 0.28 seconds.\n",
      "Norm: 1.97, NNZs: 52, Bias: -1.057309, T: 108990, Avg. loss: 0.037853\n",
      "Total training time: 0.31 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.93, NNZs: 55, Bias: -1.045578, T: 121100, Avg. loss: 0.037783\n",
      "Total training time: 0.33 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.3s remaining:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.003, max_iter=10, n_jobs=-1, penalty='elasticnet',\n",
       "              random_state=42, tol=None, verbose=1)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit classifier with best hyper-parameter\n",
    "clfs[1] = SGDClassifier(loss='hinge', penalty='elasticnet', alpha=3e-3, random_state=42, max_iter=best_max_iter, tol=None, n_jobs=-1, verbose=1)\n",
    "clfs[1].fit(tf_idf_matrix_train_lsa, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7885208012326657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.11      0.19        53\n",
      "           1       0.42      0.16      0.24       122\n",
      "           2       0.88      0.87      0.88       140\n",
      "           3       0.77      0.90      0.83       420\n",
      "           4       0.91      0.91      0.91       665\n",
      "           5       0.88      0.25      0.39       113\n",
      "           6       0.29      0.07      0.11        28\n",
      "           7       0.66      0.50      0.57        38\n",
      "           8       0.86      0.81      0.83       102\n",
      "           9       0.76      0.90      0.82       546\n",
      "          10       0.63      0.78      0.70       195\n",
      "          11       0.78      0.85      0.81       174\n",
      "\n",
      "    accuracy                           0.79      2596\n",
      "   macro avg       0.70      0.59      0.61      2596\n",
      "weighted avg       0.78      0.79      0.77      2596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on validation set\n",
    "y_preds[1] = clfs[1].predict(tf_idf_matrix_val_lsa)\n",
    "val_accs[1] = accuracy_score(y_val, y_preds[1])\n",
    "print('Accuracy on validation set: ', val_accs[1])\n",
    "\n",
    "# additionally, we check the classification report\n",
    "# look at F1-Score, Precision, Recall, and relationship between Support & metrics\n",
    "print(classification_report(y_val, y_preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: BM25, high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier with l1_ratio: 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 6.88, NNZs: 1023, Bias: -10.256723, T: 12110, Avg. loss: 0.223057\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.28, NNZs: 986, Bias: -6.662168, T: 12110, Avg. loss: 0.106063\n",
      "Total training time: 0.80 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.18, NNZs: 888, Bias: -9.893028, T: 12110, Avg. loss: 0.162649\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.29, NNZs: 883, Bias: -12.772791, T: 12110, Avg. loss: 0.180869\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.35, NNZs: 986, Bias: -12.115212, T: 12110, Avg. loss: 0.152990\n",
      "Total training time: 0.90 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.26, NNZs: 1678, Bias: -10.182145, T: 12110, Avg. loss: 0.438000\n",
      "Total training time: 0.94 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.37, NNZs: 2226, Bias: -6.981875, T: 12110, Avg. loss: 0.570927\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8.89, NNZs: 1439, Bias: -9.792056, T: 12110, Avg. loss: 0.273003\n",
      "Total training time: 1.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.03, NNZs: 1668, Bias: -10.236934, T: 12110, Avg. loss: 0.459194\n",
      "Total training time: 1.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.21, NNZs: 1837, Bias: -13.441807, T: 12110, Avg. loss: 0.542587\n",
      "Total training time: 1.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.62, NNZs: 2144, Bias: -9.717246, T: 12110, Avg. loss: 0.655401\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 13.88, NNZs: 2855, Bias: -4.938698, T: 12110, Avg. loss: 0.962215\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.22, NNZs: 911, Bias: -9.282739, T: 24220, Avg. loss: 0.057317\n",
      "Total training time: 1.57 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.55, NNZs: 771, Bias: -5.916919, T: 24220, Avg. loss: 0.031129\n",
      "Total training time: 1.56 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.84, NNZs: 738, Bias: -8.795463, T: 24220, Avg. loss: 0.043734\n",
      "Total training time: 1.63 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.65, NNZs: 875, Bias: -10.976543, T: 24220, Avg. loss: 0.043682\n",
      "Total training time: 1.66 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.10, NNZs: 857, Bias: -11.609688, T: 24220, Avg. loss: 0.052128\n",
      "Total training time: 1.76 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.63, NNZs: 1257, Bias: -8.389340, T: 24220, Avg. loss: 0.069784\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.55, NNZs: 1508, Bias: -8.474985, T: 24220, Avg. loss: 0.118776\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.69, NNZs: 1688, Bias: -11.498180, T: 24220, Avg. loss: 0.157165\n",
      "Total training time: 2.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.18, NNZs: 1925, Bias: -5.116761, T: 24220, Avg. loss: 0.111485\n",
      "Total training time: 2.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.40, NNZs: 1542, Bias: -8.617038, T: 24220, Avg. loss: 0.115820\n",
      "Total training time: 2.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.91, NNZs: 712, Bias: -5.458324, T: 36330, Avg. loss: 0.022155\n",
      "Total training time: 2.33 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.41, NNZs: 1909, Bias: -7.852877, T: 24220, Avg. loss: 0.147809\n",
      "Total training time: 2.36 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.60, NNZs: 864, Bias: -8.708258, T: 36330, Avg. loss: 0.042212\n",
      "Total training time: 2.37 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.10, NNZs: 828, Bias: -10.300710, T: 36330, Avg. loss: 0.034113\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.76, NNZs: 2558, Bias: -3.437552, T: 24220, Avg. loss: 0.149349\n",
      "Total training time: 2.51 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.61, NNZs: 803, Bias: -11.011246, T: 36330, Avg. loss: 0.037814\n",
      "Total training time: 2.54 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.36, NNZs: 692, Bias: -8.196858, T: 36330, Avg. loss: 0.035550\n",
      "Total training time: 2.60 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.74, NNZs: 1228, Bias: -7.629090, T: 36330, Avg. loss: 0.049675\n",
      "Total training time: 2.76 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.59, NNZs: 1455, Bias: -7.508754, T: 36330, Avg. loss: 0.086507\n",
      "Total training time: 2.84 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.91, NNZs: 1824, Bias: -4.185748, T: 36330, Avg. loss: 0.076813\n",
      "Total training time: 3.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.24, NNZs: 827, Bias: -8.334141, T: 48440, Avg. loss: 0.035184\n",
      "Total training time: 3.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.79, NNZs: 1676, Bias: -10.375114, T: 36330, Avg. loss: 0.119421\n",
      "Total training time: 3.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.76, NNZs: 819, Bias: -9.850694, T: 48440, Avg. loss: 0.027669\n",
      "Total training time: 3.15 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.56, NNZs: 701, Bias: -5.179013, T: 48440, Avg. loss: 0.017485\n",
      "Total training time: 3.16 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.37, NNZs: 1532, Bias: -7.694320, T: 36330, Avg. loss: 0.085414\n",
      "Total training time: 3.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.04, NNZs: 696, Bias: -7.821358, T: 48440, Avg. loss: 0.029393\n",
      "Total training time: 3.31 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.24, NNZs: 1894, Bias: -6.749913, T: 36330, Avg. loss: 0.106132\n",
      "Total training time: 3.33 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.34, NNZs: 788, Bias: -10.607290, T: 48440, Avg. loss: 0.030456\n",
      "Total training time: 3.38 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.15, NNZs: 2450, Bias: -2.819761, T: 36330, Avg. loss: 0.104071\n",
      "Total training time: 3.59 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.27, NNZs: 1191, Bias: -7.107258, T: 48440, Avg. loss: 0.039744\n",
      "Total training time: 3.65 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.94, NNZs: 1448, Bias: -6.920322, T: 48440, Avg. loss: 0.071070\n",
      "Total training time: 3.71 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.03, NNZs: 855, Bias: -8.045589, T: 60550, Avg. loss: 0.030903\n",
      "Total training time: 3.83 seconds.\n",
      "Norm: 3.35, NNZs: 712, Bias: -4.952968, T: 60550, Avg. loss: 0.015495\n",
      "Total training time: 3.99 seconds.\n",
      "Norm: 3.81, NNZs: 707, Bias: -7.546431, T: 60550, Avg. loss: 0.026645\n",
      "Total training time: 4.00 seconds.\n",
      "Norm: 6.29, NNZs: 1667, Bias: -9.619998, T: 48440, Avg. loss: 0.095011\n",
      "Total training time: 4.02 seconds.\n",
      "-- Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    4.0s remaining:   20.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.18, NNZs: 785, Bias: -10.288774, T: 60550, Avg. loss: 0.025852\n",
      "Total training time: 4.05 seconds.\n",
      "Norm: 4.56, NNZs: 818, Bias: -9.500446, T: 60550, Avg. loss: 0.024886\n",
      "Total training time: 4.05 seconds.\n",
      "Norm: 5.81, NNZs: 1545, Bias: -7.090945, T: 48440, Avg. loss: 0.070381\n",
      "Total training time: 4.05 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.20, NNZs: 1842, Bias: -3.671012, T: 48440, Avg. loss: 0.063999\n",
      "Total training time: 4.15 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.49, NNZs: 1873, Bias: -6.068197, T: 48440, Avg. loss: 0.086818\n",
      "Total training time: 4.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.93, NNZs: 1193, Bias: -6.743896, T: 60550, Avg. loss: 0.034455\n",
      "Total training time: 4.47 seconds.\n",
      "Norm: 5.60, NNZs: 1473, Bias: -6.437364, T: 60550, Avg. loss: 0.061840\n",
      "Total training time: 4.58 seconds.\n",
      "Norm: 5.96, NNZs: 1688, Bias: -9.066181, T: 60550, Avg. loss: 0.083144\n",
      "Total training time: 4.61 seconds.\n",
      "Norm: 7.27, NNZs: 2469, Bias: -2.435773, T: 48440, Avg. loss: 0.085506\n",
      "Total training time: 4.61 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.46, NNZs: 1554, Bias: -6.610272, T: 60550, Avg. loss: 0.060977\n",
      "Total training time: 4.62 seconds.\n",
      "Norm: 5.70, NNZs: 1859, Bias: -3.330013, T: 60550, Avg. loss: 0.054448\n",
      "Total training time: 4.81 seconds.\n",
      "Norm: 6.02, NNZs: 1889, Bias: -5.583595, T: 60550, Avg. loss: 0.076169\n",
      "Total training time: 4.94 seconds.\n",
      "Norm: 6.74, NNZs: 2510, Bias: -2.174986, T: 60550, Avg. loss: 0.077184\n",
      "Total training time: 5.34 seconds.\n",
      "Accuracy on validation set:  0.7846687211093991\n",
      "--------------------\n",
      "Classifier with l1_ratio: 0.1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 6.74, NNZs: 447, Bias: -12.092421, T: 12110, Avg. loss: 0.184069\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.85, NNZs: 865, Bias: -8.867608, T: 12110, Avg. loss: 0.443645\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.86, NNZs: 420, Bias: -5.921574, T: 12110, Avg. loss: 0.097169\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.90, NNZs: 488, Bias: -11.652807, T: 12110, Avg. loss: 0.153998\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.81, NNZs: 753, Bias: -9.340657, T: 12110, Avg. loss: 0.271516\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.25, NNZs: 958, Bias: -9.334163, T: 12110, Avg. loss: 0.414670\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.63, NNZs: 391, Bias: -9.206153, T: 12110, Avg. loss: 0.146311\n",
      "Total training time: 0.84 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.50, NNZs: 480, Bias: -9.710926, T: 12110, Avg. loss: 0.213059\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12.32, NNZs: 1191, Bias: -6.105655, T: 12110, Avg. loss: 0.552233\n",
      "Total training time: 0.93 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12.62, NNZs: 1173, Bias: -9.110316, T: 12110, Avg. loss: 0.647169\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.85, NNZs: 929, Bias: -11.987332, T: 12110, Avg. loss: 0.529226\n",
      "Total training time: 1.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 15.55, NNZs: 1718, Bias: -4.134602, T: 12110, Avg. loss: 0.945691\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.21, NNZs: 347, Bias: -10.902256, T: 24220, Avg. loss: 0.061363\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.92, NNZs: 275, Bias: -5.020325, T: 24220, Avg. loss: 0.036616\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.10, NNZs: 391, Bias: -10.396791, T: 24220, Avg. loss: 0.054536\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.22, NNZs: 617, Bias: -7.761399, T: 24220, Avg. loss: 0.081956\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.10, NNZs: 314, Bias: -8.013728, T: 24220, Avg. loss: 0.050623\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.52, NNZs: 338, Bias: -8.535637, T: 24220, Avg. loss: 0.069622\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.17, NNZs: 735, Bias: -7.529079, T: 24220, Avg. loss: 0.127207\n",
      "Total training time: 1.61 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.92, NNZs: 734, Bias: -7.114929, T: 24220, Avg. loss: 0.125394\n",
      "Total training time: 1.65 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.97, NNZs: 941, Bias: -4.200166, T: 24220, Avg. loss: 0.119163\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.16, NNZs: 984, Bias: -7.062558, T: 24220, Avg. loss: 0.160437\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.73, NNZs: 340, Bias: -10.189606, T: 36330, Avg. loss: 0.049192\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.14, NNZs: 1452, Bias: -2.704386, T: 24220, Avg. loss: 0.165224\n",
      "Total training time: 2.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.06, NNZs: 809, Bias: -9.896048, T: 24220, Avg. loss: 0.170076\n",
      "Total training time: 2.10 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.14, NNZs: 279, Bias: -4.539162, T: 36330, Avg. loss: 0.029261\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.14, NNZs: 595, Bias: -6.940521, T: 36330, Avg. loss: 0.062857\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.39, NNZs: 391, Bias: -9.676242, T: 36330, Avg. loss: 0.042158\n",
      "Total training time: 2.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.47, NNZs: 299, Bias: -7.376392, T: 36330, Avg. loss: 0.041645\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.75, NNZs: 348, Bias: -7.894614, T: 36330, Avg. loss: 0.055266\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.02, NNZs: 735, Bias: -6.477459, T: 36330, Avg. loss: 0.100364\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.70, NNZs: 721, Bias: -6.169080, T: 36330, Avg. loss: 0.096966\n",
      "Total training time: 2.39 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.57, NNZs: 888, Bias: -3.282205, T: 36330, Avg. loss: 0.090301\n",
      "Total training time: 2.49 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.74, NNZs: 962, Bias: -5.918567, T: 36330, Avg. loss: 0.125916\n",
      "Total training time: 2.52 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.36, NNZs: 340, Bias: -9.751776, T: 48440, Avg. loss: 0.042116\n",
      "Total training time: 2.65 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.98, NNZs: 384, Bias: -9.191598, T: 48440, Avg. loss: 0.037212\n",
      "Total training time: 2.80 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.11, NNZs: 310, Bias: -6.943856, T: 48440, Avg. loss: 0.037454\n",
      "Total training time: 2.87 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.75, NNZs: 277, Bias: -4.212367, T: 48440, Avg. loss: 0.025234\n",
      "Total training time: 2.90 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.37, NNZs: 359, Bias: -7.419593, T: 48440, Avg. loss: 0.048015\n",
      "Total training time: 2.93 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.55, NNZs: 590, Bias: -6.377559, T: 48440, Avg. loss: 0.054572\n",
      "Total training time: 2.94 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.35, NNZs: 1420, Bias: -2.139501, T: 36330, Avg. loss: 0.127120\n",
      "Total training time: 3.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.04, NNZs: 743, Bias: -5.512726, T: 48440, Avg. loss: 0.083693\n",
      "Total training time: 3.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.29, NNZs: 727, Bias: -5.862674, T: 48440, Avg. loss: 0.085906\n",
      "Total training time: 3.13 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.94, NNZs: 818, Bias: -8.726235, T: 36330, Avg. loss: 0.133680\n",
      "Total training time: 3.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.92, NNZs: 966, Bias: -5.198220, T: 48440, Avg. loss: 0.108207\n",
      "Total training time: 3.21 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.78, NNZs: 895, Bias: -2.835063, T: 48440, Avg. loss: 0.080649\n",
      "Total training time: 3.27 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.20, NNZs: 343, Bias: -9.385300, T: 60550, Avg. loss: 0.038489\n",
      "Total training time: 3.29 seconds.\n",
      "Norm: 4.72, NNZs: 381, Bias: -8.817465, T: 60550, Avg. loss: 0.034453\n",
      "Total training time: 3.44 seconds.\n",
      "Norm: 3.84, NNZs: 314, Bias: -6.646963, T: 60550, Avg. loss: 0.034381\n",
      "Total training time: 3.53 seconds.\n",
      "Norm: 3.48, NNZs: 287, Bias: -3.990057, T: 60550, Avg. loss: 0.023510\n",
      "Total training time: 3.56 seconds.\n",
      "Norm: 5.15, NNZs: 597, Bias: -5.979880, T: 60550, Avg. loss: 0.049848\n",
      "Total training time: 3.60 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    3.4s remaining:   17.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 4.07, NNZs: 360, Bias: -7.106400, T: 60550, Avg. loss: 0.043651\n",
      "Total training time: 3.64 seconds.\n",
      "Norm: 5.57, NNZs: 761, Bias: -5.053328, T: 60550, Avg. loss: 0.076020\n",
      "Total training time: 3.70 seconds.\n",
      "Norm: 6.37, NNZs: 981, Bias: -4.706907, T: 60550, Avg. loss: 0.098154\n",
      "Total training time: 3.82 seconds.\n",
      "Norm: 6.32, NNZs: 845, Bias: -7.936461, T: 48440, Avg. loss: 0.114901\n",
      "Total training time: 3.82 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.24, NNZs: 912, Bias: -2.534809, T: 60550, Avg. loss: 0.074615\n",
      "Total training time: 3.84 seconds.\n",
      "Norm: 5.86, NNZs: 735, Bias: -5.340877, T: 60550, Avg. loss: 0.078674\n",
      "Total training time: 3.84 seconds.\n",
      "Norm: 8.36, NNZs: 1439, Bias: -1.800549, T: 48440, Avg. loss: 0.113791\n",
      "Total training time: 3.87 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.89, NNZs: 862, Bias: -7.358911, T: 60550, Avg. loss: 0.103136\n",
      "Total training time: 4.27 seconds.\n",
      "Norm: 7.72, NNZs: 1469, Bias: -1.609742, T: 60550, Avg. loss: 0.104907\n",
      "Total training time: 4.40 seconds.\n",
      "Accuracy on validation set:  0.7885208012326657\n",
      "--------------------\n",
      "Classifier with l1_ratio: 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 7.38, NNZs: 200, Bias: -5.283129, T: 12110, Avg. loss: 0.091027\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.26, NNZs: 248, Bias: -11.836871, T: 12110, Avg. loss: 0.185848\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8.04, NNZs: 273, Bias: -9.096803, T: 12110, Avg. loss: 0.212072\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.13, NNZs: 202, Bias: -9.208437, T: 12110, Avg. loss: 0.152176\n",
      "Total training time: 0.74 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12.18, NNZs: 606, Bias: -8.538247, T: 12110, Avg. loss: 0.414498\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.62, NNZs: 537, Bias: -8.333596, T: 12110, Avg. loss: 0.427255\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.62, NNZs: 564, Bias: -10.748582, T: 12110, Avg. loss: 0.528223\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8.24, NNZs: 263, Bias: -9.744989, T: 12110, Avg. loss: 0.146126\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.38, NNZs: 431, Bias: -8.409751, T: 12110, Avg. loss: 0.270708\n",
      "Total training time: 0.83 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 13.52, NNZs: 742, Bias: -5.338371, T: 12110, Avg. loss: 0.544813\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 16.98, NNZs: 1148, Bias: -3.305784, T: 12110, Avg. loss: 0.920914\n",
      "Total training time: 0.87 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 13.61, NNZs: 752, Bias: -8.596703, T: 12110, Avg. loss: 0.638820\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.57, NNZs: 183, Bias: -10.508617, T: 24220, Avg. loss: 0.069180\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.25, NNZs: 136, Bias: -4.339376, T: 24220, Avg. loss: 0.039729\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.90, NNZs: 185, Bias: -7.839227, T: 24220, Avg. loss: 0.073780\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.18, NNZs: 200, Bias: -8.498137, T: 24220, Avg. loss: 0.055974\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.50, NNZs: 445, Bias: -6.360289, T: 24220, Avg. loss: 0.129994\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.46, NNZs: 165, Bias: -7.898059, T: 24220, Avg. loss: 0.058921\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.56, NNZs: 492, Bias: -8.545833, T: 24220, Avg. loss: 0.173334\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.79, NNZs: 429, Bias: -6.602873, T: 24220, Avg. loss: 0.130586\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.67, NNZs: 330, Bias: -6.833807, T: 24220, Avg. loss: 0.087628\n",
      "Total training time: 1.59 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.79, NNZs: 570, Bias: -3.463592, T: 24220, Avg. loss: 0.119151\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.40, NNZs: 959, Bias: -1.916480, T: 24220, Avg. loss: 0.176393\n",
      "Total training time: 1.69 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.89, NNZs: 577, Bias: -6.359384, T: 24220, Avg. loss: 0.168498\n",
      "Total training time: 1.75 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.39, NNZs: 143, Bias: -3.796713, T: 36330, Avg. loss: 0.033305\n",
      "Total training time: 1.90 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.89, NNZs: 188, Bias: -9.784551, T: 36330, Avg. loss: 0.057534\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.03, NNZs: 186, Bias: -7.128570, T: 36330, Avg. loss: 0.059957\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.33, NNZs: 201, Bias: -7.786874, T: 36330, Avg. loss: 0.046094\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.14, NNZs: 425, Bias: -5.344713, T: 36330, Avg. loss: 0.104804\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.75, NNZs: 161, Bias: -7.203809, T: 36330, Avg. loss: 0.048528\n",
      "Total training time: 2.12 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.27, NNZs: 494, Bias: -7.320966, T: 36330, Avg. loss: 0.137627\n",
      "Total training time: 2.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.54, NNZs: 335, Bias: -5.944653, T: 36330, Avg. loss: 0.070401\n",
      "Total training time: 2.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.49, NNZs: 441, Bias: -5.498979, T: 36330, Avg. loss: 0.107536\n",
      "Total training time: 2.21 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.27, NNZs: 552, Bias: -2.642056, T: 36330, Avg. loss: 0.098665\n",
      "Total training time: 2.41 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.35, NNZs: 569, Bias: -5.208694, T: 36330, Avg. loss: 0.136563\n",
      "Total training time: 2.43 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.92, NNZs: 145, Bias: -3.467575, T: 48440, Avg. loss: 0.030181\n",
      "Total training time: 2.57 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.50, NNZs: 189, Bias: -9.292802, T: 48440, Avg. loss: 0.050790\n",
      "Total training time: 2.59 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.87, NNZs: 204, Bias: -7.290881, T: 48440, Avg. loss: 0.041528\n",
      "Total training time: 2.60 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 10.53, NNZs: 972, Bias: -1.600706, T: 36330, Avg. loss: 0.146522\n",
      "Total training time: 2.61 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.38, NNZs: 423, Bias: -4.660225, T: 48440, Avg. loss: 0.092846\n",
      "Total training time: 2.69 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.54, NNZs: 186, Bias: -6.639513, T: 48440, Avg. loss: 0.053079\n",
      "Total training time: 2.71 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.28, NNZs: 160, Bias: -6.759990, T: 48440, Avg. loss: 0.043570\n",
      "Total training time: 2.80 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.85, NNZs: 332, Bias: -5.397841, T: 48440, Avg. loss: 0.062018\n",
      "Total training time: 2.91 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.66, NNZs: 432, Bias: -4.922322, T: 48440, Avg. loss: 0.094832\n",
      "Total training time: 2.93 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.53, NNZs: 506, Bias: -6.493276, T: 48440, Avg. loss: 0.119929\n",
      "Total training time: 2.96 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.45, NNZs: 576, Bias: -4.477070, T: 48440, Avg. loss: 0.122999\n",
      "Total training time: 3.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.41, NNZs: 569, Bias: -2.233011, T: 48440, Avg. loss: 0.090753\n",
      "Total training time: 3.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.61, NNZs: 149, Bias: -3.226335, T: 60550, Avg. loss: 0.027979\n",
      "Total training time: 3.19 seconds.\n",
      "Norm: 4.55, NNZs: 210, Bias: -6.928115, T: 60550, Avg. loss: 0.038099\n",
      "Total training time: 3.20 seconds.\n",
      "Norm: 4.29, NNZs: 195, Bias: -8.900239, T: 60550, Avg. loss: 0.046545\n",
      "Total training time: 3.25 seconds.\n",
      "Norm: 5.85, NNZs: 438, Bias: -4.179980, T: 60550, Avg. loss: 0.085554\n",
      "Total training time: 3.30 seconds.\n",
      "Norm: 3.99, NNZs: 159, Bias: -6.421972, T: 60550, Avg. loss: 0.040793\n",
      "Total training time: 3.37 seconds.\n",
      "Norm: 4.20, NNZs: 193, Bias: -6.281780, T: 60550, Avg. loss: 0.049648\n",
      "Total training time: 3.38 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    3.2s remaining:   16.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 5.39, NNZs: 330, Bias: -4.996549, T: 60550, Avg. loss: 0.058072\n",
      "Total training time: 3.46 seconds.\n",
      "Norm: 9.42, NNZs: 997, Bias: -1.358180, T: 48440, Avg. loss: 0.133882\n",
      "Total training time: 3.46 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.00, NNZs: 508, Bias: -5.907215, T: 60550, Avg. loss: 0.108973\n",
      "Total training time: 3.55 seconds.\n",
      "Norm: 6.16, NNZs: 452, Bias: -4.432819, T: 60550, Avg. loss: 0.088480\n",
      "Total training time: 3.61 seconds.\n",
      "Norm: 6.83, NNZs: 602, Bias: -3.982850, T: 60550, Avg. loss: 0.112802\n",
      "Total training time: 3.71 seconds.\n",
      "Norm: 6.81, NNZs: 579, Bias: -2.012948, T: 60550, Avg. loss: 0.085823\n",
      "Total training time: 3.71 seconds.\n",
      "Norm: 8.67, NNZs: 1062, Bias: -1.279379, T: 60550, Avg. loss: 0.127080\n",
      "Total training time: 3.99 seconds.\n",
      "Accuracy on validation set:  0.788135593220339\n",
      "--------------------\n",
      "Classifier with l1_ratio: 0.2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 7.90, NNZs: 108, Bias: -4.931381, T: 12110, Avg. loss: 0.088889\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12.50, NNZs: 366, Bias: -10.162342, T: 12110, Avg. loss: 0.527406\n",
      "Total training time: 0.66 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8.82, NNZs: 181, Bias: -8.557299, T: 12110, Avg. loss: 0.204441\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8.00, NNZs: 164, Bias: -11.572842, T: 12110, Avg. loss: 0.188438\n",
      "Total training time: 0.71 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12.58, NNZs: 365, Bias: -7.171086, T: 12110, Avg. loss: 0.413899\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.00, NNZs: 186, Bias: -10.000187, T: 12110, Avg. loss: 0.158976\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.85, NNZs: 123, Bias: -9.022725, T: 12110, Avg. loss: 0.154568\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 14.59, NNZs: 486, Bias: -7.869310, T: 12110, Avg. loss: 0.611875\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.54, NNZs: 298, Bias: -9.870990, T: 12110, Avg. loss: 0.310997\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 13.29, NNZs: 440, Bias: -8.113810, T: 12110, Avg. loss: 0.411441\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.79, NNZs: 802, Bias: -2.813266, T: 12110, Avg. loss: 0.932330\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 14.57, NNZs: 518, Bias: -4.898446, T: 12110, Avg. loss: 0.534027\n",
      "Total training time: 0.95 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.66, NNZs: 84, Bias: -3.976080, T: 24220, Avg. loss: 0.042404\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.39, NNZs: 118, Bias: -7.234250, T: 24220, Avg. loss: 0.071478\n",
      "Total training time: 1.28 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.10, NNZs: 285, Bias: -5.191096, T: 24220, Avg. loss: 0.125383\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.95, NNZs: 121, Bias: -10.228739, T: 24220, Avg. loss: 0.073102\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.92, NNZs: 104, Bias: -7.627018, T: 24220, Avg. loss: 0.063031\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.45, NNZs: 233, Bias: -8.021752, T: 24220, Avg. loss: 0.107705\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.13, NNZs: 318, Bias: -7.759385, T: 24220, Avg. loss: 0.181003\n",
      "Total training time: 1.44 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.70, NNZs: 124, Bias: -8.662018, T: 24220, Avg. loss: 0.063312\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.71, NNZs: 386, Bias: -5.586606, T: 24220, Avg. loss: 0.173415\n",
      "Total training time: 1.50 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.62, NNZs: 316, Bias: -6.164407, T: 24220, Avg. loss: 0.132775\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 13.92, NNZs: 662, Bias: -1.494657, T: 24220, Avg. loss: 0.184551\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.70, NNZs: 390, Bias: -3.065225, T: 24220, Avg. loss: 0.123018\n",
      "Total training time: 1.67 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.75, NNZs: 86, Bias: -3.418637, T: 36330, Avg. loss: 0.035875\n",
      "Total training time: 1.87 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.66, NNZs: 281, Bias: -4.174159, T: 36330, Avg. loss: 0.103981\n",
      "Total training time: 1.98 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.17, NNZs: 124, Bias: -9.461347, T: 36330, Avg. loss: 0.063640\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.21, NNZs: 226, Bias: -6.976634, T: 36330, Avg. loss: 0.086141\n",
      "Total training time: 2.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.09, NNZs: 109, Bias: -6.886090, T: 36330, Avg. loss: 0.052510\n",
      "Total training time: 2.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.38, NNZs: 113, Bias: -6.487717, T: 36330, Avg. loss: 0.061049\n",
      "Total training time: 2.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.75, NNZs: 125, Bias: -7.911493, T: 36330, Avg. loss: 0.052977\n",
      "Total training time: 2.14 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.65, NNZs: 312, Bias: -6.456091, T: 36330, Avg. loss: 0.144700\n",
      "Total training time: 2.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.04, NNZs: 378, Bias: -4.436714, T: 36330, Avg. loss: 0.139118\n",
      "Total training time: 2.24 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.13, NNZs: 324, Bias: -5.088001, T: 36330, Avg. loss: 0.111270\n",
      "Total training time: 2.25 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.85, NNZs: 690, Bias: -1.280953, T: 36330, Avg. loss: 0.159118\n",
      "Total training time: 2.30 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.06, NNZs: 389, Bias: -2.255607, T: 36330, Avg. loss: 0.104761\n",
      "Total training time: 2.38 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.72, NNZs: 121, Bias: -8.932660, T: 48440, Avg. loss: 0.056901\n",
      "Total training time: 2.58 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.78, NNZs: 276, Bias: -3.551097, T: 48440, Avg. loss: 0.093913\n",
      "Total training time: 2.60 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.20, NNZs: 81, Bias: -3.079233, T: 48440, Avg. loss: 0.032218\n",
      "Total training time: 2.63 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.45, NNZs: 234, Bias: -6.320848, T: 48440, Avg. loss: 0.076067\n",
      "Total training time: 2.65 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.81, NNZs: 122, Bias: -5.972687, T: 48440, Avg. loss: 0.055167\n",
      "Total training time: 2.74 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.55, NNZs: 108, Bias: -6.420938, T: 48440, Avg. loss: 0.047985\n",
      "Total training time: 2.76 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.82, NNZs: 321, Bias: -5.556308, T: 48440, Avg. loss: 0.127427\n",
      "Total training time: 2.86 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.23, NNZs: 128, Bias: -7.384298, T: 48440, Avg. loss: 0.047732\n",
      "Total training time: 2.88 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.20, NNZs: 311, Bias: -4.474506, T: 48440, Avg. loss: 0.099893\n",
      "Total training time: 2.91 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.04, NNZs: 372, Bias: -3.756637, T: 48440, Avg. loss: 0.124937\n",
      "Total training time: 2.96 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.13, NNZs: 394, Bias: -1.909475, T: 48440, Avg. loss: 0.097926\n",
      "Total training time: 3.12 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.42, NNZs: 122, Bias: -8.537001, T: 60550, Avg. loss: 0.052448\n",
      "Total training time: 3.16 seconds.\n",
      "Norm: 6.19, NNZs: 288, Bias: -3.102098, T: 60550, Avg. loss: 0.087941\n",
      "Total training time: 3.23 seconds.\n",
      "Norm: 10.58, NNZs: 735, Bias: -1.160005, T: 48440, Avg. loss: 0.150474\n",
      "Total training time: 3.29 seconds.\n",
      "-- Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    3.2s remaining:   16.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 5.94, NNZs: 241, Bias: -5.828813, T: 60550, Avg. loss: 0.071323\n",
      "Total training time: 3.33 seconds.\n",
      "Norm: 3.83, NNZs: 84, Bias: -2.830824, T: 60550, Avg. loss: 0.030692\n",
      "Total training time: 3.35 seconds.\n",
      "Norm: 4.18, NNZs: 109, Bias: -6.079000, T: 60550, Avg. loss: 0.044438\n",
      "Total training time: 3.41 seconds.\n",
      "Norm: 4.42, NNZs: 122, Bias: -5.585662, T: 60550, Avg. loss: 0.051873\n",
      "Total training time: 3.42 seconds.\n",
      "Norm: 6.61, NNZs: 310, Bias: -3.981646, T: 60550, Avg. loss: 0.094585\n",
      "Total training time: 3.52 seconds.\n",
      "Norm: 4.87, NNZs: 125, Bias: -6.999300, T: 60550, Avg. loss: 0.044701\n",
      "Total training time: 3.54 seconds.\n",
      "Norm: 6.21, NNZs: 318, Bias: -4.935839, T: 60550, Avg. loss: 0.116398\n",
      "Total training time: 3.53 seconds.\n",
      "Norm: 7.36, NNZs: 380, Bias: -3.322631, T: 60550, Avg. loss: 0.117864\n",
      "Total training time: 3.55 seconds.\n",
      "Norm: 7.47, NNZs: 410, Bias: -1.755625, T: 60550, Avg. loss: 0.093823\n",
      "Total training time: 3.64 seconds.\n",
      "Norm: 9.71, NNZs: 770, Bias: -1.088156, T: 60550, Avg. loss: 0.143144\n",
      "Total training time: 3.84 seconds.\n",
      "Accuracy on validation set:  0.7862095531587057\n",
      "--------------------\n",
      "Classifier with l1_ratio: 0.3\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 10.96, NNZs: 94, Bias: -10.427398, T: 12110, Avg. loss: 0.166016\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.74, NNZs: 68, Bias: -8.725557, T: 12110, Avg. loss: 0.156708\n",
      "Total training time: 0.65 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.73, NNZs: 107, Bias: -7.839251, T: 12110, Avg. loss: 0.205779\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.65, NNZs: 49, Bias: -4.419250, T: 12110, Avg. loss: 0.088529\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 17.86, NNZs: 294, Bias: -7.313493, T: 12110, Avg. loss: 0.627534\n",
      "Total training time: 0.72 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 15.07, NNZs: 214, Bias: -9.223812, T: 12110, Avg. loss: 0.524601\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 14.04, NNZs: 159, Bias: -9.156355, T: 12110, Avg. loss: 0.311685\n",
      "Total training time: 0.73 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 14.95, NNZs: 203, Bias: -5.825195, T: 12110, Avg. loss: 0.407088\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.15, NNZs: 326, Bias: -4.595430, T: 12110, Avg. loss: 0.560091\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 16.00, NNZs: 250, Bias: -7.366942, T: 12110, Avg. loss: 0.422514\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.80, NNZs: 102, Bias: -11.473758, T: 12110, Avg. loss: 0.201669\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 22.91, NNZs: 508, Bias: -2.291803, T: 12110, Avg. loss: 0.958905\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.93, NNZs: 34, Bias: -3.465453, T: 24220, Avg. loss: 0.042663\n",
      "Total training time: 1.27 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.68, NNZs: 73, Bias: -6.446845, T: 24220, Avg. loss: 0.074060\n",
      "Total training time: 1.34 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.12, NNZs: 53, Bias: -8.961964, T: 24220, Avg. loss: 0.072762\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.17, NNZs: 52, Bias: -7.298452, T: 24220, Avg. loss: 0.065607\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.23, NNZs: 128, Bias: -7.210964, T: 24220, Avg. loss: 0.116418\n",
      "Total training time: 1.33 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.91, NNZs: 169, Bias: -6.603829, T: 24220, Avg. loss: 0.184745\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 13.30, NNZs: 230, Bias: -2.568388, T: 24220, Avg. loss: 0.129718\n",
      "Total training time: 1.40 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.89, NNZs: 162, Bias: -3.728954, T: 24220, Avg. loss: 0.123370\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 11.63, NNZs: 181, Bias: -5.215603, T: 24220, Avg. loss: 0.138281\n",
      "Total training time: 1.38 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 13.01, NNZs: 208, Bias: -4.773940, T: 24220, Avg. loss: 0.177349\n",
      "Total training time: 1.54 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.08, NNZs: 48, Bias: -10.064232, T: 24220, Avg. loss: 0.084504\n",
      "Total training time: 1.55 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 17.20, NNZs: 413, Bias: -1.257000, T: 24220, Avg. loss: 0.197509\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.89, NNZs: 52, Bias: -8.120091, T: 36330, Avg. loss: 0.064405\n",
      "Total training time: 1.92 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.60, NNZs: 127, Bias: -6.111744, T: 36330, Avg. loss: 0.095000\n",
      "Total training time: 1.91 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.42, NNZs: 74, Bias: -5.613716, T: 36330, Avg. loss: 0.062380\n",
      "Total training time: 1.96 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.76, NNZs: 34, Bias: -2.895049, T: 36330, Avg. loss: 0.036893\n",
      "Total training time: 1.96 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.08, NNZs: 158, Bias: -5.126000, T: 36330, Avg. loss: 0.147202\n",
      "Total training time: 1.98 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.09, NNZs: 49, Bias: -6.503427, T: 36330, Avg. loss: 0.056254\n",
      "Total training time: 1.98 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.13, NNZs: 146, Bias: -2.861825, T: 36330, Avg. loss: 0.102729\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.22, NNZs: 225, Bias: -1.877443, T: 36330, Avg. loss: 0.112100\n",
      "Total training time: 2.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.00, NNZs: 51, Bias: -9.250066, T: 36330, Avg. loss: 0.073458\n",
      "Total training time: 2.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.77, NNZs: 186, Bias: -4.113112, T: 36330, Avg. loss: 0.113386\n",
      "Total training time: 2.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.65, NNZs: 445, Bias: -1.157744, T: 36330, Avg. loss: 0.181140\n",
      "Total training time: 2.29 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 10.92, NNZs: 199, Bias: -3.609976, T: 36330, Avg. loss: 0.145160\n",
      "Total training time: 2.50 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.15, NNZs: 49, Bias: -7.561891, T: 48440, Avg. loss: 0.059728\n",
      "Total training time: 2.51 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.62, NNZs: 126, Bias: -5.407705, T: 48440, Avg. loss: 0.084458\n",
      "Total training time: 2.51 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.69, NNZs: 77, Bias: -5.050106, T: 48440, Avg. loss: 0.055164\n",
      "Total training time: 2.54 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.97, NNZs: 155, Bias: -4.154893, T: 48440, Avg. loss: 0.127690\n",
      "Total training time: 2.61 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.07, NNZs: 34, Bias: -2.572260, T: 48440, Avg. loss: 0.034459\n",
      "Total training time: 2.61 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.41, NNZs: 51, Bias: -5.999526, T: 48440, Avg. loss: 0.052233\n",
      "Total training time: 2.62 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.07, NNZs: 141, Bias: -2.315422, T: 48440, Avg. loss: 0.094693\n",
      "Total training time: 2.72 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.38, NNZs: 52, Bias: -8.690469, T: 48440, Avg. loss: 0.066557\n",
      "Total training time: 2.76 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 10.03, NNZs: 232, Bias: -1.652878, T: 48440, Avg. loss: 0.109262\n",
      "Total training time: 2.84 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.64, NNZs: 181, Bias: -3.504648, T: 48440, Avg. loss: 0.103966\n",
      "Total training time: 2.94 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 13.07, NNZs: 469, Bias: -1.088132, T: 48440, Avg. loss: 0.172073\n",
      "Total training time: 2.94 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.65, NNZs: 48, Bias: -7.146375, T: 60550, Avg. loss: 0.056144\n",
      "Total training time: 3.10 seconds.\n",
      "Norm: 5.18, NNZs: 77, Bias: -4.631214, T: 60550, Avg. loss: 0.052175\n",
      "Total training time: 3.13 seconds.\n",
      "Norm: 6.95, NNZs: 123, Bias: -4.925196, T: 60550, Avg. loss: 0.079181\n",
      "Total training time: 3.10 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    3.1s remaining:   15.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 7.20, NNZs: 158, Bias: -3.479807, T: 60550, Avg. loss: 0.115594\n",
      "Total training time: 3.23 seconds.\n",
      "Norm: 9.69, NNZs: 197, Bias: -2.957169, T: 48440, Avg. loss: 0.134889\n",
      "Total training time: 3.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.62, NNZs: 35, Bias: -2.347445, T: 60550, Avg. loss: 0.032800\n",
      "Total training time: 3.27 seconds.\n",
      "Norm: 5.00, NNZs: 50, Bias: -8.250299, T: 60550, Avg. loss: 0.062087\n",
      "Total training time: 3.28 seconds.\n",
      "Norm: 4.94, NNZs: 49, Bias: -5.623671, T: 60550, Avg. loss: 0.049150\n",
      "Total training time: 3.30 seconds.\n",
      "Norm: 7.34, NNZs: 143, Bias: -1.954186, T: 60550, Avg. loss: 0.091017\n",
      "Total training time: 3.36 seconds.\n",
      "Norm: 9.21, NNZs: 244, Bias: -1.567817, T: 60550, Avg. loss: 0.105779\n",
      "Total training time: 3.42 seconds.\n",
      "Norm: 7.90, NNZs: 179, Bias: -3.057358, T: 60550, Avg. loss: 0.100455\n",
      "Total training time: 3.44 seconds.\n",
      "Norm: 11.97, NNZs: 504, Bias: -1.039397, T: 60550, Avg. loss: 0.167549\n",
      "Total training time: 3.52 seconds.\n",
      "Norm: 8.84, NNZs: 196, Bias: -2.600002, T: 60550, Avg. loss: 0.129029\n",
      "Total training time: 3.70 seconds.\n",
      "Accuracy on validation set:  0.7827426810477658\n",
      "--------------------\n",
      "Classifier with l1_ratio: 0.5\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 18.59, NNZs: 68, Bias: -7.454520, T: 12110, Avg. loss: 0.227045\n",
      "Total training time: 0.60 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 19.96, NNZs: 85, Bias: -11.771830, T: 12110, Avg. loss: 0.194392\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 17.38, NNZs: 58, Bias: -9.567043, T: 12110, Avg. loss: 0.183518\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 23.88, NNZs: 125, Bias: -9.948247, T: 12110, Avg. loss: 0.381338\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.82, NNZs: 97, Bias: -12.594072, T: 12110, Avg. loss: 0.254003\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 26.57, NNZs: 169, Bias: -7.092605, T: 12110, Avg. loss: 0.491439\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 16.23, NNZs: 35, Bias: -4.376361, T: 12110, Avg. loss: 0.091771\n",
      "Total training time: 0.75 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 38.06, NNZs: 370, Bias: -1.959949, T: 12110, Avg. loss: 1.128990\n",
      "Total training time: 0.78 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 25.00, NNZs: 141, Bias: -4.786637, T: 12110, Avg. loss: 0.454019\n",
      "Total training time: 0.81 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 25.73, NNZs: 145, Bias: -9.037732, T: 12110, Avg. loss: 0.610247\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 31.04, NNZs: 225, Bias: -8.919566, T: 12110, Avg. loss: 0.756298\n",
      "Total training time: 0.82 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 31.71, NNZs: 260, Bias: -4.764256, T: 12110, Avg. loss: 0.671767\n",
      "Total training time: 0.85 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 13.82, NNZs: 33, Bias: -5.847938, T: 24220, Avg. loss: 0.079255\n",
      "Total training time: 1.22 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 14.96, NNZs: 34, Bias: -10.223280, T: 24220, Avg. loss: 0.086592\n",
      "Total training time: 1.23 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 17.89, NNZs: 65, Bias: -7.861177, T: 24220, Avg. loss: 0.142188\n",
      "Total training time: 1.25 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 13.84, NNZs: 39, Bias: -11.055141, T: 24220, Avg. loss: 0.114785\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 13.06, NNZs: 35, Bias: -8.007753, T: 24220, Avg. loss: 0.075572\n",
      "Total training time: 1.30 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 19.95, NNZs: 94, Bias: -4.466988, T: 24220, Avg. loss: 0.155938\n",
      "Total training time: 1.37 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.11, NNZs: 16, Bias: -3.263864, T: 24220, Avg. loss: 0.045914\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 23.17, NNZs: 141, Bias: -5.710522, T: 24220, Avg. loss: 0.219405\n",
      "Total training time: 1.42 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 18.76, NNZs: 75, Bias: -2.343656, T: 24220, Avg. loss: 0.121547\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 19.17, NNZs: 79, Bias: -5.980779, T: 24220, Avg. loss: 0.200179\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 23.70, NNZs: 142, Bias: -2.381494, T: 24220, Avg. loss: 0.147202\n",
      "Total training time: 1.48 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 29.18, NNZs: 255, Bias: -1.001355, T: 24220, Avg. loss: 0.223385\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 15.15, NNZs: 55, Bias: -6.642475, T: 36330, Avg. loss: 0.121867\n",
      "Total training time: 1.83 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.62, NNZs: 24, Bias: -10.155795, T: 36330, Avg. loss: 0.101228\n",
      "Total training time: 1.93 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 12.68, NNZs: 25, Bias: -9.343720, T: 36330, Avg. loss: 0.077648\n",
      "Total training time: 1.94 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.09, NNZs: 32, Bias: -7.125320, T: 36330, Avg. loss: 0.066092\n",
      "Total training time: 1.96 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.63, NNZs: 35, Bias: -4.951046, T: 36330, Avg. loss: 0.067676\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.92, NNZs: 90, Bias: -3.149380, T: 36330, Avg. loss: 0.126417\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 10.20, NNZs: 13, Bias: -2.639143, T: 36330, Avg. loss: 0.039106\n",
      "Total training time: 2.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 19.56, NNZs: 123, Bias: -3.935173, T: 36330, Avg. loss: 0.171640\n",
      "Total training time: 2.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.13, NNZs: 76, Bias: -4.230099, T: 36330, Avg. loss: 0.153234\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 15.91, NNZs: 50, Bias: -1.487143, T: 36330, Avg. loss: 0.099805\n",
      "Total training time: 2.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 20.14, NNZs: 125, Bias: -1.637103, T: 36330, Avg. loss: 0.128239\n",
      "Total training time: 2.15 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 25.05, NNZs: 251, Bias: -1.037195, T: 36330, Avg. loss: 0.204642\n",
      "Total training time: 2.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 13.46, NNZs: 53, Bias: -5.840024, T: 48440, Avg. loss: 0.111030\n",
      "Total training time: 2.40 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11.31, NNZs: 22, Bias: -8.735849, T: 48440, Avg. loss: 0.074563\n",
      "Total training time: 2.54 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 10.28, NNZs: 21, Bias: -9.550971, T: 48440, Avg. loss: 0.090353\n",
      "Total training time: 2.54 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.86, NNZs: 29, Bias: -6.565253, T: 48440, Avg. loss: 0.060821\n",
      "Total training time: 2.64 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 15.07, NNZs: 86, Bias: -2.531452, T: 48440, Avg. loss: 0.112211\n",
      "Total training time: 2.63 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 17.36, NNZs: 109, Bias: -2.904539, T: 48440, Avg. loss: 0.149212\n",
      "Total training time: 2.64 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14.16, NNZs: 35, Bias: -1.153039, T: 48440, Avg. loss: 0.093238\n",
      "Total training time: 2.75 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14.26, NNZs: 71, Bias: -3.077176, T: 48440, Avg. loss: 0.127866\n",
      "Total training time: 2.77 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 10.30, NNZs: 34, Bias: -4.310721, T: 48440, Avg. loss: 0.060404\n",
      "Total training time: 2.81 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 18.00, NNZs: 119, Bias: -1.451298, T: 48440, Avg. loss: 0.124607\n",
      "Total training time: 2.83 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.05, NNZs: 13, Bias: -2.294043, T: 48440, Avg. loss: 0.036457\n",
      "Total training time: 2.86 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 22.45, NNZs: 260, Bias: -1.061434, T: 48440, Avg. loss: 0.199631\n",
      "Total training time: 2.97 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.28, NNZs: 50, Bias: -5.236042, T: 60550, Avg. loss: 0.103460\n",
      "Total training time: 2.99 seconds.\n",
      "Norm: 10.34, NNZs: 19, Bias: -8.293054, T: 60550, Avg. loss: 0.070099\n",
      "Total training time: 3.11 seconds.\n",
      "Norm: 9.38, NNZs: 20, Bias: -9.059506, T: 60550, Avg. loss: 0.084462\n",
      "Total training time: 3.13 seconds.\n",
      "Norm: 15.84, NNZs: 104, Bias: -2.312594, T: 60550, Avg. loss: 0.139006\n",
      "Total training time: 3.24 seconds.\n",
      "Norm: 9.01, NNZs: 28, Bias: -6.138100, T: 60550, Avg. loss: 0.057377\n",
      "Total training time: 3.29 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    3.1s remaining:   15.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 13.79, NNZs: 81, Bias: -2.138294, T: 60550, Avg. loss: 0.108950\n",
      "Total training time: 3.30 seconds.\n",
      "Norm: 12.93, NNZs: 21, Bias: -1.019762, T: 60550, Avg. loss: 0.090680\n",
      "Total training time: 3.34 seconds.\n",
      "Norm: 9.37, NNZs: 34, Bias: -3.849036, T: 60550, Avg. loss: 0.056166\n",
      "Total training time: 3.37 seconds.\n",
      "Norm: 12.95, NNZs: 67, Bias: -2.221234, T: 60550, Avg. loss: 0.111111\n",
      "Total training time: 3.37 seconds.\n",
      "Norm: 8.25, NNZs: 14, Bias: -2.054599, T: 60550, Avg. loss: 0.035591\n",
      "Total training time: 3.40 seconds.\n",
      "Norm: 16.49, NNZs: 120, Bias: -1.363959, T: 60550, Avg. loss: 0.121310\n",
      "Total training time: 3.43 seconds.\n",
      "Norm: 20.63, NNZs: 281, Bias: -1.033064, T: 60550, Avg. loss: 0.196690\n",
      "Total training time: 3.52 seconds.\n",
      "Accuracy on validation set:  0.7580893682588598\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    3.5s finished\n"
     ]
    }
   ],
   "source": [
    "# tune hyper-parameter l1_ratio\n",
    "hyper_param_values = [0.05, 0.10, 0.15, 0.20, 0.30, 0.50]\n",
    "accuracies = []\n",
    "\n",
    "for hyper_param_value in hyper_param_values:\n",
    "    \n",
    "    print(f\"Classifier with l1_ratio: {hyper_param_value}\")\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = SGDClassifier(loss='hinge', penalty='elasticnet', l1_ratio=hyper_param_value, alpha=5e-3, random_state=42, max_iter=5, tol=None, n_jobs=-1, verbose=1)\n",
    "    clf.fit(bm25_matrix_train, y_train)\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    y_pred = clf.predict(bm25_matrix_val)\n",
    "    acc_score = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(acc_score)\n",
    "    print('Accuracy on validation set: ', acc_score)\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studied hyper-parameters and the corresponding accuracies on the validation set: \n",
      "l1_ratio: 0.05, Accuracy: 0.7847\n",
      "l1_ratio: 0.10, Accuracy: 0.7885\n",
      "l1_ratio: 0.15, Accuracy: 0.7881\n",
      "l1_ratio: 0.20, Accuracy: 0.7862\n",
      "l1_ratio: 0.30, Accuracy: 0.7827\n",
      "l1_ratio: 0.50, Accuracy: 0.7581\n"
     ]
    }
   ],
   "source": [
    "print(\"Studied hyper-parameters and the corresponding accuracies on the validation set: \")\n",
    "_ = [print(f\"l1_ratio: {param:.2f}, Accuracy: {acc:.4f}\") for param, acc in zip(hyper_param_values, accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected l1_ratio: 0.1\n"
     ]
    }
   ],
   "source": [
    "best_l1_ratio = hyper_param_values[np.argmax(accuracies)]\n",
    "print(f\"Selected l1_ratio: {best_l1_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 6.63, NNZs: 391, Bias: -9.206153, T: 12110, Avg. loss: 0.146311\n",
      "Total training time: 0.67 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.74, NNZs: 447, Bias: -12.092421, T: 12110, Avg. loss: 0.184069\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.90, NNZs: 488, Bias: -11.652807, T: 12110, Avg. loss: 0.153998\n",
      "Total training time: 0.68 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.50, NNZs: 480, Bias: -9.710926, T: 12110, Avg. loss: 0.213059\n",
      "Total training time: 0.69 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.86, NNZs: 420, Bias: -5.921574, T: 12110, Avg. loss: 0.097169\n",
      "Total training time: 0.76 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.85, NNZs: 865, Bias: -8.867608, T: 12110, Avg. loss: 0.443645\n",
      "Total training time: 0.77 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.81, NNZs: 753, Bias: -9.340657, T: 12110, Avg. loss: 0.271516\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12.32, NNZs: 1191, Bias: -6.105655, T: 12110, Avg. loss: 0.552233\n",
      "Total training time: 0.88 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.85, NNZs: 929, Bias: -11.987332, T: 12110, Avg. loss: 0.529226\n",
      "Total training time: 0.89 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.25, NNZs: 958, Bias: -9.334163, T: 12110, Avg. loss: 0.414670\n",
      "Total training time: 0.91 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12.62, NNZs: 1173, Bias: -9.110316, T: 12110, Avg. loss: 0.647169\n",
      "Total training time: 1.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 15.55, NNZs: 1718, Bias: -4.134602, T: 12110, Avg. loss: 0.945691\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.10, NNZs: 314, Bias: -8.013728, T: 24220, Avg. loss: 0.050623\n",
      "Total training time: 1.29 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.52, NNZs: 338, Bias: -8.535637, T: 24220, Avg. loss: 0.069622\n",
      "Total training time: 1.32 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.21, NNZs: 347, Bias: -10.902256, T: 24220, Avg. loss: 0.061363\n",
      "Total training time: 1.39 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.92, NNZs: 275, Bias: -5.020325, T: 24220, Avg. loss: 0.036616\n",
      "Total training time: 1.45 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.10, NNZs: 391, Bias: -10.396791, T: 24220, Avg. loss: 0.054536\n",
      "Total training time: 1.46 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.92, NNZs: 734, Bias: -7.114929, T: 24220, Avg. loss: 0.125394\n",
      "Total training time: 1.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.17, NNZs: 735, Bias: -7.529079, T: 24220, Avg. loss: 0.127207\n",
      "Total training time: 1.60 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.22, NNZs: 617, Bias: -7.761399, T: 24220, Avg. loss: 0.081956\n",
      "Total training time: 1.78 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.16, NNZs: 984, Bias: -7.062558, T: 24220, Avg. loss: 0.160437\n",
      "Total training time: 1.85 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.47, NNZs: 299, Bias: -7.376392, T: 36330, Avg. loss: 0.041645\n",
      "Total training time: 1.89 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.97, NNZs: 941, Bias: -4.200166, T: 24220, Avg. loss: 0.119163\n",
      "Total training time: 1.93 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.06, NNZs: 809, Bias: -9.896048, T: 24220, Avg. loss: 0.170076\n",
      "Total training time: 1.95 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.75, NNZs: 348, Bias: -7.894614, T: 36330, Avg. loss: 0.055266\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.73, NNZs: 340, Bias: -10.189606, T: 36330, Avg. loss: 0.049192\n",
      "Total training time: 2.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.14, NNZs: 279, Bias: -4.539162, T: 36330, Avg. loss: 0.029261\n",
      "Total training time: 2.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.70, NNZs: 721, Bias: -6.169080, T: 36330, Avg. loss: 0.096966\n",
      "Total training time: 2.13 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.39, NNZs: 391, Bias: -9.676242, T: 36330, Avg. loss: 0.042158\n",
      "Total training time: 2.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.14, NNZs: 1452, Bias: -2.704386, T: 24220, Avg. loss: 0.165224\n",
      "Total training time: 2.21 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.02, NNZs: 735, Bias: -6.477459, T: 36330, Avg. loss: 0.100364\n",
      "Total training time: 2.32 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.11, NNZs: 310, Bias: -6.943856, T: 48440, Avg. loss: 0.037454\n",
      "Total training time: 2.52 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.74, NNZs: 962, Bias: -5.918567, T: 36330, Avg. loss: 0.125916\n",
      "Total training time: 2.56 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.14, NNZs: 595, Bias: -6.940521, T: 36330, Avg. loss: 0.062857\n",
      "Total training time: 2.61 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.57, NNZs: 888, Bias: -3.282205, T: 36330, Avg. loss: 0.090301\n",
      "Total training time: 2.64 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.94, NNZs: 818, Bias: -8.726235, T: 36330, Avg. loss: 0.133680\n",
      "Total training time: 2.67 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.36, NNZs: 340, Bias: -9.751776, T: 48440, Avg. loss: 0.042116\n",
      "Total training time: 2.67 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.37, NNZs: 359, Bias: -7.419593, T: 48440, Avg. loss: 0.048015\n",
      "Total training time: 2.75 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.75, NNZs: 277, Bias: -4.212367, T: 48440, Avg. loss: 0.025234\n",
      "Total training time: 2.81 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.04, NNZs: 743, Bias: -5.512726, T: 48440, Avg. loss: 0.083693\n",
      "Total training time: 2.81 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.98, NNZs: 384, Bias: -9.191598, T: 48440, Avg. loss: 0.037212\n",
      "Total training time: 2.83 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.29, NNZs: 727, Bias: -5.862674, T: 48440, Avg. loss: 0.085906\n",
      "Total training time: 3.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.35, NNZs: 1420, Bias: -2.139501, T: 36330, Avg. loss: 0.127120\n",
      "Total training time: 3.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.84, NNZs: 314, Bias: -6.646963, T: 60550, Avg. loss: 0.034381\n",
      "Total training time: 3.17 seconds.\n",
      "Norm: 6.92, NNZs: 966, Bias: -5.198220, T: 48440, Avg. loss: 0.108207\n",
      "Total training time: 3.31 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.20, NNZs: 343, Bias: -9.385300, T: 60550, Avg. loss: 0.038489\n",
      "Total training time: 3.31 seconds.\n",
      "Norm: 5.55, NNZs: 590, Bias: -6.377559, T: 48440, Avg. loss: 0.054572\n",
      "Total training time: 3.36 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.78, NNZs: 895, Bias: -2.835063, T: 48440, Avg. loss: 0.080649\n",
      "Total training time: 3.38 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.32, NNZs: 845, Bias: -7.936461, T: 48440, Avg. loss: 0.114901\n",
      "Total training time: 3.44 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.07, NNZs: 360, Bias: -7.106400, T: 60550, Avg. loss: 0.043651\n",
      "Total training time: 3.44 seconds.\n",
      "Norm: 5.57, NNZs: 761, Bias: -5.053328, T: 60550, Avg. loss: 0.076020\n",
      "Total training time: 3.46 seconds.\n",
      "Norm: 4.72, NNZs: 381, Bias: -8.817465, T: 60550, Avg. loss: 0.034453\n",
      "Total training time: 3.48 seconds.\n",
      "Norm: 3.48, NNZs: 287, Bias: -3.990057, T: 60550, Avg. loss: 0.023510\n",
      "Total training time: 3.48 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    3.3s remaining:   16.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 5.86, NNZs: 735, Bias: -5.340877, T: 60550, Avg. loss: 0.078674\n",
      "Total training time: 3.59 seconds.\n",
      "Norm: 8.36, NNZs: 1439, Bias: -1.800549, T: 48440, Avg. loss: 0.113791\n",
      "Total training time: 3.84 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.15, NNZs: 597, Bias: -5.979880, T: 60550, Avg. loss: 0.049848\n",
      "Total training time: 3.84 seconds.\n",
      "Norm: 6.24, NNZs: 912, Bias: -2.534809, T: 60550, Avg. loss: 0.074615\n",
      "Total training time: 3.87 seconds.\n",
      "Norm: 6.37, NNZs: 981, Bias: -4.706907, T: 60550, Avg. loss: 0.098154\n",
      "Total training time: 3.90 seconds.\n",
      "Norm: 5.89, NNZs: 862, Bias: -7.358911, T: 60550, Avg. loss: 0.103136\n",
      "Total training time: 3.91 seconds.\n",
      "Norm: 7.72, NNZs: 1469, Bias: -1.609742, T: 60550, Avg. loss: 0.104907\n",
      "Total training time: 4.41 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    4.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.005, l1_ratio=0.1, max_iter=5, n_jobs=-1,\n",
       "              penalty='elasticnet', random_state=42, tol=None, verbose=1)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit classifier with best hyper-parameter\n",
    "clfs[2] = SGDClassifier(loss='hinge', penalty='elasticnet', l1_ratio=best_l1_ratio, alpha=5e-3, random_state=42, max_iter=5, tol=None, n_jobs=-1, verbose=1)\n",
    "clfs[2].fit(bm25_matrix_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7885208012326657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.23      0.33        53\n",
      "           1       0.58      0.21      0.31       122\n",
      "           2       0.90      0.81      0.85       140\n",
      "           3       0.82      0.84      0.83       420\n",
      "           4       0.87      0.91      0.89       665\n",
      "           5       0.67      0.50      0.57       113\n",
      "           6       0.50      0.18      0.26        28\n",
      "           7       0.58      0.68      0.63        38\n",
      "           8       0.89      0.75      0.81       102\n",
      "           9       0.72      0.90      0.80       546\n",
      "          10       0.73      0.70      0.71       195\n",
      "          11       0.83      0.84      0.84       174\n",
      "\n",
      "    accuracy                           0.79      2596\n",
      "   macro avg       0.72      0.63      0.65      2596\n",
      "weighted avg       0.78      0.79      0.77      2596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on validation set\n",
    "y_preds[2] = clfs[2].predict(bm25_matrix_val)\n",
    "val_accs[2] = accuracy_score(y_val, y_preds[2])\n",
    "print('Accuracy on validation set: ', val_accs[2])\n",
    "\n",
    "# additionally, we check the classification report\n",
    "# look at F1-Score, Precision, Recall, and relationship between Support & metrics\n",
    "print(classification_report(y_val, y_preds[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: BM25, low-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dimensionality size of: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.1s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 5.20, NNZs: 15, Bias: -9.706744, T: 12110, Avg. loss: 0.157074\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 6.44, NNZs: 12, Bias: -1.348879, T: 12110, Avg. loss: 0.442522\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8.18, NNZs: 11, Bias: -1.973816, T: 12110, Avg. loss: 0.409514\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.42, NNZs: 11, Bias: -1.012621, T: 24220, Avg. loss: 0.144675\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "-- Epoch 1\n",
      "Norm: 9.09, NNZs: 11, Bias: -2.336209, T: 12110, Avg. loss: 0.610737\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 6.62, NNZs: 13, Bias: -1.912213, T: 12110, Avg. loss: 0.482815\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.87, NNZs: 11, Bias: -2.615539, T: 12110, Avg. loss: 0.216549\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 4.90, NNZs: 16, Bias: -6.040601, T: 12110, Avg. loss: 0.188220\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.85, NNZs: 15, Bias: -7.307824, T: 24220, Avg. loss: 0.062016\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.64, NNZs: 15, Bias: -3.626474, T: 24220, Avg. loss: 0.064237\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "-- Epoch 1\n",
      "Norm: 3.19, NNZs: 15, Bias: -2.464781, T: 12110, Avg. loss: 0.136687\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.51, NNZs: 15, Bias: -3.357041, T: 12110, Avg. loss: 0.435410\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 4.24, NNZs: 10, Bias: -2.228027, T: 12110, Avg. loss: 0.163455\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.71, NNZs: 12, Bias: -1.405624, T: 24220, Avg. loss: 0.064309\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.39, NNZs: 9, Bias: -1.128173, T: 24220, Avg. loss: 0.055691\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.98, NNZs: 12, Bias: -1.107278, T: 36330, Avg. loss: 0.054231\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "-- Epoch 1\n",
      "Norm: 7.53, NNZs: 13, Bias: -1.534217, T: 24220, Avg. loss: 0.191254\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.98, NNZs: 15, Bias: -0.278022, T: 12110, Avg. loss: 0.789048\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.51, NNZs: 13, Bias: -3.656524, T: 12110, Avg. loss: 0.302148\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.16, NNZs: 17, Bias: -1.202490, T: 24220, Avg. loss: 0.181613\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.62, NNZs: 13, Bias: -0.677375, T: 24220, Avg. loss: 0.288561\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.23, NNZs: 10, Bias: -1.156024, T: 24220, Avg. loss: 0.121940\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.54, NNZs: 8, Bias: -1.153787, T: 24220, Avg. loss: 0.037690\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.25, NNZs: 9, Bias: -1.178597, T: 36330, Avg. loss: 0.031233\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.45, NNZs: 13, Bias: -2.110300, T: 24220, Avg. loss: 0.171469\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.16, NNZs: 15, Bias: -2.638225, T: 36330, Avg. loss: 0.052469\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.42, NNZs: 17, Bias: -2.142632, T: 36330, Avg. loss: 0.156270\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.91, NNZs: 11, Bias: -1.055628, T: 36330, Avg. loss: 0.046894\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.19, NNZs: 15, Bias: -5.875056, T: 36330, Avg. loss: 0.052732\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.64, NNZs: 14, Bias: -1.071420, T: 48440, Avg. loss: 0.043506\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.29, NNZs: 15, Bias: -1.635074, T: 36330, Avg. loss: 0.179937\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.41, NNZs: 12, Bias: -1.064668, T: 60550, Avg. loss: 0.042430\n",
      "Total training time: 0.04 seconds.\n",
      "Norm: 4.42, NNZs: 16, Bias: -1.143160, T: 36330, Avg. loss: 0.162477\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.83, NNZs: 14, Bias: -0.443232, T: 36330, Avg. loss: 0.260368\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.22, NNZs: 10, Bias: -1.170690, T: 36330, Avg. loss: 0.105192\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.61, NNZs: 16, Bias: -1.490263, T: 48440, Avg. loss: 0.170453\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.90, NNZs: 17, Bias: -4.936204, T: 48440, Avg. loss: 0.048053\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.03, NNZs: 10, Bias: -0.997257, T: 48440, Avg. loss: 0.029802\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.69, NNZs: 17, Bias: -1.808417, T: 24220, Avg. loss: 0.113363\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.63, NNZs: 13, Bias: -1.095016, T: 36330, Avg. loss: 0.124482\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.82, NNZs: 18, Bias: -2.214436, T: 48440, Avg. loss: 0.048820\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.07, NNZs: 16, Bias: -1.046051, T: 48440, Avg. loss: 0.156778\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.64, NNZs: 12, Bias: -1.149136, T: 48440, Avg. loss: 0.097896\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.27, NNZs: 14, Bias: -0.496996, T: 48440, Avg. loss: 0.253281\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.00, NNZs: 16, Bias: -1.568642, T: 60550, Avg. loss: 0.168391\n",
      "Total training time: 0.06 seconds.\n",
      "Norm: 3.84, NNZs: 16, Bias: -1.384690, T: 36330, Avg. loss: 0.098543\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.90, NNZs: 12, Bias: -1.053636, T: 60550, Avg. loss: 0.028646\n",
      "Total training time: 0.05 seconds.\n",
      "Norm: 3.66, NNZs: 17, Bias: -1.141402, T: 60550, Avg. loss: 0.151693\n",
      "Total training time: 0.06 seconds.\n",
      "Norm: 2.49, NNZs: 17, Bias: -1.894823, T: 60550, Avg. loss: 0.048293\n",
      "Total training time: 0.05 seconds.\n",
      "Norm: 2.63, NNZs: 15, Bias: -1.056562, T: 48440, Avg. loss: 0.049227\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.14, NNZs: 12, Bias: -1.126910, T: 48440, Avg. loss: 0.115027\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.17, NNZs: 11, Bias: -1.058486, T: 60550, Avg. loss: 0.095538\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 2.39, NNZs: 13, Bias: -1.100100, T: 60550, Avg. loss: 0.046995\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 2.73, NNZs: 17, Bias: -4.159663, T: 60550, Avg. loss: 0.043741\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 3.78, NNZs: 10, Bias: -1.067059, T: 60550, Avg. loss: 0.109650\n",
      "Total training time: 0.06 seconds.\n",
      "Norm: 4.93, NNZs: 14, Bias: -0.433273, T: 60550, Avg. loss: 0.244640\n",
      "Total training time: 0.04 seconds.\n",
      "Norm: 3.79, NNZs: 17, Bias: -1.772640, T: 48440, Avg. loss: 0.148992\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.33, NNZs: 16, Bias: -1.266353, T: 48440, Avg. loss: 0.093670\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.37, NNZs: 18, Bias: -1.663899, T: 60550, Avg. loss: 0.144763\n",
      "Total training time: 0.04 seconds.\n",
      "Norm: 2.96, NNZs: 18, Bias: -1.282588, T: 60550, Avg. loss: 0.090037\n",
      "Total training time: 0.03 seconds.\n",
      "Accuracy on validation set:  0.7407550077041603\n",
      "--------------------\n",
      "Using dimensionality size of: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.1s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 7.56, NNZs: 32, Bias: -14.217122, T: 12110, Avg. loss: 0.166771\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.18, NNZs: 35, Bias: -4.694294, T: 12110, Avg. loss: 0.581313\n",
      "Total training time: 0.01 seconds.\n",
      "Norm: 5.74, NNZs: 29, Bias: -11.629346, T: 24220, Avg. loss: 0.073430\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.80, NNZs: 32, Bias: -7.935771, T: 12110, Avg. loss: 0.634542\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 2\n",
      "Norm: 9.42, NNZs: 29, Bias: -2.274911, T: 12110, Avg. loss: 0.641417\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.95, NNZs: 30, Bias: -10.049120, T: 36330, Avg. loss: 0.062693\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 4\n",
      "-- Epoch 1\n",
      "Norm: 7.65, NNZs: 33, Bias: -1.348962, T: 24220, Avg. loss: 0.182163\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.13, NNZs: 31, Bias: -2.026916, T: 24220, Avg. loss: 0.168526\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.58, NNZs: 34, Bias: -8.945335, T: 48440, Avg. loss: 0.057723\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.80, NNZs: 36, Bias: -13.635576, T: 12110, Avg. loss: 0.245145\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 6.52, NNZs: 36, Bias: -1.202546, T: 36330, Avg. loss: 0.157452\n",
      "Total training time: 0.02 seconds.\n",
      "Norm: 6.53, NNZs: 35, Bias: -7.381567, T: 12110, Avg. loss: 0.285620\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.83, NNZs: 39, Bias: -1.104453, T: 12110, Avg. loss: 0.985332\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.95, NNZs: 22, Bias: -3.202196, T: 24220, Avg. loss: 0.177049\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "-- Epoch 1\n",
      "Norm: 6.10, NNZs: 30, Bias: -1.390484, T: 36330, Avg. loss: 0.137718\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.30, NNZs: 25, Bias: -1.584132, T: 36330, Avg. loss: 0.121463\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 8.83, NNZs: 34, Bias: -8.826873, T: 12110, Avg. loss: 0.611059\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.28, NNZs: 33, Bias: -8.077758, T: 60550, Avg. loss: 0.053301\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 4\n",
      "Norm: 5.78, NNZs: 37, Bias: -11.064965, T: 12110, Avg. loss: 0.229166\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.90, NNZs: 22, Bias: -1.369791, T: 48440, Avg. loss: 0.103775\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.71, NNZs: 34, Bias: -0.907353, T: 24220, Avg. loss: 0.281270\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.67, NNZs: 31, Bias: -8.099434, T: 12110, Avg. loss: 0.402883\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.42, NNZs: 32, Bias: -1.300018, T: 48440, Avg. loss: 0.124294\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.44, NNZs: 23, Bias: -1.165898, T: 60550, Avg. loss: 0.101190\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 5.75, NNZs: 34, Bias: -4.560822, T: 24220, Avg. loss: 0.129127\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.86, NNZs: 37, Bias: -1.201856, T: 48440, Avg. loss: 0.146905\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.54, NNZs: 34, Bias: -0.661817, T: 36330, Avg. loss: 0.241359\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.63, NNZs: 29, Bias: -4.235722, T: 24220, Avg. loss: 0.194605\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.21, NNZs: 34, Bias: -8.056935, T: 24220, Avg. loss: 0.076059\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.76, NNZs: 32, Bias: -4.887923, T: 12110, Avg. loss: 0.813531\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.86, NNZs: 33, Bias: -4.841540, T: 24220, Avg. loss: 0.083489\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.21, NNZs: 33, Bias: -3.106793, T: 36330, Avg. loss: 0.099348\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.41, NNZs: 36, Bias: -10.409147, T: 24220, Avg. loss: 0.090657\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.89, NNZs: 25, Bias: -1.245917, T: 60550, Avg. loss: 0.118276\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 5.30, NNZs: 33, Bias: -1.065759, T: 60550, Avg. loss: 0.141844\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 6.68, NNZs: 38, Bias: -0.747304, T: 48440, Avg. loss: 0.228500\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.46, NNZs: 38, Bias: -6.299629, T: 36330, Avg. loss: 0.061476\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.54, NNZs: 35, Bias: -7.805308, T: 12110, Avg. loss: 0.193232\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.17, NNZs: 36, Bias: -2.825933, T: 36330, Avg. loss: 0.148629\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.29, NNZs: 32, Bias: -2.129796, T: 24220, Avg. loss: 0.213867\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.11, NNZs: 33, Bias: -3.373905, T: 36330, Avg. loss: 0.066108\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.68, NNZs: 34, Bias: -2.178821, T: 48440, Avg. loss: 0.091625\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.08, NNZs: 34, Bias: -5.045712, T: 48440, Avg. loss: 0.051202\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.49, NNZs: 35, Bias: -2.350684, T: 48440, Avg. loss: 0.140413\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.55, NNZs: 36, Bias: -8.580446, T: 36330, Avg. loss: 0.074979\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.77, NNZs: 33, Bias: -2.515234, T: 48440, Avg. loss: 0.057506\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.21, NNZs: 34, Bias: -1.610372, T: 36330, Avg. loss: 0.179075\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.73, NNZs: 36, Bias: -4.131541, T: 60550, Avg. loss: 0.046218\n",
      "Total training time: 0.03 seconds.\n",
      "Norm: 4.11, NNZs: 35, Bias: -1.817248, T: 60550, Avg. loss: 0.085037\n",
      "Total training time: 0.03 seconds.\n",
      "Norm: 4.70, NNZs: 37, Bias: -5.422516, T: 24220, Avg. loss: 0.068181\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.01, NNZs: 38, Bias: -2.100305, T: 60550, Avg. loss: 0.134544\n",
      "Total training time: 0.05 seconds.\n",
      "Norm: 3.46, NNZs: 28, Bias: -2.202038, T: 60550, Avg. loss: 0.051704\n",
      "Total training time: 0.09 seconds.\n",
      "Norm: 3.99, NNZs: 38, Bias: -7.322656, T: 48440, Avg. loss: 0.065576\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.48, NNZs: 35, Bias: -1.471098, T: 48440, Avg. loss: 0.167587\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.97, NNZs: 37, Bias: -3.960338, T: 36330, Avg. loss: 0.054871\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.80, NNZs: 38, Bias: -1.579101, T: 60550, Avg. loss: 0.163974\n",
      "Total training time: 0.05 seconds.\n",
      "Norm: 6.16, NNZs: 36, Bias: -0.776633, T: 60550, Avg. loss: 0.220891\n",
      "Total training time: 0.08 seconds.\n",
      "Norm: 3.75, NNZs: 37, Bias: -6.336203, T: 60550, Avg. loss: 0.059664\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 3.45, NNZs: 38, Bias: -3.317715, T: 48440, Avg. loss: 0.047127\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.19, NNZs: 42, Bias: -2.884754, T: 60550, Avg. loss: 0.044555\n",
      "Total training time: 0.04 seconds.\n",
      "Accuracy on validation set:  0.764637904468413\n",
      "--------------------\n",
      "Using dimensionality size of: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.1s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 11.30, NNZs: 65, Bias: -15.777536, T: 12110, Avg. loss: 0.899397\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 8.31, NNZs: 71, Bias: -13.606805, T: 12110, Avg. loss: 0.253916\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8.93, NNZs: 64, Bias: -15.423417, T: 12110, Avg. loss: 0.370383\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.47, NNZs: 63, Bias: -8.925262, T: 12110, Avg. loss: 0.759340\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.94, NNZs: 58, Bias: -4.281558, T: 12110, Avg. loss: 0.838108\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 8.47, NNZs: 65, Bias: -17.711554, T: 12110, Avg. loss: 0.290663\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8.51, NNZs: 65, Bias: -16.728900, T: 12110, Avg. loss: 0.288545\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.19, NNZs: 62, Bias: -18.808293, T: 12110, Avg. loss: 0.222068\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12.39, NNZs: 62, Bias: -9.942181, T: 12110, Avg. loss: 1.126283\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.37, NNZs: 63, Bias: -14.611463, T: 24220, Avg. loss: 0.099741\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.08, NNZs: 64, Bias: -15.918327, T: 12110, Avg. loss: 0.808903\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8.71, NNZs: 52, Bias: -9.255017, T: 24220, Avg. loss: 0.265963\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.40, NNZs: 67, Bias: -13.102497, T: 12110, Avg. loss: 0.518820\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 6.38, NNZs: 72, Bias: -10.810345, T: 24220, Avg. loss: 0.084942\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.06, NNZs: 63, Bias: -9.586902, T: 24220, Avg. loss: 0.257752\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.76, NNZs: 59, Bias: -12.135191, T: 24220, Avg. loss: 0.122783\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.01, NNZs: 66, Bias: -13.531828, T: 24220, Avg. loss: 0.093925\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.74, NNZs: 56, Bias: -4.569797, T: 24220, Avg. loss: 0.264803\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.32, NNZs: 50, Bias: -5.769649, T: 36330, Avg. loss: 0.187721\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 10.10, NNZs: 58, Bias: -2.004098, T: 24220, Avg. loss: 0.187179\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.83, NNZs: 67, Bias: -16.049397, T: 24220, Avg. loss: 0.087981\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.82, NNZs: 70, Bias: -8.728618, T: 24220, Avg. loss: 0.161479\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 13.63, NNZs: 60, Bias: -1.864232, T: 12110, Avg. loss: 1.307735\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.62, NNZs: 70, Bias: -9.124537, T: 36330, Avg. loss: 0.069379\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.47, NNZs: 70, Bias: -6.447496, T: 36330, Avg. loss: 0.191894\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.93, NNZs: 54, Bias: -4.399748, T: 24220, Avg. loss: 0.202585\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.60, NNZs: 59, Bias: -2.440379, T: 36330, Avg. loss: 0.191654\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.56, NNZs: 53, Bias: -3.669870, T: 48440, Avg. loss: 0.148437\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.59, NNZs: 59, Bias: -1.326977, T: 36330, Avg. loss: 0.161200\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.69, NNZs: 72, Bias: -12.690215, T: 36330, Avg. loss: 0.084103\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.82, NNZs: 62, Bias: -10.145593, T: 36330, Avg. loss: 0.097403\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.36, NNZs: 57, Bias: -0.846721, T: 24220, Avg. loss: 0.299501\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.03, NNZs: 70, Bias: -11.735247, T: 36330, Avg. loss: 0.079894\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.42, NNZs: 71, Bias: -6.455354, T: 36330, Avg. loss: 0.124340\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.81, NNZs: 69, Bias: -8.086732, T: 48440, Avg. loss: 0.061123\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.03, NNZs: 59, Bias: -2.046810, T: 48440, Avg. loss: 0.166459\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.45, NNZs: 67, Bias: -4.667859, T: 48440, Avg. loss: 0.159324\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.00, NNZs: 57, Bias: -2.598018, T: 36330, Avg. loss: 0.144331\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.06, NNZs: 50, Bias: -2.315002, T: 60550, Avg. loss: 0.124726\n",
      "Total training time: 0.09 seconds.\n",
      "Norm: 7.77, NNZs: 58, Bias: -1.230467, T: 48440, Avg. loss: 0.147695\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.89, NNZs: 64, Bias: -0.822843, T: 36330, Avg. loss: 0.249751\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.94, NNZs: 71, Bias: -14.358876, T: 36330, Avg. loss: 0.074230\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.47, NNZs: 64, Bias: -1.817240, T: 60550, Avg. loss: 0.161102\n",
      "Total training time: 0.09 seconds.\n",
      "Norm: 5.17, NNZs: 70, Bias: -11.384714, T: 48440, Avg. loss: 0.072960\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.39, NNZs: 61, Bias: -8.676982, T: 48440, Avg. loss: 0.084453\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.68, NNZs: 71, Bias: -4.918868, T: 48440, Avg. loss: 0.103302\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.50, NNZs: 73, Bias: -10.474234, T: 48440, Avg. loss: 0.069861\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.10, NNZs: 50, Bias: -1.832904, T: 48440, Avg. loss: 0.130718\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.92, NNZs: 66, Bias: -3.452663, T: 60550, Avg. loss: 0.138719\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 4.46, NNZs: 75, Bias: -7.202205, T: 60550, Avg. loss: 0.057414\n",
      "Total training time: 0.10 seconds.\n",
      "Norm: 7.02, NNZs: 63, Bias: -1.225653, T: 60550, Avg. loss: 0.139158\n",
      "Total training time: 0.11 seconds.\n",
      "Norm: 6.34, NNZs: 74, Bias: -13.229567, T: 48440, Avg. loss: 0.066643\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.74, NNZs: 66, Bias: -7.784584, T: 60550, Avg. loss: 0.076921\n",
      "Total training time: 0.11 seconds.\n",
      "Norm: 8.86, NNZs: 59, Bias: -0.773664, T: 48440, Avg. loss: 0.233404\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.05, NNZs: 73, Bias: -3.932731, T: 60550, Avg. loss: 0.092261\n",
      "Total training time: 0.09 seconds.\n",
      "Norm: 5.13, NNZs: 76, Bias: -9.528881, T: 60550, Avg. loss: 0.064521\n",
      "Total training time: 0.10 seconds.\n",
      "Norm: 4.75, NNZs: 70, Bias: -10.396109, T: 60550, Avg. loss: 0.067758\n",
      "Total training time: 0.11 seconds.\n",
      "Norm: 6.46, NNZs: 60, Bias: -1.497662, T: 60550, Avg. loss: 0.121076\n",
      "Total training time: 0.11 seconds.\n",
      "Norm: 6.11, NNZs: 73, Bias: -12.239260, T: 60550, Avg. loss: 0.061613\n",
      "Total training time: 0.12 seconds.\n",
      "Norm: 8.14, NNZs: 62, Bias: -0.722050, T: 60550, Avg. loss: 0.221135\n",
      "Total training time: 0.07 seconds.\n",
      "Accuracy on validation set:  0.775808936825886\n",
      "--------------------\n",
      "Using dimensionality size of: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 12.25, NNZs: 135, Bias: -22.318023, T: 12110, Avg. loss: 0.501763\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 9.30, NNZs: 125, Bias: -19.023174, T: 24220, Avg. loss: 0.151434\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.86, NNZs: 129, Bias: -21.296207, T: 12110, Avg. loss: 0.297165\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.40, NNZs: 126, Bias: -21.294953, T: 12110, Avg. loss: 0.358786\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.54, NNZs: 129, Bias: -23.683758, T: 12110, Avg. loss: 0.363767\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 15.01, NNZs: 135, Bias: -21.954901, T: 12110, Avg. loss: 1.197320\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 15.43, NNZs: 142, Bias: -19.935137, T: 12110, Avg. loss: 1.090612\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 14.90, NNZs: 142, Bias: -21.543583, T: 12110, Avg. loss: 0.677930\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.57, NNZs: 135, Bias: -7.401391, T: 12110, Avg. loss: 1.866764\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8.38, NNZs: 120, Bias: -16.948292, T: 36330, Avg. loss: 0.118967\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.44, NNZs: 128, Bias: -22.396201, T: 12110, Avg. loss: 0.971439\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.12, NNZs: 136, Bias: -16.246704, T: 12110, Avg. loss: 0.299803\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.88, NNZs: 119, Bias: -18.547298, T: 24220, Avg. loss: 0.086692\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 17.02, NNZs: 120, Bias: -14.886860, T: 12110, Avg. loss: 1.442405\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 16.06, NNZs: 131, Bias: -10.232294, T: 12110, Avg. loss: 1.181007\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.22, NNZs: 124, Bias: -20.473590, T: 24220, Avg. loss: 0.111343\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.31, NNZs: 115, Bias: -17.849678, T: 24220, Avg. loss: 0.107104\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 11.50, NNZs: 137, Bias: -13.887800, T: 24220, Avg. loss: 0.307043\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 11.42, NNZs: 126, Bias: -15.526790, T: 24220, Avg. loss: 0.347040\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.82, NNZs: 129, Bias: -15.429788, T: 48440, Avg. loss: 0.104921\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.39, NNZs: 123, Bias: -15.735618, T: 24220, Avg. loss: 0.324174\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 11.26, NNZs: 142, Bias: -16.681421, T: 24220, Avg. loss: 0.203166\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.61, NNZs: 123, Bias: -16.912497, T: 36330, Avg. loss: 0.071769\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.34, NNZs: 121, Bias: -15.951062, T: 36330, Avg. loss: 0.087251\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.33, NNZs: 133, Bias: -18.578264, T: 36330, Avg. loss: 0.094085\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.71, NNZs: 139, Bias: -10.206643, T: 36330, Avg. loss: 0.236630\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.50, NNZs: 100, Bias: -2.399747, T: 24220, Avg. loss: 0.382470\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.55, NNZs: 123, Bias: -11.724526, T: 36330, Avg. loss: 0.256818\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.10, NNZs: 132, Bias: -14.407264, T: 60550, Avg. loss: 0.096655\n",
      "Total training time: 0.13 seconds.\n",
      "Norm: 10.40, NNZs: 135, Bias: -12.022159, T: 36330, Avg. loss: 0.239565\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.55, NNZs: 133, Bias: -13.987479, T: 36330, Avg. loss: 0.157405\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.46, NNZs: 123, Bias: -13.783638, T: 24220, Avg. loss: 0.089997\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.00, NNZs: 115, Bias: -5.289380, T: 24220, Avg. loss: 0.260679\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.15, NNZs: 129, Bias: -15.627397, T: 48440, Avg. loss: 0.066964\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.61, NNZs: 126, Bias: -14.672736, T: 48440, Avg. loss: 0.075932\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.44, NNZs: 135, Bias: -17.403886, T: 48440, Avg. loss: 0.084076\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.46, NNZs: 134, Bias: -7.921529, T: 48440, Avg. loss: 0.189225\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.48, NNZs: 127, Bias: -9.227141, T: 48440, Avg. loss: 0.208887\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.67, NNZs: 105, Bias: -1.226210, T: 36330, Avg. loss: 0.270507\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.92, NNZs: 141, Bias: -9.702535, T: 48440, Avg. loss: 0.200196\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.46, NNZs: 135, Bias: -12.083780, T: 48440, Avg. loss: 0.134375\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.75, NNZs: 109, Bias: -8.972617, T: 24220, Avg. loss: 0.331931\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.40, NNZs: 128, Bias: -12.184329, T: 36330, Avg. loss: 0.071843\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 10.14, NNZs: 102, Bias: -2.869989, T: 36330, Avg. loss: 0.184234\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.69, NNZs: 132, Bias: -14.722661, T: 60550, Avg. loss: 0.061364\n",
      "Total training time: 0.15 seconds.\n",
      "Norm: 7.08, NNZs: 126, Bias: -13.726952, T: 60550, Avg. loss: 0.070708\n",
      "Total training time: 0.12 seconds.\n",
      "Norm: 7.65, NNZs: 123, Bias: -7.385958, T: 60550, Avg. loss: 0.179943\n",
      "Total training time: 0.16 seconds.\n",
      "Norm: 7.53, NNZs: 136, Bias: -6.236564, T: 60550, Avg. loss: 0.160424\n",
      "Total training time: 0.13 seconds.\n",
      "Norm: 7.10, NNZs: 136, Bias: -16.372459, T: 60550, Avg. loss: 0.077647\n",
      "Total training time: 0.12 seconds.\n",
      "Norm: 7.63, NNZs: 148, Bias: -10.730449, T: 60550, Avg. loss: 0.120747\n",
      "Total training time: 0.12 seconds.\n",
      "Norm: 11.47, NNZs: 107, Bias: -1.000530, T: 48440, Avg. loss: 0.245871\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.03, NNZs: 136, Bias: -7.866249, T: 60550, Avg. loss: 0.174517\n",
      "Total training time: 0.12 seconds.\n",
      "Norm: 10.76, NNZs: 114, Bias: -5.389091, T: 36330, Avg. loss: 0.239237\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.66, NNZs: 136, Bias: -11.161318, T: 48440, Avg. loss: 0.060896\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.30, NNZs: 111, Bias: -1.937267, T: 48440, Avg. loss: 0.151554\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.53, NNZs: 123, Bias: -3.482178, T: 48440, Avg. loss: 0.194722\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 10.67, NNZs: 118, Bias: -0.917469, T: 60550, Avg. loss: 0.228859\n",
      "Total training time: 0.16 seconds.\n",
      "Norm: 6.27, NNZs: 133, Bias: -10.260583, T: 60550, Avg. loss: 0.056899\n",
      "Total training time: 0.15 seconds.\n",
      "Norm: 8.61, NNZs: 119, Bias: -1.779908, T: 60550, Avg. loss: 0.141168\n",
      "Total training time: 0.17 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.2s remaining:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 8.74, NNZs: 118, Bias: -2.555404, T: 60550, Avg. loss: 0.172368\n",
      "Total training time: 0.19 seconds.\n",
      "Accuracy on validation set:  0.7673343605546995\n",
      "--------------------\n",
      "Using dimensionality size of: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "-- Epoch 1\n",
      "\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 15.18, NNZs: 203, Bias: -28.954972, T: 12110, Avg. loss: 0.617692\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.30, NNZs: 209, Bias: -29.175245, T: 12110, Avg. loss: 1.408530\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 14.87, NNZs: 198, Bias: -23.505227, T: 12110, Avg. loss: 0.337911\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 13.81, NNZs: 180, Bias: -28.715844, T: 12110, Avg. loss: 0.417174\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 13.49, NNZs: 197, Bias: -26.590713, T: 12110, Avg. loss: 0.401281\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 13.88, NNZs: 196, Bias: -23.320370, T: 12110, Avg. loss: 0.366795\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.13, NNZs: 197, Bias: -22.936745, T: 12110, Avg. loss: 1.206874\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 14.08, NNZs: 196, Bias: -22.260445, T: 24220, Avg. loss: 0.427082\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 11.81, NNZs: 201, Bias: -25.468563, T: 24220, Avg. loss: 0.171130\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 19.91, NNZs: 211, Bias: -14.581019, T: 12110, Avg. loss: 1.360902\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 20.55, NNZs: 201, Bias: -20.299715, T: 12110, Avg. loss: 1.641021\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.19, NNZs: 181, Bias: -20.780956, T: 24220, Avg. loss: 0.090516\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 19.83, NNZs: 194, Bias: -26.922417, T: 12110, Avg. loss: 1.149370\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 22.91, NNZs: 200, Bias: -9.870438, T: 12110, Avg. loss: 2.314423\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 11.70, NNZs: 174, Bias: -24.776830, T: 24220, Avg. loss: 0.134231\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.70, NNZs: 189, Bias: -23.248958, T: 24220, Avg. loss: 0.120271\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.96, NNZs: 191, Bias: -20.359205, T: 24220, Avg. loss: 0.109834\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.88, NNZs: 195, Bias: -23.101389, T: 36330, Avg. loss: 0.135747\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 13.40, NNZs: 200, Bias: -16.895638, T: 24220, Avg. loss: 0.334148\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 11.85, NNZs: 189, Bias: -18.247301, T: 36330, Avg. loss: 0.328620\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 10.06, NNZs: 183, Bias: -19.017578, T: 36330, Avg. loss: 0.071548\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.71, NNZs: 178, Bias: -20.201904, T: 24220, Avg. loss: 0.364543\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.65, NNZs: 183, Bias: -22.677173, T: 36330, Avg. loss: 0.104543\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 15.00, NNZs: 163, Bias: -13.684147, T: 24220, Avg. loss: 0.394735\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 18.07, NNZs: 204, Bias: -26.831808, T: 12110, Avg. loss: 0.822946\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.13, NNZs: 204, Bias: -21.586199, T: 48440, Avg. loss: 0.114096\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.81, NNZs: 195, Bias: -21.314312, T: 36330, Avg. loss: 0.094684\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.76, NNZs: 188, Bias: -18.545952, T: 36330, Avg. loss: 0.085581\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.62, NNZs: 178, Bias: -4.078969, T: 24220, Avg. loss: 0.447572\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.41, NNZs: 183, Bias: -15.510337, T: 48440, Avg. loss: 0.270828\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14.40, NNZs: 182, Bias: -8.769676, T: 24220, Avg. loss: 0.312255\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.38, NNZs: 189, Bias: -17.845544, T: 48440, Avg. loss: 0.064118\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.59, NNZs: 186, Bias: -16.301924, T: 36330, Avg. loss: 0.276931\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.98, NNZs: 191, Bias: -20.050913, T: 48440, Avg. loss: 0.083676\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.89, NNZs: 187, Bias: -17.361824, T: 48440, Avg. loss: 0.072877\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.55, NNZs: 163, Bias: -9.433618, T: 36330, Avg. loss: 0.290010\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.40, NNZs: 161, Bias: -1.966393, T: 36330, Avg. loss: 0.303082\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.81, NNZs: 178, Bias: -21.275210, T: 48440, Avg. loss: 0.091026\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.42, NNZs: 205, Bias: -20.485851, T: 60550, Avg. loss: 0.106348\n",
      "Total training time: 0.17 seconds.\n",
      "Norm: 9.48, NNZs: 192, Bias: -13.335506, T: 60550, Avg. loss: 0.239399\n",
      "Total training time: 0.18 seconds.\n",
      "Norm: 11.45, NNZs: 208, Bias: -13.309740, T: 36330, Avg. loss: 0.252684\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.05, NNZs: 198, Bias: -21.733075, T: 24220, Avg. loss: 0.222289\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.62, NNZs: 196, Bias: -19.027506, T: 60550, Avg. loss: 0.074491\n",
      "Total training time: 0.16 seconds.\n",
      "Norm: 8.43, NNZs: 190, Bias: -16.400368, T: 60550, Avg. loss: 0.066042\n",
      "Total training time: 0.16 seconds.\n",
      "Norm: 8.93, NNZs: 190, Bias: -16.876811, T: 60550, Avg. loss: 0.058430\n",
      "Total training time: 0.19 seconds.\n",
      "Norm: 10.99, NNZs: 188, Bias: -13.855499, T: 48440, Avg. loss: 0.226358\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.2s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 13.02, NNZs: 157, Bias: -1.469682, T: 48440, Avg. loss: 0.258653\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11.68, NNZs: 187, Bias: -5.762086, T: 36330, Avg. loss: 0.214866\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.28, NNZs: 187, Bias: -20.199096, T: 60550, Avg. loss: 0.085881\n",
      "Total training time: 0.17 seconds.\n",
      "Norm: 11.02, NNZs: 179, Bias: -6.805389, T: 48440, Avg. loss: 0.233007\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.14, NNZs: 193, Bias: -18.779003, T: 36330, Avg. loss: 0.173546\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 10.00, NNZs: 216, Bias: -10.928590, T: 48440, Avg. loss: 0.210346\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 10.02, NNZs: 190, Bias: -11.814045, T: 60550, Avg. loss: 0.200459\n",
      "Total training time: 0.19 seconds.\n",
      "Norm: 10.31, NNZs: 170, Bias: -4.017092, T: 48440, Avg. loss: 0.177282\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.12, NNZs: 166, Bias: -1.229819, T: 60550, Avg. loss: 0.239385\n",
      "Total training time: 0.20 seconds.\n",
      "Norm: 9.89, NNZs: 175, Bias: -5.148558, T: 60550, Avg. loss: 0.198767\n",
      "Total training time: 0.21 seconds.\n",
      "Norm: 10.71, NNZs: 199, Bias: -16.933695, T: 48440, Avg. loss: 0.150735\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.00, NNZs: 212, Bias: -9.161035, T: 60550, Avg. loss: 0.179942\n",
      "Total training time: 0.21 seconds.\n",
      "Norm: 9.35, NNZs: 178, Bias: -2.820042, T: 60550, Avg. loss: 0.151578\n",
      "Total training time: 0.23 seconds.\n",
      "Norm: 9.88, NNZs: 196, Bias: -15.431111, T: 60550, Avg. loss: 0.132366\n",
      "Total training time: 0.22 seconds.\n",
      "Accuracy on validation set:  0.7688751926040062\n",
      "--------------------\n",
      "Using dimensionality size of: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 18.01, NNZs: 322, Bias: -33.634002, T: 12110, Avg. loss: 0.710058\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.55, NNZs: 321, Bias: -31.447308, T: 12110, Avg. loss: 0.390784\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 23.12, NNZs: 334, Bias: -26.858279, T: 12110, Avg. loss: 1.450327\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 23.30, NNZs: 304, Bias: -36.567231, T: 12110, Avg. loss: 1.770294\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 17.90, NNZs: 323, Bias: -27.957330, T: 12110, Avg. loss: 0.455386\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 26.27, NNZs: 323, Bias: -29.778221, T: 12110, Avg. loss: 1.955220\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 17.95, NNZs: 323, Bias: -34.410780, T: 12110, Avg. loss: 0.472345\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 25.11, NNZs: 343, Bias: -34.065343, T: 12110, Avg. loss: 1.419339\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 16.16, NNZs: 302, Bias: -26.042249, T: 12110, Avg. loss: 0.438803\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 24.84, NNZs: 341, Bias: -17.726490, T: 12110, Avg. loss: 1.620352\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 22.53, NNZs: 350, Bias: -32.352806, T: 12110, Avg. loss: 0.970483\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 27.93, NNZs: 340, Bias: -12.712489, T: 12110, Avg. loss: 2.714905\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 14.40, NNZs: 314, Bias: -29.872681, T: 24220, Avg. loss: 0.182206\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 14.26, NNZs: 291, Bias: -28.384326, T: 24220, Avg. loss: 0.103667\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.61, NNZs: 288, Bias: -22.763895, T: 24220, Avg. loss: 0.110303\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 18.13, NNZs: 306, Bias: -29.908220, T: 24220, Avg. loss: 0.485981\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 19.61, NNZs: 304, Bias: -21.957732, T: 24220, Avg. loss: 0.501577\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 17.02, NNZs: 308, Bias: -21.211726, T: 24220, Avg. loss: 0.363171\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 18.87, NNZs: 302, Bias: -27.359895, T: 24220, Avg. loss: 0.415827\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 14.00, NNZs: 309, Bias: -30.999115, T: 24220, Avg. loss: 0.132253\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 17.42, NNZs: 344, Bias: -27.291573, T: 24220, Avg. loss: 0.238226\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 17.46, NNZs: 294, Bias: -11.929402, T: 24220, Avg. loss: 0.350251\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 20.13, NNZs: 285, Bias: -6.549001, T: 24220, Avg. loss: 0.512995\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 13.55, NNZs: 303, Bias: -25.034808, T: 24220, Avg. loss: 0.107140\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.87, NNZs: 310, Bias: -27.656908, T: 36330, Avg. loss: 0.134732\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 13.03, NNZs: 292, Bias: -26.347828, T: 36330, Avg. loss: 0.079528\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.39, NNZs: 280, Bias: -20.762893, T: 36330, Avg. loss: 0.088332\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 15.63, NNZs: 325, Bias: -25.767798, T: 36330, Avg. loss: 0.379626\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.44, NNZs: 326, Bias: -17.444201, T: 36330, Avg. loss: 0.362986\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 15.21, NNZs: 342, Bias: -24.320549, T: 36330, Avg. loss: 0.178125\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.52, NNZs: 311, Bias: -23.234508, T: 36330, Avg. loss: 0.312474\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 12.67, NNZs: 316, Bias: -28.952351, T: 36330, Avg. loss: 0.106354\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.39, NNZs: 307, Bias: -8.315597, T: 36330, Avg. loss: 0.247497\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.62, NNZs: 323, Bias: -17.553958, T: 36330, Avg. loss: 0.275068\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.86, NNZs: 281, Bias: -4.261595, T: 36330, Avg. loss: 0.337675\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 12.20, NNZs: 295, Bias: -23.109807, T: 36330, Avg. loss: 0.082398\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 10.36, NNZs: 279, Bias: -19.512637, T: 48440, Avg. loss: 0.075000\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.12, NNZs: 312, Bias: -26.062178, T: 48440, Avg. loss: 0.116123\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14.01, NNZs: 324, Bias: -22.886343, T: 48440, Avg. loss: 0.320102\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.15, NNZs: 285, Bias: -25.036528, T: 48440, Avg. loss: 0.066729\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 13.71, NNZs: 338, Bias: -22.430399, T: 48440, Avg. loss: 0.149369\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14.51, NNZs: 326, Bias: -20.694445, T: 48440, Avg. loss: 0.261154\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14.27, NNZs: 330, Bias: -14.566454, T: 48440, Avg. loss: 0.295469\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.85, NNZs: 332, Bias: -15.295191, T: 48440, Avg. loss: 0.226905\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11.86, NNZs: 318, Bias: -27.538361, T: 48440, Avg. loss: 0.091053\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.54, NNZs: 308, Bias: -6.205875, T: 48440, Avg. loss: 0.196507\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 15.09, NNZs: 282, Bias: -2.831737, T: 48440, Avg. loss: 0.270423\n",
      "Total training time: 0.25 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11.13, NNZs: 294, Bias: -21.906708, T: 48440, Avg. loss: 0.069761\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.79, NNZs: 287, Bias: -18.546534, T: 60550, Avg. loss: 0.068522\n",
      "Total training time: 0.26 seconds.\n",
      "Norm: 11.42, NNZs: 317, Bias: -24.941576, T: 60550, Avg. loss: 0.106852\n",
      "Total training time: 0.33 seconds.\n",
      "Norm: 12.89, NNZs: 330, Bias: -20.697195, T: 60550, Avg. loss: 0.285926\n",
      "Total training time: 0.32 seconds.\n",
      "Norm: 11.75, NNZs: 289, Bias: -23.964329, T: 60550, Avg. loss: 0.062103\n",
      "Total training time: 0.33 seconds.\n",
      "Norm: 12.81, NNZs: 344, Bias: -20.874081, T: 60550, Avg. loss: 0.135838\n",
      "Total training time: 0.28 seconds.\n",
      "Norm: 13.49, NNZs: 337, Bias: -18.451357, T: 60550, Avg. loss: 0.230493\n",
      "Total training time: 0.29 seconds.\n",
      "Norm: 11.76, NNZs: 326, Bias: -13.479451, T: 60550, Avg. loss: 0.195783\n",
      "Total training time: 0.31 seconds.\n",
      "Norm: 11.28, NNZs: 314, Bias: -26.491351, T: 60550, Avg. loss: 0.083205\n",
      "Total training time: 0.31 seconds.\n",
      "Norm: 10.65, NNZs: 297, Bias: -20.913843, T: 60550, Avg. loss: 0.061977\n",
      "Total training time: 0.31 seconds.\n",
      "Norm: 12.82, NNZs: 338, Bias: -12.409668, T: 60550, Avg. loss: 0.255932\n",
      "Total training time: 0.34 seconds.\n",
      "Norm: 11.20, NNZs: 309, Bias: -4.896837, T: 60550, Avg. loss: 0.162696\n",
      "Total training time: 0.30 seconds.\n",
      "Norm: 13.96, NNZs: 269, Bias: -2.016447, T: 60550, Avg. loss: 0.240476\n",
      "Total training time: 0.29 seconds.\n",
      "Accuracy on validation set:  0.7692604006163328\n",
      "--------------------\n",
      "Using dimensionality size of: 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.3s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 26.97, NNZs: 466, Bias: -35.933807, T: 12110, Avg. loss: 1.865438\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 20.06, NNZs: 433, Bias: -32.863079, T: 12110, Avg. loss: 0.711306\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 21.16, NNZs: 428, Bias: -33.884433, T: 12110, Avg. loss: 0.459489\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 26.84, NNZs: 501, Bias: -30.064658, T: 12110, Avg. loss: 1.537672\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 19.38, NNZs: 432, Bias: -25.031036, T: 12110, Avg. loss: 0.456431\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 30.21, NNZs: 464, Bias: -31.625935, T: 12110, Avg. loss: 2.376367\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 19.19, NNZs: 416, Bias: -34.675595, T: 12110, Avg. loss: 0.492133\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 24.87, NNZs: 456, Bias: -29.838544, T: 12110, Avg. loss: 0.944501\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 28.65, NNZs: 471, Bias: -18.333531, T: 12110, Avg. loss: 1.777449\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 20.35, NNZs: 447, Bias: -29.466563, T: 24220, Avg. loss: 0.483096\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 28.07, NNZs: 451, Bias: -33.298525, T: 12110, Avg. loss: 1.431511\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 19.62, NNZs: 444, Bias: -34.647863, T: 12110, Avg. loss: 0.537434\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 15.72, NNZs: 397, Bias: -29.348425, T: 24220, Avg. loss: 0.158805\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 16.35, NNZs: 388, Bias: -30.577917, T: 24220, Avg. loss: 0.094982\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 14.45, NNZs: 409, Bias: -21.944172, T: 24220, Avg. loss: 0.091099\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 19.57, NNZs: 455, Bias: -24.432103, T: 24220, Avg. loss: 0.393048\n",
      "Total training time: 0.21 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 20.01, NNZs: 418, Bias: -12.816989, T: 24220, Avg. loss: 0.357263\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 33.30, NNZs: 455, Bias: -16.000789, T: 12110, Avg. loss: 3.275428\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 20.95, NNZs: 437, Bias: -26.935636, T: 24220, Avg. loss: 0.385171\n",
      "Total training time: 0.19 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 17.41, NNZs: 429, Bias: -25.656318, T: 36330, Avg. loss: 0.363165\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 22.15, NNZs: 430, Bias: -24.352147, T: 24220, Avg. loss: 0.513197\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 18.39, NNZs: 424, Bias: -25.389336, T: 24220, Avg. loss: 0.214013\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 15.48, NNZs: 385, Bias: -30.733551, T: 24220, Avg. loss: 0.135449\n",
      "Total training time: 0.23 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 15.31, NNZs: 412, Bias: -31.299437, T: 24220, Avg. loss: 0.116115\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 14.61, NNZs: 396, Bias: -28.566893, T: 36330, Avg. loss: 0.071616\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 12.57, NNZs: 397, Bias: -20.255105, T: 36330, Avg. loss: 0.062093\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.07, NNZs: 378, Bias: -27.202890, T: 36330, Avg. loss: 0.118897\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 16.57, NNZs: 418, Bias: -9.524659, T: 36330, Avg. loss: 0.239385\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 18.03, NNZs: 438, Bias: -23.181682, T: 36330, Avg. loss: 0.283617\n",
      "Total training time: 0.28 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 15.70, NNZs: 446, Bias: -22.905569, T: 48440, Avg. loss: 0.295443\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 16.78, NNZs: 478, Bias: -20.957583, T: 36330, Avg. loss: 0.291488\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 23.26, NNZs: 422, Bias: -9.513502, T: 24220, Avg. loss: 0.577895\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 18.59, NNZs: 436, Bias: -19.921610, T: 36330, Avg. loss: 0.369312\n",
      "Total training time: 0.38 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 13.82, NNZs: 396, Bias: -29.280956, T: 36330, Avg. loss: 0.087943\n",
      "Total training time: 0.34 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.02, NNZs: 388, Bias: -28.470834, T: 36330, Avg. loss: 0.106225\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 13.46, NNZs: 391, Bias: -27.278884, T: 48440, Avg. loss: 0.060205\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 16.02, NNZs: 431, Bias: -22.594943, T: 36330, Avg. loss: 0.155661\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 13.23, NNZs: 379, Bias: -25.684432, T: 48440, Avg. loss: 0.093707\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14.55, NNZs: 424, Bias: -7.477157, T: 48440, Avg. loss: 0.190562\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11.40, NNZs: 404, Bias: -19.187341, T: 48440, Avg. loss: 0.052351\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 19.14, NNZs: 419, Bias: -6.469663, T: 36330, Avg. loss: 0.382474\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.43, NNZs: 449, Bias: -20.895870, T: 60550, Avg. loss: 0.264108\n",
      "Total training time: 0.47 seconds.\n",
      "Norm: 14.84, NNZs: 479, Bias: -18.761623, T: 48440, Avg. loss: 0.233731\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.97, NNZs: 395, Bias: -27.877374, T: 48440, Avg. loss: 0.073466\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.94, NNZs: 400, Bias: -26.177802, T: 60550, Avg. loss: 0.054069\n",
      "Total training time: 0.48 seconds.\n",
      "Norm: 16.23, NNZs: 461, Bias: -17.046771, T: 48440, Avg. loss: 0.305975\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.96, NNZs: 383, Bias: -27.022749, T: 48440, Avg. loss: 0.087616\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 16.11, NNZs: 459, Bias: -20.697624, T: 48440, Avg. loss: 0.234419\n",
      "Total training time: 0.41 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.44, NNZs: 388, Bias: -24.628341, T: 60550, Avg. loss: 0.088590\n",
      "Total training time: 0.50 seconds.\n",
      "Norm: 14.53, NNZs: 430, Bias: -20.746094, T: 48440, Avg. loss: 0.129440\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 10.84, NNZs: 398, Bias: -18.252212, T: 60550, Avg. loss: 0.044380\n",
      "Total training time: 0.47 seconds.\n",
      "Norm: 12.96, NNZs: 433, Bias: -6.108088, T: 60550, Avg. loss: 0.159392\n",
      "Total training time: 0.45 seconds.\n",
      "Norm: 13.73, NNZs: 471, Bias: -16.923104, T: 60550, Avg. loss: 0.202044\n",
      "Total training time: 0.50 seconds.\n",
      "Norm: 14.85, NNZs: 465, Bias: -18.709524, T: 60550, Avg. loss: 0.204138\n",
      "Total training time: 0.47 seconds.\n",
      "Norm: 12.21, NNZs: 399, Bias: -26.928129, T: 60550, Avg. loss: 0.065364\n",
      "Total training time: 0.50 seconds.\n",
      "Norm: 16.82, NNZs: 407, Bias: -4.855003, T: 48440, Avg. loss: 0.295633\n",
      "Total training time: 0.48 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.27, NNZs: 394, Bias: -25.923296, T: 60550, Avg. loss: 0.080578\n",
      "Total training time: 0.49 seconds.\n",
      "Norm: 14.74, NNZs: 454, Bias: -14.848278, T: 60550, Avg. loss: 0.260623\n",
      "Total training time: 0.55 seconds.\n",
      "Norm: 13.42, NNZs: 445, Bias: -19.395246, T: 60550, Avg. loss: 0.114006\n",
      "Total training time: 0.49 seconds.\n",
      "Norm: 15.28, NNZs: 420, Bias: -3.742522, T: 60550, Avg. loss: 0.254474\n",
      "Total training time: 0.54 seconds.\n",
      "Accuracy on validation set:  0.773497688751926\n",
      "--------------------\n",
      "Using dimensionality size of: 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.5s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 23.09, NNZs: 563, Bias: -37.758134, T: 12110, Avg. loss: 0.791230\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 20.99, NNZs: 527, Bias: -32.460252, T: 12110, Avg. loss: 0.495675\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 21.30, NNZs: 538, Bias: -27.415646, T: 12110, Avg. loss: 0.434518\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 21.77, NNZs: 588, Bias: -38.583473, T: 12110, Avg. loss: 0.585251\n",
      "Total training time: 0.17 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 29.40, NNZs: 583, Bias: -40.821323, T: 12110, Avg. loss: 1.097221\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 30.52, NNZs: 581, Bias: -40.516647, T: 12110, Avg. loss: 2.021235\n",
      "Total training time: 0.18 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 34.89, NNZs: 600, Bias: -38.753111, T: 12110, Avg. loss: 2.533070\n",
      "Total training time: 0.20 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 32.19, NNZs: 608, Bias: -38.341236, T: 12110, Avg. loss: 1.589279\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 17.72, NNZs: 533, Bias: -34.155828, T: 24220, Avg. loss: 0.171360\n",
      "Total training time: 0.22 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 37.47, NNZs: 591, Bias: -17.533676, T: 12110, Avg. loss: 3.351476\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 23.42, NNZs: 549, Bias: -37.192487, T: 12110, Avg. loss: 0.526486\n",
      "Total training time: 0.24 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 32.81, NNZs: 595, Bias: -20.673282, T: 12110, Avg. loss: 1.977902\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 30.45, NNZs: 598, Bias: -33.828960, T: 12110, Avg. loss: 1.772049\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 16.10, NNZs: 477, Bias: -28.675274, T: 24220, Avg. loss: 0.125373\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 15.95, NNZs: 503, Bias: -24.673359, T: 24220, Avg. loss: 0.090649\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 17.12, NNZs: 542, Bias: -34.892311, T: 24220, Avg. loss: 0.122453\n",
      "Total training time: 0.29 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 15.71, NNZs: 518, Bias: -31.857266, T: 36330, Avg. loss: 0.124653\n",
      "Total training time: 0.32 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 22.27, NNZs: 582, Bias: -35.417078, T: 24220, Avg. loss: 0.254145\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 23.83, NNZs: 568, Bias: -31.731056, T: 24220, Avg. loss: 0.401185\n",
      "Total training time: 0.27 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 25.66, NNZs: 572, Bias: -30.975257, T: 24220, Avg. loss: 0.561577\n",
      "Total training time: 0.33 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 17.53, NNZs: 509, Bias: -33.978851, T: 24220, Avg. loss: 0.101322\n",
      "Total training time: 0.35 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 26.07, NNZs: 557, Bias: -11.060552, T: 24220, Avg. loss: 0.602483\n",
      "Total training time: 0.30 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 13.78, NNZs: 474, Bias: -22.955197, T: 36330, Avg. loss: 0.060745\n",
      "Total training time: 0.37 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.19, NNZs: 468, Bias: -26.632288, T: 36330, Avg. loss: 0.089698\n",
      "Total training time: 0.36 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 21.99, NNZs: 557, Bias: -28.207371, T: 24220, Avg. loss: 0.403695\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 22.57, NNZs: 531, Bias: -14.964837, T: 24220, Avg. loss: 0.373481\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 15.17, NNZs: 525, Bias: -32.934706, T: 36330, Avg. loss: 0.089856\n",
      "Total training time: 0.40 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.75, NNZs: 508, Bias: -30.297796, T: 48440, Avg. loss: 0.101597\n",
      "Total training time: 0.42 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 20.66, NNZs: 569, Bias: -27.780791, T: 36330, Avg. loss: 0.282570\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 19.65, NNZs: 564, Bias: -32.193971, T: 36330, Avg. loss: 0.185850\n",
      "Total training time: 0.39 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 15.86, NNZs: 513, Bias: -31.812902, T: 36330, Avg. loss: 0.075774\n",
      "Total training time: 0.45 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 12.51, NNZs: 467, Bias: -21.865315, T: 48440, Avg. loss: 0.048176\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 22.97, NNZs: 563, Bias: -33.988124, T: 24220, Avg. loss: 0.510116\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 21.82, NNZs: 578, Bias: -26.208271, T: 36330, Avg. loss: 0.409622\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 13.26, NNZs: 475, Bias: -25.162472, T: 48440, Avg. loss: 0.073789\n",
      "Total training time: 0.47 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 21.15, NNZs: 550, Bias: -7.826994, T: 36330, Avg. loss: 0.393468\n",
      "Total training time: 0.43 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 14.39, NNZs: 517, Bias: -31.467217, T: 48440, Avg. loss: 0.076099\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 18.37, NNZs: 537, Bias: -11.664909, T: 36330, Avg. loss: 0.248536\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 13.91, NNZs: 515, Bias: -29.196333, T: 60550, Avg. loss: 0.093455\n",
      "Total training time: 0.53 seconds.\n",
      "Norm: 18.81, NNZs: 560, Bias: -24.695651, T: 36330, Avg. loss: 0.294438\n",
      "Total training time: 0.53 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 18.55, NNZs: 577, Bias: -25.260850, T: 48440, Avg. loss: 0.234702\n",
      "Total training time: 0.50 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14.69, NNZs: 490, Bias: -30.435234, T: 48440, Avg. loss: 0.059769\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 11.96, NNZs: 470, Bias: -20.889357, T: 60550, Avg. loss: 0.042638\n",
      "Total training time: 0.58 seconds.\n",
      "Norm: 18.00, NNZs: 564, Bias: -30.087055, T: 48440, Avg. loss: 0.147145\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 12.46, NNZs: 466, Bias: -24.158439, T: 60550, Avg. loss: 0.066958\n",
      "Total training time: 0.58 seconds.\n",
      "Norm: 13.73, NNZs: 525, Bias: -30.413433, T: 60550, Avg. loss: 0.064772\n",
      "Total training time: 0.61 seconds.\n",
      "Norm: 18.41, NNZs: 552, Bias: -5.846245, T: 48440, Avg. loss: 0.302122\n",
      "Total training time: 0.54 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 19.95, NNZs: 566, Bias: -30.010384, T: 36330, Avg. loss: 0.369881\n",
      "Total training time: 0.61 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 19.35, NNZs: 589, Bias: -23.086488, T: 48440, Avg. loss: 0.331139\n",
      "Total training time: 0.63 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 16.15, NNZs: 546, Bias: -9.467048, T: 48440, Avg. loss: 0.200402\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 17.05, NNZs: 574, Bias: -22.212137, T: 48440, Avg. loss: 0.237462\n",
      "Total training time: 0.64 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 17.17, NNZs: 571, Bias: -23.277365, T: 60550, Avg. loss: 0.202563\n",
      "Total training time: 0.60 seconds.\n",
      "Norm: 14.08, NNZs: 483, Bias: -29.302245, T: 60550, Avg. loss: 0.052332\n",
      "Total training time: 0.66 seconds.\n",
      "Norm: 16.60, NNZs: 570, Bias: -4.679653, T: 60550, Avg. loss: 0.257540\n",
      "Total training time: 0.63 seconds.\n",
      "Norm: 16.86, NNZs: 575, Bias: -28.493037, T: 60550, Avg. loss: 0.129396\n",
      "Total training time: 0.65 seconds.\n",
      "Norm: 17.91, NNZs: 583, Bias: -27.310967, T: 48440, Avg. loss: 0.311526\n",
      "Total training time: 0.70 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 14.41, NNZs: 562, Bias: -7.957366, T: 60550, Avg. loss: 0.169281\n",
      "Total training time: 0.72 seconds.\n",
      "Norm: 15.79, NNZs: 575, Bias: -20.290007, T: 60550, Avg. loss: 0.205015\n",
      "Total training time: 0.72 seconds.\n",
      "Norm: 17.60, NNZs: 588, Bias: -20.760324, T: 60550, Avg. loss: 0.287696\n",
      "Total training time: 0.73 seconds.\n",
      "Norm: 16.68, NNZs: 583, Bias: -25.179612, T: 60550, Avg. loss: 0.268961\n",
      "Total training time: 0.76 seconds.\n",
      "Accuracy on validation set:  0.7750385208012327\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.6s remaining:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "# tune the dimensionality of the feature vector\n",
    "hyper_param_values = [20, 50, 100, 200, 300, 500, 700, 900]\n",
    "accuracies = []\n",
    "\n",
    "for hyper_param_value in hyper_param_values:\n",
    "    \n",
    "    print(f\"Using dimensionality size of: {hyper_param_value}\")\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=hyper_param_value, n_iter=7, random_state=42)\n",
    "    svd.fit(bm25_matrix_train)\n",
    "\n",
    "    bm25_matrix_train_lsa_x = svd.transform(bm25_matrix_train)\n",
    "    bm25_matrix_val_lsa_x = svd.transform(bm25_matrix_val)\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = SGDClassifier(loss='hinge', penalty='elasticnet', alpha=1e-3, random_state=42, max_iter=5, tol=None, n_jobs=-1, verbose=1)\n",
    "    clf.fit(bm25_matrix_train_lsa_x, y_train)\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    y_pred = clf.predict(bm25_matrix_val_lsa_x)\n",
    "    acc_score = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(acc_score)\n",
    "    print('Accuracy on validation set: ', acc_score)\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studied hyper-parameters and the corresponding accuracies on the validation set: \n",
      "Dimensionality size: 20, Accuracy: 0.7408\n",
      "Dimensionality size: 50, Accuracy: 0.7646\n",
      "Dimensionality size: 100, Accuracy: 0.7758\n",
      "Dimensionality size: 200, Accuracy: 0.7673\n",
      "Dimensionality size: 300, Accuracy: 0.7689\n",
      "Dimensionality size: 500, Accuracy: 0.7693\n",
      "Dimensionality size: 700, Accuracy: 0.7735\n",
      "Dimensionality size: 900, Accuracy: 0.7750\n"
     ]
    }
   ],
   "source": [
    "print(\"Studied hyper-parameters and the corresponding accuracies on the validation set: \")\n",
    "_ = [print(f\"Dimensionality size: {param:.0f}, Accuracy: {acc:.4f}\") for param, acc in zip(hyper_param_values, accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dimensionality size: 100\n"
     ]
    }
   ],
   "source": [
    "best_dim_size = hyper_param_values[np.argmax(accuracies)]\n",
    "print(f\"Selected dimensionality size: {best_dim_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 11.94, NNZs: 58, Bias: -4.281558, T: 12110, Avg. loss: 0.838108\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 10.19, NNZs: 62, Bias: -18.808293, T: 12110, Avg. loss: 0.222068\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 7.83, NNZs: 67, Bias: -16.049397, T: 24220, Avg. loss: 0.087981\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "-- Epoch 1\n",
      "Norm: 11.30, NNZs: 65, Bias: -15.777536, T: 12110, Avg. loss: 0.899397\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "-- Epoch 1\n",
      "Norm: 8.31, NNZs: 71, Bias: -13.606805, T: 12110, Avg. loss: 0.253916\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "-- Epoch 2\n",
      "Norm: 13.63, NNZs: 60, Bias: -1.864232, T: 12110, Avg. loss: 1.307735\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 8.47, NNZs: 65, Bias: -17.711554, T: 12110, Avg. loss: 0.290663\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "-- Epoch 1\n",
      "Norm: 8.93, NNZs: 64, Bias: -15.423417, T: 12110, Avg. loss: 0.370383\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.94, NNZs: 71, Bias: -14.358876, T: 36330, Avg. loss: 0.074230\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 11.47, NNZs: 63, Bias: -8.925262, T: 12110, Avg. loss: 0.759340\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 12.39, NNZs: 62, Bias: -9.942181, T: 12110, Avg. loss: 1.126283\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.76, NNZs: 59, Bias: -12.135191, T: 24220, Avg. loss: 0.122783\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.38, NNZs: 72, Bias: -10.810345, T: 24220, Avg. loss: 0.084942\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.51, NNZs: 65, Bias: -16.728900, T: 12110, Avg. loss: 0.288545\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 10.40, NNZs: 67, Bias: -13.102497, T: 12110, Avg. loss: 0.518820\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.37, NNZs: 63, Bias: -14.611463, T: 24220, Avg. loss: 0.099741\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 8.71, NNZs: 52, Bias: -9.255017, T: 24220, Avg. loss: 0.265963\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 10.10, NNZs: 58, Bias: -2.004098, T: 24220, Avg. loss: 0.187179\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 11.36, NNZs: 57, Bias: -0.846721, T: 24220, Avg. loss: 0.299501\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 12.08, NNZs: 64, Bias: -15.918327, T: 12110, Avg. loss: 0.808903\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.34, NNZs: 74, Bias: -13.229567, T: 48440, Avg. loss: 0.066643\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.93, NNZs: 54, Bias: -4.399748, T: 24220, Avg. loss: 0.202585\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.82, NNZs: 62, Bias: -10.145593, T: 36330, Avg. loss: 0.097403\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.01, NNZs: 66, Bias: -13.531828, T: 24220, Avg. loss: 0.093925\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.74, NNZs: 56, Bias: -4.569797, T: 24220, Avg. loss: 0.264803\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.82, NNZs: 70, Bias: -8.728618, T: 24220, Avg. loss: 0.161479\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.69, NNZs: 72, Bias: -12.690215, T: 36330, Avg. loss: 0.084103\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.32, NNZs: 50, Bias: -5.769649, T: 36330, Avg. loss: 0.187721\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.62, NNZs: 70, Bias: -9.124537, T: 36330, Avg. loss: 0.069379\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.89, NNZs: 64, Bias: -0.822843, T: 36330, Avg. loss: 0.249751\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.59, NNZs: 59, Bias: -1.326977, T: 36330, Avg. loss: 0.161200\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.06, NNZs: 63, Bias: -9.586902, T: 24220, Avg. loss: 0.257752\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.11, NNZs: 73, Bias: -12.239260, T: 60550, Avg. loss: 0.061613\n",
      "Total training time: 0.09 seconds.\n",
      "Norm: 5.39, NNZs: 61, Bias: -8.676982, T: 48440, Avg. loss: 0.084453\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.42, NNZs: 71, Bias: -6.455354, T: 36330, Avg. loss: 0.124340\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.00, NNZs: 57, Bias: -2.598018, T: 36330, Avg. loss: 0.144331\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 8.60, NNZs: 59, Bias: -2.440379, T: 36330, Avg. loss: 0.191654\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.17, NNZs: 70, Bias: -11.384714, T: 48440, Avg. loss: 0.072960\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.03, NNZs: 70, Bias: -11.735247, T: 36330, Avg. loss: 0.079894\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.81, NNZs: 69, Bias: -8.086732, T: 48440, Avg. loss: 0.061123\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.56, NNZs: 53, Bias: -3.669870, T: 48440, Avg. loss: 0.148437\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.68, NNZs: 71, Bias: -4.918868, T: 48440, Avg. loss: 0.103302\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.86, NNZs: 59, Bias: -0.773664, T: 48440, Avg. loss: 0.233404\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.77, NNZs: 58, Bias: -1.230467, T: 48440, Avg. loss: 0.147695\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.74, NNZs: 66, Bias: -7.784584, T: 60550, Avg. loss: 0.076921\n",
      "Total training time: 0.08 seconds.\n",
      "Norm: 8.03, NNZs: 59, Bias: -2.046810, T: 48440, Avg. loss: 0.166459\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.47, NNZs: 70, Bias: -6.447496, T: 36330, Avg. loss: 0.191894\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.46, NNZs: 75, Bias: -7.202205, T: 60550, Avg. loss: 0.057414\n",
      "Total training time: 0.09 seconds.\n",
      "Norm: 7.10, NNZs: 50, Bias: -1.832904, T: 48440, Avg. loss: 0.130718\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.05, NNZs: 73, Bias: -3.932731, T: 60550, Avg. loss: 0.092261\n",
      "Total training time: 0.06 seconds.\n",
      "Norm: 6.06, NNZs: 50, Bias: -2.315002, T: 60550, Avg. loss: 0.124726\n",
      "Total training time: 0.08 seconds.\n",
      "Norm: 4.75, NNZs: 70, Bias: -10.396109, T: 60550, Avg. loss: 0.067758\n",
      "Total training time: 0.08 seconds.\n",
      "Norm: 5.50, NNZs: 73, Bias: -10.474234, T: 48440, Avg. loss: 0.069861\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.14, NNZs: 62, Bias: -0.722050, T: 60550, Avg. loss: 0.221135\n",
      "Total training time: 0.08 seconds.\n",
      "Norm: 7.47, NNZs: 64, Bias: -1.817240, T: 60550, Avg. loss: 0.161102\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 7.02, NNZs: 63, Bias: -1.225653, T: 60550, Avg. loss: 0.139158\n",
      "Total training time: 0.12 seconds.\n",
      "Norm: 6.45, NNZs: 67, Bias: -4.667859, T: 48440, Avg. loss: 0.159324\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.46, NNZs: 60, Bias: -1.497662, T: 60550, Avg. loss: 0.121076\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 5.13, NNZs: 76, Bias: -9.528881, T: 60550, Avg. loss: 0.064521\n",
      "Total training time: 0.07 seconds.\n",
      "Norm: 5.92, NNZs: 66, Bias: -3.452663, T: 60550, Avg. loss: 0.138719\n",
      "Total training time: 0.07 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  12 | elapsed:    0.1s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, max_iter=5, n_jobs=-1, penalty='elasticnet',\n",
       "              random_state=42, tol=None, verbose=1)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit classifier on data with the best dimensionality size\n",
    "svd = TruncatedSVD(n_components=best_dim_size, n_iter=7, random_state=42)\n",
    "svd.fit(bm25_matrix_train)\n",
    "\n",
    "bm25_matrix_train_lsa_tuned = svd.transform(bm25_matrix_train)\n",
    "bm25_matrix_val_lsa_tuned = svd.transform(bm25_matrix_val)\n",
    "bm25_matrix_test_lsa_tuned = svd.transform(bm25_matrix_test)\n",
    "\n",
    "clfs[3] = SGDClassifier(loss='hinge', penalty='elasticnet', alpha=1e-3, random_state=42, max_iter=5, tol=None, n_jobs=-1, verbose=1)\n",
    "clfs[3].fit(bm25_matrix_train_lsa_tuned, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.775808936825886\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.08      0.13        53\n",
      "           1       0.42      0.16      0.23       122\n",
      "           2       0.87      0.83      0.85       140\n",
      "           3       0.77      0.86      0.82       420\n",
      "           4       0.88      0.90      0.89       665\n",
      "           5       0.52      0.38      0.44       113\n",
      "           6       0.40      0.07      0.12        28\n",
      "           7       0.67      0.63      0.65        38\n",
      "           8       0.88      0.75      0.81       102\n",
      "           9       0.73      0.89      0.80       546\n",
      "          10       0.67      0.70      0.69       195\n",
      "          11       0.80      0.82      0.81       174\n",
      "\n",
      "    accuracy                           0.78      2596\n",
      "   macro avg       0.68      0.59      0.60      2596\n",
      "weighted avg       0.76      0.78      0.76      2596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on validation set\n",
    "y_preds[3] = clfs[3].predict(bm25_matrix_val_lsa_tuned)\n",
    "val_accs[3] = accuracy_score(y_val, y_preds[3])\n",
    "print('Accuracy on validation set: ', val_accs[3])\n",
    "\n",
    "# additionally, we check the classification report\n",
    "# look at F1-Score, Precision, Recall, and relationship between Support & metrics\n",
    "print(classification_report(y_val, y_preds[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5: TF-IDF, high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier with hidden layer size of: (100,)\n",
      "Accuracy on validation set:  0.7765793528505393\n",
      "--------------------\n",
      "Classifier with hidden layer size of: (200,)\n",
      "Accuracy on validation set:  0.7827426810477658\n",
      "--------------------\n",
      "Classifier with hidden layer size of: (300,)\n",
      "Accuracy on validation set:  0.785824345146379\n",
      "--------------------\n",
      "Classifier with hidden layer size of: (500,)\n",
      "Accuracy on validation set:  0.7711864406779662\n",
      "--------------------\n",
      "Classifier with hidden layer size of: (300, 100)\n",
      "Accuracy on validation set:  0.7862095531587057\n",
      "--------------------\n",
      "Classifier with hidden layer size of: (500, 300)\n",
      "Accuracy on validation set:  0.7854391371340523\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# tune hidden layer sizes\n",
    "hyper_param_values = [(100,), (200,), (300,), (500,), (300, 100,), (500, 300,)]\n",
    "accuracies = []\n",
    "\n",
    "for hyper_param_value in hyper_param_values:\n",
    "    \n",
    "    print(f\"Classifier with hidden layer size of: {hyper_param_value}\")\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = MLPClassifier(random_state=42, hidden_layer_sizes=hyper_param_value, max_iter=100, learning_rate_init=0.001)\n",
    "    clf.fit(tf_idf_matrix_train, y_train)\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    y_pred = clf.predict(tf_idf_matrix_val)\n",
    "    acc_score = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(acc_score)\n",
    "    print('Accuracy on validation set: ', acc_score)\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected hidden layer size: (300, 100)\n"
     ]
    }
   ],
   "source": [
    "best_hidden_layer_size = hyper_param_values[np.argmax(accuracies)]\n",
    "print(f\"Selected hidden layer size: {best_hidden_layer_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(300, 100), max_iter=100, random_state=42)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit classifier with best hyper-parameter\n",
    "clfs[4] = MLPClassifier(random_state=42, hidden_layer_sizes=best_hidden_layer_size, max_iter=100, learning_rate_init=0.001)\n",
    "clfs[4].fit(tf_idf_matrix_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7862095531587057\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.32      0.41        53\n",
      "           1       0.41      0.21      0.28       122\n",
      "           2       0.89      0.83      0.86       140\n",
      "           3       0.79      0.86      0.82       420\n",
      "           4       0.90      0.90      0.90       665\n",
      "           5       0.57      0.52      0.54       113\n",
      "           6       0.62      0.29      0.39        28\n",
      "           7       0.68      0.68      0.68        38\n",
      "           8       0.89      0.78      0.83       102\n",
      "           9       0.76      0.85      0.80       546\n",
      "          10       0.66      0.74      0.70       195\n",
      "          11       0.80      0.81      0.81       174\n",
      "\n",
      "    accuracy                           0.79      2596\n",
      "   macro avg       0.71      0.65      0.67      2596\n",
      "weighted avg       0.78      0.79      0.78      2596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on validation set\n",
    "y_preds[4] = clfs[4].predict(tf_idf_matrix_val)\n",
    "val_accs[4] = accuracy_score(y_val, y_preds[4])\n",
    "print('Accuracy on validation set: ', val_accs[4])\n",
    "\n",
    "# additionally, we check the classification report\n",
    "# look at F1-Score, Precision, Recall, and relationship between Support & metrics\n",
    "print(classification_report(y_val, y_preds[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6: TF-IDF, low-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier with a learning rate of 0.002.\n",
      "Accuracy on validation set:  0.7773497688751926\n",
      "--------------------\n",
      "Classifier with a learning rate of 0.001.\n",
      "Accuracy on validation set:  0.7708012326656395\n",
      "--------------------\n",
      "Classifier with a learning rate of 0.0005.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7719568567026194\n",
      "--------------------\n",
      "Classifier with a learning rate of 0.0003.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7800462249614792\n",
      "--------------------\n",
      "Classifier with a learning rate of 0.0001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.8046995377503852\n",
      "--------------------\n",
      "Classifier with a learning rate of 1e-05.\n",
      "Accuracy on validation set:  0.7831278890600925\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tune hyper-parameter learning_rate\n",
    "hyper_param_values = [2e-3, 1e-3, 5e-4, 3e-4, 1e-4, 1e-5]\n",
    "accuracies = []\n",
    "\n",
    "for hyper_param_value in hyper_param_values:\n",
    "    \n",
    "    print(f\"Classifier with a learning rate of {hyper_param_value}.\")\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = MLPClassifier(random_state=42, hidden_layer_sizes=best_hidden_layer_size, max_iter=100, learning_rate_init=hyper_param_value)\n",
    "    clf.fit(tf_idf_matrix_train_lsa, y_train)\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    y_pred = clf.predict(tf_idf_matrix_val_lsa)\n",
    "    acc_score = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(acc_score)\n",
    "    print('Accuracy on validation set: ', acc_score)\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studied hyper-parameters and the corresponding accuracies on the validation set: \n",
      "learning_rate_init: 0.0020, Accuracy: 0.7773\n",
      "learning_rate_init: 0.0010, Accuracy: 0.7708\n",
      "learning_rate_init: 0.0005, Accuracy: 0.7720\n",
      "learning_rate_init: 0.0003, Accuracy: 0.7800\n",
      "learning_rate_init: 0.0001, Accuracy: 0.8047\n",
      "learning_rate_init: 0.0000, Accuracy: 0.7831\n"
     ]
    }
   ],
   "source": [
    "print(\"Studied hyper-parameters and the corresponding accuracies on the validation set: \")\n",
    "_ = [print(f\"learning_rate_init: {param:.4f}, Accuracy: {acc:.4f}\") for param, acc in zip(hyper_param_values, accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected learning_rate_init: 0.0001\n"
     ]
    }
   ],
   "source": [
    "best_lr_init = hyper_param_values[np.argmax(accuracies)]\n",
    "print(f\"Selected learning_rate_init: {best_lr_init}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(300, 100), learning_rate_init=0.0001,\n",
       "              max_iter=100, random_state=42)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit classifier with best hyper-parameter\n",
    "clfs[5] = MLPClassifier(random_state=42, hidden_layer_sizes=best_hidden_layer_size, max_iter=100, learning_rate_init=best_lr_init)\n",
    "clfs[5].fit(tf_idf_matrix_train_lsa, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.8046995377503852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.43      0.51        53\n",
      "           1       0.44      0.26      0.33       122\n",
      "           2       0.90      0.89      0.89       140\n",
      "           3       0.83      0.87      0.85       420\n",
      "           4       0.91      0.91      0.91       665\n",
      "           5       0.65      0.48      0.55       113\n",
      "           6       0.56      0.36      0.43        28\n",
      "           7       0.60      0.74      0.66        38\n",
      "           8       0.86      0.81      0.84       102\n",
      "           9       0.79      0.87      0.83       546\n",
      "          10       0.65      0.74      0.69       195\n",
      "          11       0.84      0.84      0.84       174\n",
      "\n",
      "    accuracy                           0.80      2596\n",
      "   macro avg       0.72      0.68      0.69      2596\n",
      "weighted avg       0.80      0.80      0.80      2596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on validation set\n",
    "y_preds[5] = clfs[5].predict(tf_idf_matrix_val_lsa)\n",
    "val_accs[5] = accuracy_score(y_val, y_preds[5])\n",
    "print('Accuracy on validation set: ', val_accs[5])\n",
    "\n",
    "# additionally, we check the classification report\n",
    "# look at F1-Score, Precision, Recall, and relationship between Support & metrics\n",
    "print(classification_report(y_val, y_preds[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7: BM25, high-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier with 10 maximum iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7931432973805855\n",
      "--------------------\n",
      "Classifier with 20 maximum iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7854391371340523\n",
      "--------------------\n",
      "Classifier with 30 maximum iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7827426810477658\n",
      "--------------------\n",
      "Classifier with 50 maximum iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7827426810477658\n",
      "--------------------\n",
      "Classifier with 100 maximum iterations.\n",
      "Accuracy on validation set:  0.7823574730354391\n",
      "--------------------\n",
      "Classifier with 200 maximum iterations.\n",
      "Accuracy on validation set:  0.7823574730354391\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# tune hyper-parameter max_iter\n",
    "hyper_param_values = [10, 20, 30, 50, 100, 200]\n",
    "accuracies = []\n",
    "\n",
    "for hyper_param_value in hyper_param_values:\n",
    "    \n",
    "    print(f\"Classifier with {hyper_param_value} maximum iterations.\")\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = MLPClassifier(random_state=42, hidden_layer_sizes=best_hidden_layer_size, max_iter=hyper_param_value, learning_rate_init=best_lr_init)\n",
    "    clf.fit(bm25_matrix_train, y_train)\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    y_pred = clf.predict(bm25_matrix_val)\n",
    "    acc_score = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(acc_score)\n",
    "    print('Accuracy on validation set: ', acc_score)\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studied hyper-parameters and the corresponding accuracies on the validation set: \n",
      "max_iter: 10.0000, Accuracy: 0.7931\n",
      "max_iter: 20.0000, Accuracy: 0.7854\n",
      "max_iter: 30.0000, Accuracy: 0.7827\n",
      "max_iter: 50.0000, Accuracy: 0.7827\n",
      "max_iter: 100.0000, Accuracy: 0.7824\n",
      "max_iter: 200.0000, Accuracy: 0.7824\n"
     ]
    }
   ],
   "source": [
    "print(\"Studied hyper-parameters and the corresponding accuracies on the validation set: \")\n",
    "_ = [print(f\"max_iter: {param:.4f}, Accuracy: {acc:.4f}\") for param, acc in zip(hyper_param_values, accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected max_iter: 10\n"
     ]
    }
   ],
   "source": [
    "best_max_iter = hyper_param_values[np.argmax(accuracies)]\n",
    "print(f\"Selected max_iter: {best_max_iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(300, 100), learning_rate_init=0.0001,\n",
       "              max_iter=10, random_state=42)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit classifier with best hyper-parameter\n",
    "clfs[6] = MLPClassifier(random_state=42, hidden_layer_sizes=best_hidden_layer_size, max_iter=best_max_iter, learning_rate_init=best_lr_init)\n",
    "clfs[6].fit(bm25_matrix_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7931432973805855\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.30      0.40        53\n",
      "           1       0.41      0.24      0.30       122\n",
      "           2       0.91      0.84      0.88       140\n",
      "           3       0.77      0.86      0.82       420\n",
      "           4       0.90      0.91      0.90       665\n",
      "           5       0.63      0.44      0.52       113\n",
      "           6       0.50      0.21      0.30        28\n",
      "           7       0.67      0.58      0.62        38\n",
      "           8       0.91      0.80      0.85       102\n",
      "           9       0.77      0.88      0.82       546\n",
      "          10       0.70      0.74      0.72       195\n",
      "          11       0.80      0.83      0.81       174\n",
      "\n",
      "    accuracy                           0.79      2596\n",
      "   macro avg       0.71      0.64      0.66      2596\n",
      "weighted avg       0.78      0.79      0.78      2596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on validation set\n",
    "y_preds[6] = clfs[6].predict(bm25_matrix_val)\n",
    "val_accs[6] = accuracy_score(y_val, y_preds[6])\n",
    "print('Accuracy on validation set: ', val_accs[6])\n",
    "\n",
    "# additionally, we check the classification report\n",
    "# look at F1-Score, Precision, Recall, and relationship between Support & metrics\n",
    "print(classification_report(y_val, y_preds[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 8: BM25, low-dimensional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dimensionality size of: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7542372881355932\n",
      "--------------------\n",
      "Using dimensionality size of: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7854391371340523\n",
      "--------------------\n",
      "Using dimensionality size of: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7850539291217258\n",
      "--------------------\n",
      "Using dimensionality size of: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7896764252696457\n",
      "--------------------\n",
      "Using dimensionality size of: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.789291217257319\n",
      "--------------------\n",
      "Using dimensionality size of: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7950693374422187\n",
      "--------------------\n",
      "Using dimensionality size of: 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7954545454545454\n",
      "--------------------\n",
      "Using dimensionality size of: 900\n",
      "Accuracy on validation set:  0.7927580893682589\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tune dimensionality of feature vectors\n",
    "hyper_param_values = [20, 50, 100, 200, 300, 500, 700, 900]\n",
    "accuracies = []\n",
    "\n",
    "for hyper_param_value in hyper_param_values:\n",
    "    \n",
    "    print(f\"Using dimensionality size of: {hyper_param_value}\")\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=hyper_param_value, n_iter=7, random_state=42)\n",
    "    svd.fit(bm25_matrix_train)\n",
    "\n",
    "    bm25_matrix_train_lsa_x = svd.transform(bm25_matrix_train)\n",
    "    bm25_matrix_val_lsa_x = svd.transform(bm25_matrix_val)\n",
    "    \n",
    "    # fit classifier\n",
    "    clf = MLPClassifier(random_state=42, hidden_layer_sizes=best_hidden_layer_size, max_iter=best_max_iter, learning_rate_init=best_lr_init)\n",
    "    clf.fit(bm25_matrix_train_lsa_x, y_train)\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    y_pred = clf.predict(bm25_matrix_val_lsa_x)\n",
    "    acc_score = accuracy_score(y_val, y_pred)\n",
    "    accuracies.append(acc_score)\n",
    "    print('Accuracy on validation set: ', acc_score)\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studied hyper-parameters and the corresponding accuracies on the validation set: \n",
      "Dimensionality size: 20, Accuracy: 0.7542\n",
      "Dimensionality size: 50, Accuracy: 0.7854\n",
      "Dimensionality size: 100, Accuracy: 0.7851\n",
      "Dimensionality size: 200, Accuracy: 0.7897\n",
      "Dimensionality size: 300, Accuracy: 0.7893\n",
      "Dimensionality size: 500, Accuracy: 0.7951\n",
      "Dimensionality size: 700, Accuracy: 0.7955\n",
      "Dimensionality size: 900, Accuracy: 0.7928\n"
     ]
    }
   ],
   "source": [
    "print(\"Studied hyper-parameters and the corresponding accuracies on the validation set: \")\n",
    "_ = [print(f\"Dimensionality size: {param:.0f}, Accuracy: {acc:.4f}\") for param, acc in zip(hyper_param_values, accuracies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected dimensionality size: 700\n"
     ]
    }
   ],
   "source": [
    "best_dim_size = hyper_param_values[np.argmax(accuracies)]\n",
    "print(f\"Selected dimensionality size: {best_dim_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/hands-on-ai/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(300, 100), learning_rate_init=0.0001,\n",
       "              max_iter=10, random_state=42)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit classifier on data with the best dimensionality size\n",
    "svd = TruncatedSVD(n_components=best_dim_size, n_iter=7, random_state=42)\n",
    "svd.fit(bm25_matrix_train)\n",
    "\n",
    "bm25_matrix_train_lsa_tuned_exp_8 = svd.transform(bm25_matrix_train)\n",
    "bm25_matrix_val_lsa_tuned_exp_8 = svd.transform(bm25_matrix_val)\n",
    "bm25_matrix_test_lsa_tuned_exp_8 = svd.transform(bm25_matrix_test)\n",
    "\n",
    "clfs[7] = MLPClassifier(random_state=42, hidden_layer_sizes=best_hidden_layer_size, max_iter=best_max_iter, learning_rate_init=best_lr_init)\n",
    "clfs[7].fit(bm25_matrix_train_lsa_tuned_exp_8, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set:  0.7954545454545454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.26      0.39        53\n",
      "           1       0.48      0.20      0.29       122\n",
      "           2       0.89      0.81      0.85       140\n",
      "           3       0.79      0.88      0.83       420\n",
      "           4       0.89      0.91      0.90       665\n",
      "           5       0.62      0.49      0.55       113\n",
      "           6       0.50      0.14      0.22        28\n",
      "           7       0.72      0.55      0.63        38\n",
      "           8       0.89      0.76      0.82       102\n",
      "           9       0.75      0.89      0.82       546\n",
      "          10       0.69      0.74      0.71       195\n",
      "          11       0.83      0.86      0.85       174\n",
      "\n",
      "    accuracy                           0.80      2596\n",
      "   macro avg       0.73      0.63      0.65      2596\n",
      "weighted avg       0.78      0.80      0.78      2596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on validation set\n",
    "y_preds[7] = clfs[7].predict(bm25_matrix_val_lsa_tuned_exp_8)\n",
    "val_accs[7] = accuracy_score(y_val, y_preds[7])\n",
    "print('Accuracy on validation set: ', val_accs[7])\n",
    "\n",
    "# additionally, we check the classification report\n",
    "# look at F1-Score, Precision, Recall, and relationship between Support & metrics\n",
    "print(classification_report(y_val, y_preds[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation, reporting results and discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets = [tf_idf_matrix_test, tf_idf_matrix_test_lsa, bm25_matrix_test, bm25_matrix_test_lsa_tuned, tf_idf_matrix_test, tf_idf_matrix_test_lsa, bm25_matrix_test, bm25_matrix_test_lsa_tuned_exp_8]\n",
    "\n",
    "y_val_preds = y_preds\n",
    "y_test_preds = [clf.predict(test_set) for clf, test_set in zip(clfs, test_sets)]\n",
    "test_accs = [accuracy_score(y_test_pred, y_test) for y_test_pred in y_test_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXP 1</th>\n",
       "      <th>EXP 2</th>\n",
       "      <th>EXP 3</th>\n",
       "      <th>EXP 4</th>\n",
       "      <th>EXP 5</th>\n",
       "      <th>EXP 6</th>\n",
       "      <th>EXP 7</th>\n",
       "      <th>EXP 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>0.798536</td>\n",
       "      <td>0.788521</td>\n",
       "      <td>0.788521</td>\n",
       "      <td>0.775809</td>\n",
       "      <td>0.786210</td>\n",
       "      <td>0.804700</td>\n",
       "      <td>0.793143</td>\n",
       "      <td>0.795455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.790366</td>\n",
       "      <td>0.782659</td>\n",
       "      <td>0.780732</td>\n",
       "      <td>0.764933</td>\n",
       "      <td>0.793064</td>\n",
       "      <td>0.803083</td>\n",
       "      <td>0.805395</td>\n",
       "      <td>0.793834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         EXP 1     EXP 2     EXP 3     EXP 4     EXP 5     EXP 6     EXP 7  \\\n",
       "val   0.798536  0.788521  0.788521  0.775809  0.786210  0.804700  0.793143   \n",
       "test  0.790366  0.782659  0.780732  0.764933  0.793064  0.803083  0.805395   \n",
       "\n",
       "         EXP 8  \n",
       "val   0.795455  \n",
       "test  0.793834  "
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "val_accs_df = pd.DataFrame([val_accs], columns=['EXP 1', 'EXP 2', 'EXP 3', 'EXP 4', 'EXP 5', 'EXP 6', 'EXP 7', 'EXP 8'], index=['val'])\n",
    "test_accs_df = pd.DataFrame([test_accs], columns=['EXP 1', 'EXP 2', 'EXP 3', 'EXP 4', 'EXP 5', 'EXP 6', 'EXP 7', 'EXP 8'], index=['test'])\n",
    "accs_df = pd.concat([val_accs_df, test_accs_df])\n",
    "accs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EXP 1</th>\n",
       "      <td>0.798536</td>\n",
       "      <td>0.790366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXP 2</th>\n",
       "      <td>0.788521</td>\n",
       "      <td>0.782659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXP 3</th>\n",
       "      <td>0.788521</td>\n",
       "      <td>0.780732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXP 4</th>\n",
       "      <td>0.775809</td>\n",
       "      <td>0.764933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXP 5</th>\n",
       "      <td>0.786210</td>\n",
       "      <td>0.793064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXP 6</th>\n",
       "      <td>0.804700</td>\n",
       "      <td>0.803083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXP 7</th>\n",
       "      <td>0.793143</td>\n",
       "      <td>0.805395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXP 8</th>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.793834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            val      test\n",
       "EXP 1  0.798536  0.790366\n",
       "EXP 2  0.788521  0.782659\n",
       "EXP 3  0.788521  0.780732\n",
       "EXP 4  0.775809  0.764933\n",
       "EXP 5  0.786210  0.793064\n",
       "EXP 6  0.804700  0.803083\n",
       "EXP 7  0.793143  0.805395\n",
       "EXP 8  0.795455  0.793834"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f776c4fe640>"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = accs_df.T.plot.bar(ylim=(0.75, 0.81), figsize=(10, 7))\n",
    "ax.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearly experiment 6 is the best one! \n",
    "best_model = clfs[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(159.0, 0.5, 'Actual')"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x864 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_test_preds[5])\n",
    "plt.figure(figsize=(20, 12))\n",
    "s = sns.heatmap(conf_mat, annot=True, fmt='.0f')\n",
    "s.set_xlabel('Predicted')\n",
    "s.set_ylabel('Actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# was a confusion matrix like this asked or were multiple 2x2 matrices asked? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"section-optional\"></a><h2 style=\"color:rgb(0,120,170)\">Task C: Linear Model Interpretability (2 extra point)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"background-color:rgb(224, 243, 255)\">\n",
    "Train a logistic regression model on the high-dimensional vectors. Take the coefficient weights, learned by the model, on each dimension (which here corresponds to each token in the dictionary). Separately for each class, study what are the tokens that have the highest contributions/importance for the predictions of the model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
